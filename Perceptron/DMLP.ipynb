{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Perceptron(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path  =r\"C:\\workspace\\django_workspace\\prototype4_\\framework\\PearMountEngine\\Dataset\\iris.csv\"\n",
    "query = \"TirionFordring$DMLP$InputLayer_CSV(y_label_index_starting=4, y_label_index_last=4)§Perceptron(hidden_layer_num=10)§Activation_Relu()§Perceptron(final_layer=True)§Activation_Sigmoid()§MeanSquaredError()§PerceptronParameter(learning_rate=0.001, batch_size=10, epochs=100, optimizer='SGD', accuracy_limit=90)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ParentClass 이름 불러오는 메쏘드\n",
    "class util:\n",
    "    def print_base(class_name):\n",
    "        for base in class_name.__class__.__bases__:\n",
    "            return base.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoopController:\n",
    "#     full_layer = []\n",
    "#     train_loss_list = []\n",
    "#     train_acc_list = []\n",
    "#     test_acc_list = []\n",
    "#     param = None\n",
    "    \n",
    "    def __init__(self, file_path, query):\n",
    "        self.file_path = file_path\n",
    "        self.query = query\n",
    "        \n",
    "        self.session = self.query.split(\"$\")[0]\n",
    "        self.network = self.query.split(\"$\")[1] #DMLP, CNN, RNN.. \n",
    "        string_to_pass = self.query.split(\"$\")[2].split(\"§\")\n",
    "        \n",
    "        self.full_layer = []\n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "        self.param = None\n",
    "        \n",
    "        for string in string_to_pass:\n",
    "            string_class =eval(string)\n",
    "            if util.print_base(string_class) == \"InputLayer\":\n",
    "                self.x_train, self.y_train, self.x_test, self.y_test = string_class.build_layer(self.file_path)\n",
    "                \n",
    "            elif util.print_base(string_class) == \"Parameter\":\n",
    "                self.param = string_class.get_parameter()\n",
    "                \n",
    "            else:\n",
    "                self.full_layer.append(string)        \n",
    "        \n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        y_batch = self.y_train[batch_mask]\n",
    "        \n",
    "        grads = self.learn_net.gradient(x_batch, y_batch)\n",
    "        self.optimizer.update(self.learn_net.weight_layer, grads)\n",
    "        \n",
    "        loss = self.learn_net.loss(x_batch, y_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        \n",
    "        print(\"train loss: \" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch +=1\n",
    "            \n",
    "            x_train_sample, y_train_sample = self.x_train, self.y_train\n",
    "            x_test_sample, y_test_sample = self.x_test, self.y_test\n",
    "            \n",
    "            train_acc = self.learn_net.accuracy(x_train_sample, y_train_sample)\n",
    "            test_acc = self.learn_net.accuracy(x_test_sample, y_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "            \n",
    "            print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "            \n",
    "            \n",
    "        self.current_iter += 1\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def learn(self):\n",
    "        self.learn_net = eval(self.network)(self.full_layer)\n",
    "        self.learn_net.build_model(self.x_train, self.y_train)\n",
    "#         print(\"build_model 완료\")\n",
    "        \n",
    "        self.epochs = self.param[\"epochs\"]\n",
    "        self.batch_size = self.param[\"batch_size\"]\n",
    "        self.learning_rate = self.param[\"learning_rate\"]\n",
    "        self.optimizer = eval(self.param[\"optimizer\"])(self.learning_rate)\n",
    "        \n",
    "        self.train_size = self.x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / self.batch_size, 1)\n",
    "        self.max_iter = int(self.epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "            if self.param[\"accuracy_limit\"] != None:\n",
    "                if self.test_acc_list[-1] >= self.param[\"accuracy_limit\"]*0.01:\n",
    "                    print(\"Reached accuracy_limit\")\n",
    "                    break\n",
    "            \n",
    "        test_acc = self.learn_net.accuracy(self.x_test, self.y_test)\n",
    "        \n",
    "        print(\"=====Final Test Accuracy====\")\n",
    "        print(\"test acc: \"+str(test_acc))\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "#### ParentClass 이름 불러오는 메쏘드\n",
    "import collections\n",
    "\n",
    "class DMLP:\n",
    "    def __init__(self, full_layer):\n",
    "        self.full_layer = full_layer\n",
    "        self.hidden_size_list = []\n",
    "        \n",
    "        self.weight_layer = {}                  #weight의 모음\n",
    "        self.layers = collections.OrderedDict() #쌓여있는 층\n",
    "        self.final_layer = None             #costlayer가 담겨있는 층\n",
    "        \n",
    "    def build_model(self, x, y, random_seed = 3, weight_init_std='sigmoid', weight_decay_lambda=0):\n",
    "        #난수 생성할때 랜덤값 고정하기, 평소에는 이거 지우면 된다. \n",
    "#         np.random.seed(random_seed)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        \n",
    "        for sum_class in self.full_layer:\n",
    "            if util.print_base(eval(sum_class)) == \"SummationLayer\":\n",
    "                hidden = eval(sum_class).pass_hidden_layer_num()\n",
    "                if hidden == \"final_layer\":\n",
    "                    hidden = y.shape[-1]\n",
    "                self.hidden_size_list.append(hidden)\n",
    "        \n",
    "        self.input_size = x.shape[-1]\n",
    "        self.hidden_layer_num = len(self.hidden_size_list)\n",
    "        \n",
    "        #가중치 초기화\n",
    "        self.__init_weight(weight_init_std)\n",
    "        \n",
    "        #계층 생성\n",
    "        sum_idx = 1\n",
    "        act_idx = 1\n",
    "        for sum_class in self.full_layer:\n",
    "            if util.print_base(eval(sum_class)) == \"SummationLayer\":\n",
    "                self.layers['Perceptron' + str(sum_idx)] = eval(sum_class)\n",
    "                self.layers['Perceptron' + str(sum_idx)].update(self.weight_layer['W' + str(sum_idx)],self.weight_layer['b' + str(sum_idx)])\n",
    "                sum_idx+=1\n",
    "            elif util.print_base(eval(sum_class)) == \"ActivationLayer\":\n",
    "                self.layers['Activation_function' + str(act_idx)] = eval(sum_class)\n",
    "                act_idx+=1\n",
    "                \n",
    "            elif util.print_base(eval(sum_class)) == \"CostLayer\":\n",
    "                self.final_layer = eval(sum_class)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ##가중치 초기화\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        '''\n",
    "        가중치 초기화할때 표준편차를 지정하는 것도 아주 중요한 파라미터.\n",
    "        'relu'일때는 'He 초깃값'\n",
    "        'sigmoid'일때는 'Xavier 초깃값'\n",
    "        '''\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list  \n",
    "        print(all_size_list)\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in (\"sigmoid\"):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  #sigmoid사용할때 초깃값\n",
    "            elif str(weight_init_std).lower() in (\"relu\"):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx -1])   #relu사용할 때 권장값\n",
    "                \n",
    "            self.weight_layer['W'+str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.weight_layer['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "            \n",
    "#             print(\"weight값: \"+str(self.weight_layer))\n",
    "     \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self,x,y):\n",
    "        y_hat = self.predict(x)\n",
    "        \n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 1):\n",
    "            W = self.weight_layer['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
    "            \n",
    "        return self.final_layer.forward(y_hat, y) + weight_decay\n",
    "    \n",
    "    \n",
    "    def accuracy(self,x, y):\n",
    "        y_hat = self.predict(x)\n",
    "        y_hat = (y_hat == y_hat.max(axis=1)[:,None]).astype(int)\n",
    "        accuracy = np.all(y_hat == y, axis=1)\n",
    "        \n",
    "        return np.sum(accuracy)/len(accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def gradient(self, x, y):\n",
    "        #forward\n",
    "        self.loss(x,y)\n",
    "        \n",
    "        #backward\n",
    "        gradient = 1\n",
    "        gradient = self.final_layer.backward(gradient)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        for layer in layers:\n",
    "            gradient = layer.backward(gradient)\n",
    "            \n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            grads['W' + str(idx)] = self.layers['Perceptron' + str(idx)].weight_gradient + self.weight_decay_lambda * self.layers[\"Perceptron\" + str(idx)].weight\n",
    "            \n",
    "            grads['b' + str(idx)] = self.layers['Perceptron'+str(idx)].bias_gradient\n",
    "            \n",
    "        return grads\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 10, 3]\n",
      "train loss: 4.5987006643804955\n",
      "=== epoch:1, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.822554280148707\n",
      "train loss: 5.35961499267525\n",
      "train loss: 6.040540927144539\n",
      "train loss: 5.491470266416775\n",
      "train loss: 4.8674057748752055\n",
      "train loss: 5.236646737738923\n",
      "train loss: 3.619297530609515\n",
      "train loss: 4.770245604053582\n",
      "train loss: 4.561553982455839\n",
      "=== epoch:2, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 5.449105855107102\n",
      "train loss: 5.254599565939041\n",
      "train loss: 6.124763897057135\n",
      "train loss: 3.7639381955822335\n",
      "train loss: 3.7230794084722842\n",
      "train loss: 3.3901877997989622\n",
      "train loss: 5.355207914135293\n",
      "train loss: 4.813409469315956\n",
      "train loss: 4.589771940857693\n",
      "=== epoch:3, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.698531836307386\n",
      "train loss: 5.384481082092766\n",
      "train loss: 4.8696563599731215\n",
      "train loss: 4.115182846150958\n",
      "train loss: 3.7330090698664855\n",
      "train loss: 3.8060107265009635\n",
      "train loss: 4.861669034607023\n",
      "train loss: 5.250661140762781\n",
      "train loss: 6.260883537939658\n",
      "=== epoch:4, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.955255668536818\n",
      "train loss: 5.397725658755534\n",
      "train loss: 4.825631245577585\n",
      "train loss: 4.816914005703184\n",
      "train loss: 5.859227184998171\n",
      "train loss: 5.50112390153331\n",
      "train loss: 5.182699947787446\n",
      "train loss: 5.380537823040171\n",
      "train loss: 4.508159070395554\n",
      "=== epoch:5, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 5.572176523715539\n",
      "train loss: 4.923395999229521\n",
      "train loss: 4.059424910999432\n",
      "train loss: 4.444055088403318\n",
      "train loss: 4.451685243635258\n",
      "train loss: 5.0065787145678415\n",
      "train loss: 3.7481754274524843\n",
      "train loss: 3.8169910084155423\n",
      "train loss: 5.079495366090044\n",
      "=== epoch:6, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.410313115595702\n",
      "train loss: 5.110783022748594\n",
      "train loss: 4.156970694498098\n",
      "train loss: 5.344267553521489\n",
      "train loss: 4.074701027538556\n",
      "train loss: 5.296232772666935\n",
      "train loss: 5.589982354401614\n",
      "train loss: 5.800952117459686\n",
      "train loss: 4.221536751212829\n",
      "=== epoch:7, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.580985831862657\n",
      "train loss: 5.642953684184724\n",
      "train loss: 3.785830858925731\n",
      "train loss: 4.425903709888385\n",
      "train loss: 4.42134717037813\n",
      "train loss: 5.3241617469849745\n",
      "train loss: 4.254827011325761\n",
      "train loss: 4.390363721991924\n",
      "train loss: 3.9824888004545245\n",
      "=== epoch:8, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 5.244579110090908\n",
      "train loss: 4.450044965455882\n",
      "train loss: 4.820834454180427\n",
      "train loss: 4.563459684245011\n",
      "train loss: 4.268609915495649\n",
      "train loss: 5.180581850216702\n",
      "train loss: 4.95868119305996\n",
      "train loss: 3.5479648191321425\n",
      "train loss: 4.319752393676297\n",
      "=== epoch:9, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.025897611395396\n",
      "train loss: 5.127618438854306\n",
      "train loss: 4.56193856413193\n",
      "train loss: 2.9038711644124025\n",
      "train loss: 4.155862562622927\n",
      "train loss: 5.369833961923451\n",
      "train loss: 4.743357055842991\n",
      "train loss: 4.732587836947331\n",
      "train loss: 4.542294050191067\n",
      "=== epoch:10, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.46001431080476\n",
      "train loss: 4.777537514137854\n",
      "train loss: 4.810631860584608\n",
      "train loss: 3.809853869543277\n",
      "train loss: 4.425193178298087\n",
      "train loss: 5.289795208734742\n",
      "train loss: 3.579407999302688\n",
      "train loss: 4.355031862642391\n",
      "train loss: 4.501775953284533\n",
      "=== epoch:11, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.8884334957278113\n",
      "train loss: 4.063820829412473\n",
      "train loss: 4.7985285062452725\n",
      "train loss: 4.645038282386775\n",
      "train loss: 3.6319914873893517\n",
      "train loss: 5.141337385153861\n",
      "train loss: 5.462908754346986\n",
      "train loss: 5.020037851599096\n",
      "train loss: 5.356065957917676\n",
      "=== epoch:12, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.178597912229461\n",
      "train loss: 4.888601140058766\n",
      "train loss: 4.724242660726598\n",
      "train loss: 4.554546932409874\n",
      "train loss: 4.184276426020983\n",
      "train loss: 4.6427013614520405\n",
      "train loss: 4.4248179433756265\n",
      "train loss: 3.5231752822127986\n",
      "train loss: 4.817640465272709\n",
      "=== epoch:13, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.633601392642879\n",
      "train loss: 4.736812282154383\n",
      "train loss: 5.213391974969207\n",
      "train loss: 3.984078092594131\n",
      "train loss: 4.5356245960783586\n",
      "train loss: 4.873911498140458\n",
      "train loss: 3.894419200109589\n",
      "train loss: 4.1762693661976105\n",
      "train loss: 5.417817646012781\n",
      "=== epoch:14, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 5.060035262237256\n",
      "train loss: 3.947886158993435\n",
      "train loss: 4.853540846105963\n",
      "train loss: 3.439597160761972\n",
      "train loss: 4.047576553918281\n",
      "train loss: 4.184040553841708\n",
      "train loss: 3.5519300864934538\n",
      "train loss: 4.776926088417997\n",
      "train loss: 4.380839122996241\n",
      "=== epoch:15, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 5.098478002115813\n",
      "train loss: 4.976085841703439\n",
      "train loss: 4.297192680570218\n",
      "train loss: 4.913553231838392\n",
      "train loss: 4.070392401897016\n",
      "train loss: 4.191248366361827\n",
      "train loss: 4.790189655430731\n",
      "train loss: 3.782120662402811\n",
      "train loss: 3.1644434006690103\n",
      "=== epoch:16, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.901760516465341\n",
      "train loss: 5.054951649179726\n",
      "train loss: 5.012051126900779\n",
      "train loss: 4.764317476361007\n",
      "train loss: 4.783836968733956\n",
      "train loss: 3.974756548737818\n",
      "train loss: 5.076225059258634\n",
      "train loss: 4.9496447597135855\n",
      "train loss: 3.898781523279252\n",
      "=== epoch:17, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.390495835326288\n",
      "train loss: 4.202917347062641\n",
      "train loss: 5.028054977298819\n",
      "train loss: 5.322076416381103\n",
      "train loss: 5.576454210376543\n",
      "train loss: 4.214317230054782\n",
      "train loss: 4.760806911602244\n",
      "train loss: 4.29113075444446\n",
      "train loss: 4.25076808473036\n",
      "=== epoch:18, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.9553689604649\n",
      "train loss: 5.348141536575593\n",
      "train loss: 4.376746501190205\n",
      "train loss: 4.2526610731821455\n",
      "train loss: 5.0998181402712985\n",
      "train loss: 4.6224784766646945\n",
      "train loss: 4.876409366431861\n",
      "train loss: 3.778981972302152\n",
      "train loss: 4.859814733228813\n",
      "=== epoch:19, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.7163621892663254\n",
      "train loss: 4.406276340082032\n",
      "train loss: 4.981035800601617\n",
      "train loss: 3.8512453380537037\n",
      "train loss: 4.673874801685797\n",
      "train loss: 3.9679558302192808\n",
      "train loss: 4.656057250314846\n",
      "train loss: 3.052869777809955\n",
      "train loss: 4.387107040060346\n",
      "=== epoch:20, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.916515634219502\n",
      "train loss: 4.139596550515138\n",
      "train loss: 4.21512444299413\n",
      "train loss: 4.129763320566017\n",
      "train loss: 3.8814260729503505\n",
      "train loss: 4.418131016301595\n",
      "train loss: 4.507522955829475\n",
      "train loss: 3.7975880456229705\n",
      "train loss: 3.663216381809034\n",
      "=== epoch:21, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.7082399196135905\n",
      "train loss: 4.432010378915303\n",
      "train loss: 5.024458642116977\n",
      "train loss: 4.836156498245203\n",
      "train loss: 4.792716129855519\n",
      "train loss: 4.150349357359286\n",
      "train loss: 4.091134145146705\n",
      "train loss: 4.627550754995697\n",
      "train loss: 4.203082986497689\n",
      "=== epoch:22, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.402806714342334\n",
      "train loss: 4.018984169541609\n",
      "train loss: 2.9566889403145993\n",
      "train loss: 3.7155985688937054\n",
      "train loss: 5.002139674870395\n",
      "train loss: 4.321672585750012\n",
      "train loss: 5.256837746944165\n",
      "train loss: 4.410157704808059\n",
      "train loss: 3.722814143702247\n",
      "=== epoch:23, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.22520590356537\n",
      "train loss: 4.278092433136558\n",
      "train loss: 3.902208425820434\n",
      "train loss: 4.851161450513834\n",
      "train loss: 4.425771826815722\n",
      "train loss: 2.9820366975592343\n",
      "train loss: 3.6863729000386067\n",
      "train loss: 4.026928130654282\n",
      "train loss: 3.4656310151106515\n",
      "=== epoch:24, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.678504266103865\n",
      "train loss: 4.140531653388259\n",
      "train loss: 4.558633743632177\n",
      "train loss: 4.156388848299121\n",
      "train loss: 4.963631478641086\n",
      "train loss: 4.586081406928153\n",
      "train loss: 4.510205099914581\n",
      "train loss: 4.696749214827284\n",
      "train loss: 4.485803864541998\n",
      "=== epoch:25, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.915296678075284\n",
      "train loss: 4.117326816385286\n",
      "train loss: 4.052376623474582\n",
      "train loss: 3.8446848197261434\n",
      "train loss: 4.14346780700711\n",
      "train loss: 4.179810002569826\n",
      "train loss: 3.814189783057589\n",
      "train loss: 3.766423304596082\n",
      "train loss: 4.855016126846636\n",
      "=== epoch:26, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.179577012252528\n",
      "train loss: 4.71475670387296\n",
      "train loss: 3.194812207132369\n",
      "train loss: 4.100164561814841\n",
      "train loss: 4.669335642028352\n",
      "train loss: 4.206093647833\n",
      "train loss: 3.12516742754134\n",
      "train loss: 5.08331913776767\n",
      "train loss: 4.441474065128851\n",
      "=== epoch:27, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.347138662359189\n",
      "train loss: 3.4731311235333058\n",
      "train loss: 3.7007873373844187\n",
      "train loss: 4.336265909149693\n",
      "train loss: 4.062490768635431\n",
      "train loss: 3.7737698121230894\n",
      "train loss: 3.989786885621333\n",
      "train loss: 4.388207482483052\n",
      "train loss: 4.237811309145176\n",
      "=== epoch:28, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 3.7333118038403907\n",
      "train loss: 3.943054817083597\n",
      "train loss: 3.7694420438903284\n",
      "train loss: 3.674996537957589\n",
      "train loss: 3.8687006699375543\n",
      "train loss: 3.0222585158623163\n",
      "train loss: 4.05883695572588\n",
      "train loss: 4.245014670808549\n",
      "train loss: 4.03034571759204\n",
      "=== epoch:29, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.353253458745118\n",
      "train loss: 3.7415778459290485\n",
      "train loss: 4.089616563177886\n",
      "train loss: 4.711073318310927\n",
      "train loss: 3.768819754001951\n",
      "train loss: 3.991942232075177\n",
      "train loss: 4.082417439965284\n",
      "train loss: 3.545222394692195\n",
      "train loss: 3.7118856974312773\n",
      "=== epoch:30, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.9254082627834945\n",
      "train loss: 3.899643824743755\n",
      "train loss: 4.00080150626678\n",
      "train loss: 3.966172191706826\n",
      "train loss: 3.9702169300669023\n",
      "train loss: 4.205686820270557\n",
      "train loss: 3.2829985102779187\n",
      "train loss: 4.3058851330523575\n",
      "train loss: 3.7721606297593167\n",
      "=== epoch:31, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.53936506521902\n",
      "train loss: 4.098350898496198\n",
      "train loss: 4.421971543731176\n",
      "train loss: 4.219087980949932\n",
      "train loss: 3.6931112681516627\n",
      "train loss: 3.7691359309285084\n",
      "train loss: 4.124299501503341\n",
      "train loss: 4.636879168525037\n",
      "train loss: 3.831238326878113\n",
      "=== epoch:32, train acc:0.34444444444444444, test acc:0.4 ===\n",
      "train loss: 4.604142886033276\n",
      "train loss: 3.7379467925065244\n",
      "train loss: 3.990952744691796\n",
      "train loss: 4.40941248981527\n",
      "train loss: 4.258311166109229\n",
      "train loss: 3.628264534973614\n",
      "train loss: 4.261919701488081\n",
      "train loss: 3.969884967723738\n",
      "train loss: 4.471741630089593\n",
      "=== epoch:33, train acc:0.3333333333333333, test acc:0.4 ===\n",
      "train loss: 3.447285242141226\n",
      "train loss: 3.5074459574765955\n",
      "train loss: 4.614519051347179\n",
      "train loss: 4.007480226189877\n",
      "train loss: 3.075367043361291\n",
      "train loss: 4.0269675817514985\n",
      "train loss: 4.515612287059817\n",
      "train loss: 3.724464000408456\n",
      "train loss: 3.900519065936599\n",
      "=== epoch:34, train acc:0.3333333333333333, test acc:0.4 ===\n",
      "train loss: 3.8484048619997013\n",
      "train loss: 4.039306086954145\n",
      "train loss: 4.7343454422304685\n",
      "train loss: 3.6308516094381287\n",
      "train loss: 3.6726407891921213\n",
      "train loss: 3.613778287493608\n",
      "train loss: 4.011782485693343\n",
      "train loss: 4.4365383184469795\n",
      "train loss: 3.8558787471956766\n",
      "=== epoch:35, train acc:0.3333333333333333, test acc:0.4 ===\n",
      "train loss: 4.186159069716002\n",
      "train loss: 3.4204125396757252\n",
      "train loss: 3.7696755887743465\n",
      "train loss: 3.7007468775597823\n",
      "train loss: 3.9284662412633335\n",
      "train loss: 4.022111409864905\n",
      "train loss: 3.6294228046686214\n",
      "train loss: 4.356626288411569\n",
      "train loss: 3.3898624338195207\n",
      "=== epoch:36, train acc:0.35555555555555557, test acc:0.4 ===\n",
      "train loss: 4.4163395086167325\n",
      "train loss: 4.066908085475895\n",
      "train loss: 4.724569840802871\n",
      "train loss: 4.194828567663971\n",
      "train loss: 3.6801938651332358\n",
      "train loss: 3.5701490584763125\n",
      "train loss: 3.574627555917342\n",
      "train loss: 3.382625012070206\n",
      "train loss: 4.053306577195009\n",
      "=== epoch:37, train acc:0.35555555555555557, test acc:0.4 ===\n",
      "train loss: 3.8465323721091806\n",
      "train loss: 4.5315484962430475\n",
      "train loss: 3.511250376741092\n",
      "train loss: 3.928980692402498\n",
      "train loss: 4.480813038532905\n",
      "train loss: 3.734170314493657\n",
      "train loss: 3.520639345656038\n",
      "train loss: 3.65044063719721\n",
      "train loss: 4.219689561045369\n",
      "=== epoch:38, train acc:0.35555555555555557, test acc:0.4 ===\n",
      "train loss: 3.4975868974755495\n",
      "train loss: 4.240778044199901\n",
      "train loss: 4.617945830140577\n",
      "train loss: 3.487125202905067\n",
      "train loss: 4.117941537339834\n",
      "train loss: 4.289262357890514\n",
      "train loss: 4.2528642836216735\n",
      "train loss: 3.512482368109324\n",
      "train loss: 4.341368582934758\n",
      "=== epoch:39, train acc:0.35555555555555557, test acc:0.4 ===\n",
      "train loss: 3.380643693930527\n",
      "train loss: 3.6467739032073885\n",
      "train loss: 4.209381303876422\n",
      "train loss: 4.272849688158878\n",
      "train loss: 3.9733867531580596\n",
      "train loss: 3.1285927244272815\n",
      "train loss: 3.588060266751728\n",
      "train loss: 3.7096527690587004\n",
      "train loss: 3.3244742703374777\n",
      "=== epoch:40, train acc:0.37777777777777777, test acc:0.4 ===\n",
      "train loss: 3.852241353571484\n",
      "train loss: 3.6395842566659318\n",
      "train loss: 3.851764468746752\n",
      "train loss: 4.278242087384014\n",
      "train loss: 4.215054818581876\n",
      "train loss: 4.778278306576578\n",
      "train loss: 4.281457730582245\n",
      "train loss: 3.748424307262474\n",
      "train loss: 3.959596249121943\n",
      "=== epoch:41, train acc:0.37777777777777777, test acc:0.4 ===\n",
      "train loss: 4.201423238318111\n",
      "train loss: 4.519146186575331\n",
      "train loss: 3.9378547192792923\n",
      "train loss: 3.8396735516226954\n",
      "train loss: 3.9048378578894822\n",
      "train loss: 4.0719431659784915\n",
      "train loss: 4.180527630451322\n",
      "train loss: 3.6825685564314723\n",
      "train loss: 4.757340646517061\n",
      "=== epoch:42, train acc:0.37777777777777777, test acc:0.4 ===\n",
      "train loss: 3.506671199742973\n",
      "train loss: 3.562282387520098\n",
      "train loss: 3.6750706196717617\n",
      "train loss: 4.5491955213477615\n",
      "train loss: 4.1041228494751545\n",
      "train loss: 4.068021379884054\n",
      "train loss: 3.5292834336173518\n",
      "train loss: 3.7162181762614006\n",
      "train loss: 4.207249345024301\n",
      "=== epoch:43, train acc:0.4, test acc:0.4 ===\n",
      "train loss: 3.551668702943864\n",
      "train loss: 4.129822140654136\n",
      "train loss: 3.8875524373852803\n",
      "train loss: 3.943852524541629\n",
      "train loss: 3.717120550534183\n",
      "train loss: 3.8952130567458827\n",
      "train loss: 3.5358247281408035\n",
      "train loss: 3.312425064225163\n",
      "train loss: 3.843733945422733\n",
      "=== epoch:44, train acc:0.4, test acc:0.4 ===\n",
      "train loss: 3.805638196504726\n",
      "train loss: 3.9376900843524245\n",
      "train loss: 3.9070738669635796\n",
      "train loss: 3.9514174548016348\n",
      "train loss: 3.939158579848248\n",
      "train loss: 3.777216166388972\n",
      "train loss: 3.4335588726718993\n",
      "train loss: 4.025899203309485\n",
      "train loss: 3.640808776023446\n",
      "=== epoch:45, train acc:0.4, test acc:0.4 ===\n",
      "train loss: 3.8051452286543808\n",
      "train loss: 3.49318437317362\n",
      "train loss: 4.333232123451373\n",
      "train loss: 4.015119636967915\n",
      "train loss: 4.111043962612089\n",
      "train loss: 4.040854023548632\n",
      "train loss: 3.3435201475161347\n",
      "train loss: 4.225956977779952\n",
      "train loss: 4.0754815951227314\n",
      "=== epoch:46, train acc:0.4, test acc:0.4 ===\n",
      "train loss: 3.468554831249861\n",
      "train loss: 4.5743996990585165\n",
      "train loss: 3.6799746122032504\n",
      "train loss: 4.0992536367323735\n",
      "train loss: 3.890884952151804\n",
      "train loss: 3.772407450445814\n",
      "train loss: 3.8287810161393887\n",
      "train loss: 3.661979838466403\n",
      "train loss: 4.141289009273526\n",
      "=== epoch:47, train acc:0.4111111111111111, test acc:0.4 ===\n",
      "train loss: 3.9717991485608897\n",
      "train loss: 3.871985540923962\n",
      "train loss: 3.7285537080187847\n",
      "train loss: 3.4984368071290715\n",
      "train loss: 4.116228170684438\n",
      "train loss: 3.585721894484784\n",
      "train loss: 3.36192937618311\n",
      "train loss: 4.135025261099872\n",
      "train loss: 4.44231414630982\n",
      "=== epoch:48, train acc:0.4111111111111111, test acc:0.43333333333333335 ===\n",
      "train loss: 3.423496012757161\n",
      "train loss: 3.9049370737127043\n",
      "train loss: 3.995461535274455\n",
      "train loss: 4.172028225242394\n",
      "train loss: 3.4510447205680337\n",
      "train loss: 3.418683877453092\n",
      "train loss: 3.485532888192119\n",
      "train loss: 3.8933094323448194\n",
      "train loss: 4.1792949578230765\n",
      "=== epoch:49, train acc:0.4111111111111111, test acc:0.43333333333333335 ===\n",
      "train loss: 4.088848011175177\n",
      "train loss: 4.531010558283402\n",
      "train loss: 4.131849934755221\n",
      "train loss: 3.74197883567111\n",
      "train loss: 3.827103232506659\n",
      "train loss: 4.216440464015198\n",
      "train loss: 3.7416545226844042\n",
      "train loss: 3.5007989609709784\n",
      "train loss: 3.8855455222379263\n",
      "=== epoch:50, train acc:0.4222222222222222, test acc:0.43333333333333335 ===\n",
      "train loss: 4.329415915647803\n",
      "train loss: 3.7691534473379473\n",
      "train loss: 3.8201384802412344\n",
      "train loss: 4.403847604781523\n",
      "train loss: 3.7926190675215463\n",
      "train loss: 3.5307785492908557\n",
      "train loss: 3.5730632534097007\n",
      "train loss: 3.794014764221026\n",
      "train loss: 3.998900295435373\n",
      "=== epoch:51, train acc:0.43333333333333335, test acc:0.5 ===\n",
      "Reached accuracy_limit\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.5\n"
     ]
    }
   ],
   "source": [
    "test = LoopController(file_path, query)\n",
    "test.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "plt.plot(test.train_acc_list, label=\"train_acc\")\n",
    "plt.plot(test.test_acc_list, label=\"test_acc\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2580798e668>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test.train_loss_list, label=\"train_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 10, 3]\n",
      "train loss: 3.933399140967781\n",
      "=== epoch:1, train acc:0.17777777777777778, test acc:0.3333333333333333 ===\n",
      "train loss: 4.116122658241115\n",
      "train loss: 4.072332913161946\n",
      "train loss: 4.295735059554384\n",
      "train loss: 4.272727081208288\n",
      "train loss: 3.850866143614928\n",
      "train loss: 3.881245014914913\n",
      "train loss: 3.726041576409269\n",
      "train loss: 4.0803862081492674\n",
      "train loss: 4.135044841167566\n",
      "=== epoch:2, train acc:0.18888888888888888, test acc:0.3333333333333333 ===\n",
      "train loss: 3.5124764990627235\n",
      "train loss: 3.751276653204434\n",
      "train loss: 3.864142019902329\n",
      "train loss: 4.099601633645444\n",
      "train loss: 4.080639039957171\n",
      "train loss: 4.077747999067333\n",
      "train loss: 4.497469086167783\n",
      "train loss: 4.003848739292145\n",
      "train loss: 3.9553149976062563\n",
      "=== epoch:3, train acc:0.2111111111111111, test acc:0.36666666666666664 ===\n",
      "train loss: 4.0145781223227335\n",
      "train loss: 4.303519183064934\n",
      "train loss: 4.057521092677258\n",
      "train loss: 3.692010619642164\n",
      "train loss: 3.925631210177777\n",
      "train loss: 3.7119091274632385\n",
      "train loss: 3.695555129795732\n",
      "train loss: 4.026624492666651\n",
      "train loss: 4.249271379563475\n",
      "=== epoch:4, train acc:0.2222222222222222, test acc:0.36666666666666664 ===\n",
      "train loss: 3.8685275219548445\n",
      "train loss: 4.409432193637956\n",
      "train loss: 3.613560300400679\n",
      "train loss: 4.039181476825272\n",
      "train loss: 4.183800952558675\n",
      "train loss: 3.319042623950907\n",
      "train loss: 3.8193689038633627\n",
      "train loss: 3.728171127826027\n",
      "train loss: 4.389513065227256\n",
      "=== epoch:5, train acc:0.2222222222222222, test acc:0.4 ===\n",
      "train loss: 3.710769367594952\n",
      "train loss: 4.417598823696143\n",
      "train loss: 4.202464330488533\n",
      "train loss: 3.704981428236102\n",
      "train loss: 3.858090782356481\n",
      "train loss: 3.6704049115390904\n",
      "train loss: 4.019566108641266\n",
      "train loss: 3.832841948379663\n",
      "train loss: 3.8236598876027217\n",
      "=== epoch:6, train acc:0.23333333333333334, test acc:0.4 ===\n",
      "train loss: 3.862532853216863\n",
      "train loss: 4.376765652898329\n",
      "train loss: 3.647483602351491\n",
      "train loss: 3.8102416101831524\n",
      "train loss: 3.8449597914748397\n",
      "train loss: 3.8686876134407875\n",
      "train loss: 3.8346933591420287\n",
      "train loss: 3.865300034355838\n",
      "train loss: 3.522725263000975\n",
      "=== epoch:7, train acc:0.24444444444444444, test acc:0.4 ===\n",
      "train loss: 4.297794355287104\n",
      "train loss: 4.180002001835717\n",
      "train loss: 4.224409315840812\n",
      "train loss: 3.831713277669419\n",
      "train loss: 4.057500323127906\n",
      "train loss: 4.320067945396621\n",
      "train loss: 3.9728246732696384\n",
      "train loss: 3.8576257417219093\n",
      "train loss: 3.817414340837474\n",
      "=== epoch:8, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.829484816332284\n",
      "train loss: 4.049864808263511\n",
      "train loss: 4.014381327548095\n",
      "train loss: 3.9518110151252137\n",
      "train loss: 4.174976266753013\n",
      "train loss: 4.172306832872105\n",
      "train loss: 4.243472743963304\n",
      "train loss: 3.528677160067452\n",
      "train loss: 3.5065432309619133\n",
      "=== epoch:9, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.6325538694754536\n",
      "train loss: 3.2869725055870287\n",
      "train loss: 3.8080790319857076\n",
      "train loss: 3.6289466231075593\n",
      "train loss: 3.64026815199477\n",
      "train loss: 3.620386380956968\n",
      "train loss: 3.9383718900712865\n",
      "train loss: 4.292172321389183\n",
      "train loss: 3.4702357326415942\n",
      "=== epoch:10, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.635586329538182\n",
      "train loss: 3.621394261252222\n",
      "train loss: 3.51543624678813\n",
      "train loss: 4.445321419620101\n",
      "train loss: 3.934363629694783\n",
      "train loss: 4.1002697001487425\n",
      "train loss: 4.061651420235477\n",
      "train loss: 3.5967942101710952\n",
      "train loss: 3.7968541126600908\n",
      "=== epoch:11, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.6320569758113224\n",
      "train loss: 3.8548568373933185\n",
      "train loss: 3.8321811541811126\n",
      "train loss: 4.396268261963824\n",
      "train loss: 3.3974319185764603\n",
      "train loss: 4.0920174529714695\n",
      "train loss: 3.6191470418695446\n",
      "train loss: 3.784325001517693\n",
      "train loss: 3.9918816574187406\n",
      "=== epoch:12, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.6218758058524583\n",
      "train loss: 3.943419167458232\n",
      "train loss: 4.112725339603763\n",
      "train loss: 3.894868893021348\n",
      "train loss: 3.910639978876046\n",
      "train loss: 3.893479506734357\n",
      "train loss: 3.2887509963689525\n",
      "train loss: 3.9616363638903693\n",
      "train loss: 4.0695036595435425\n",
      "=== epoch:13, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.9480221497367998\n",
      "train loss: 3.773837033306179\n",
      "train loss: 3.953094438374548\n",
      "train loss: 3.9485973724484555\n",
      "train loss: 3.9357747684050923\n",
      "train loss: 3.771936576778468\n",
      "train loss: 3.5920546532461386\n",
      "train loss: 3.7663597294864273\n",
      "train loss: 4.06971126107668\n",
      "=== epoch:14, train acc:0.25555555555555554, test acc:0.4 ===\n",
      "train loss: 3.7229091487895145\n",
      "train loss: 3.4571133848931\n",
      "train loss: 3.6211424741372302\n",
      "train loss: 3.576404349842721\n",
      "train loss: 3.7768002377105416\n",
      "train loss: 4.068945748133475\n",
      "train loss: 3.4257742487934224\n",
      "train loss: 4.200671121018485\n",
      "train loss: 3.4645636271743974\n",
      "=== epoch:15, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.937664255165489\n",
      "train loss: 3.8998300207746683\n",
      "train loss: 4.052744595656799\n",
      "train loss: 3.7141443418170734\n",
      "train loss: 3.8926468278904\n",
      "train loss: 3.735127209605808\n",
      "train loss: 3.562757839964671\n",
      "train loss: 3.425386925487313\n",
      "train loss: 3.676836810972269\n",
      "=== epoch:16, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.91697847597383\n",
      "train loss: 3.7687588075886276\n",
      "train loss: 3.5651423079043023\n",
      "train loss: 4.0259672230343835\n",
      "train loss: 3.6015943831516055\n",
      "train loss: 4.026887444510457\n",
      "train loss: 4.165319406737256\n",
      "train loss: 4.063888175659487\n",
      "train loss: 3.752784353110307\n",
      "=== epoch:17, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.7372382521019905\n",
      "train loss: 3.7426433324602875\n",
      "train loss: 4.048245751778567\n",
      "train loss: 3.72010333058729\n",
      "train loss: 3.860547469004262\n",
      "train loss: 3.7179454293664542\n",
      "train loss: 3.983565639761414\n",
      "train loss: 3.5666730409814935\n",
      "train loss: 3.8546402964481\n",
      "=== epoch:18, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.736277237063663\n",
      "train loss: 3.8858119151062227\n",
      "train loss: 3.558773997382848\n",
      "train loss: 3.550769758918728\n",
      "train loss: 3.7111998987912624\n",
      "train loss: 3.905611274437515\n",
      "train loss: 4.0311113962374066\n",
      "train loss: 3.8303002330660703\n",
      "train loss: 3.8514472586608077\n",
      "=== epoch:19, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.7133958642289144\n",
      "train loss: 3.5815526746645006\n",
      "train loss: 4.211356251495639\n",
      "train loss: 3.985412870518329\n",
      "train loss: 3.429676654450027\n",
      "train loss: 3.5649182474396466\n",
      "train loss: 3.6796788910495204\n",
      "train loss: 4.009054773535052\n",
      "train loss: 3.9797561107567305\n",
      "=== epoch:20, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 4.155254542063513\n",
      "train loss: 3.6947400639213437\n",
      "train loss: 3.6404517306055357\n",
      "train loss: 3.8567165845582125\n",
      "train loss: 3.9703477621866328\n",
      "train loss: 3.412011449407819\n",
      "train loss: 3.8363075082064184\n",
      "train loss: 3.731670401432916\n",
      "train loss: 3.9456695959455987\n",
      "=== epoch:21, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.516536636322238\n",
      "train loss: 4.096175125680891\n",
      "train loss: 3.981314894963913\n",
      "train loss: 3.6661772886472983\n",
      "train loss: 3.9488450995420994\n",
      "train loss: 3.995936065632943\n",
      "train loss: 3.8072085604492245\n",
      "train loss: 3.292566237802476\n",
      "train loss: 3.683326843235855\n",
      "=== epoch:22, train acc:0.26666666666666666, test acc:0.4 ===\n",
      "train loss: 3.7674630550842694\n",
      "train loss: 3.5268777316611604\n",
      "train loss: 3.5544612228672685\n",
      "train loss: 3.56146550722167\n",
      "train loss: 3.789912587378397\n",
      "train loss: 4.043615900506879\n",
      "train loss: 3.690943491091232\n",
      "train loss: 3.801346135884974\n",
      "train loss: 3.6334027232818182\n",
      "=== epoch:23, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 4.156632121861513\n",
      "train loss: 3.5427231032980355\n",
      "train loss: 3.526496863255445\n",
      "train loss: 3.635071414537096\n",
      "train loss: 3.785868133919541\n",
      "train loss: 3.928081317587559\n",
      "train loss: 3.7908057235192127\n",
      "train loss: 3.640838443678845\n",
      "train loss: 3.7771287291238047\n",
      "=== epoch:24, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.773409101799928\n",
      "train loss: 3.264621052028623\n",
      "train loss: 3.4908985220325413\n",
      "train loss: 3.9515098935752397\n",
      "train loss: 3.8820113202080133\n",
      "train loss: 3.777287208249004\n",
      "train loss: 3.9009862821380175\n",
      "train loss: 3.7212075698520066\n",
      "train loss: 3.6317710810419936\n",
      "=== epoch:25, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.8037779797873648\n",
      "train loss: 3.6286461920184063\n",
      "train loss: 3.655421681205394\n",
      "train loss: 3.7936961436266037\n",
      "train loss: 3.7356237161217622\n",
      "train loss: 3.880423731767696\n",
      "train loss: 3.5026578672359014\n",
      "train loss: 3.771215321262018\n",
      "train loss: 3.906016714559903\n",
      "=== epoch:26, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.679664312110999\n",
      "train loss: 3.88320497715035\n",
      "train loss: 3.6366485223627825\n",
      "train loss: 3.9907352423027436\n",
      "train loss: 4.00643475138815\n",
      "train loss: 3.5274543006863213\n",
      "train loss: 3.6544821379047874\n",
      "train loss: 3.5200552041892794\n",
      "train loss: 3.5308791234744294\n",
      "=== epoch:27, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.5003202953736103\n",
      "train loss: 3.746506037211473\n",
      "train loss: 3.588581962321339\n",
      "train loss: 3.586625904008437\n",
      "train loss: 3.848127704261174\n",
      "train loss: 3.6484581725558467\n",
      "train loss: 3.618096125320042\n",
      "train loss: 3.8376835822521675\n",
      "train loss: 3.765346113903933\n",
      "=== epoch:28, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.59588990874097\n",
      "train loss: 3.8380220989255265\n",
      "train loss: 3.942491747119582\n",
      "train loss: 3.488175880835507\n",
      "train loss: 4.054097430852166\n",
      "train loss: 3.6318053899421887\n",
      "train loss: 3.5632896059563874\n",
      "train loss: 3.8184903777866768\n",
      "train loss: 3.854005620154491\n",
      "=== epoch:29, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.713421270353837\n",
      "train loss: 3.6217059631032438\n",
      "train loss: 3.747331294302068\n",
      "train loss: 3.4687964122621104\n",
      "train loss: 3.798932773979083\n",
      "train loss: 3.600565396614116\n",
      "train loss: 3.361770991708746\n",
      "train loss: 3.6404234199744896\n",
      "train loss: 3.8784707344043774\n",
      "=== epoch:30, train acc:0.26666666666666666, test acc:0.43333333333333335 ===\n",
      "train loss: 3.701939928427316\n",
      "train loss: 4.078841988730719\n",
      "train loss: 3.705704906635652\n",
      "train loss: 3.592896959178411\n",
      "train loss: 3.5133860872506224\n",
      "train loss: 3.569029617179967\n",
      "train loss: 3.4423233920131597\n",
      "train loss: 3.562380370908171\n",
      "train loss: 3.5923184909004444\n",
      "=== epoch:31, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.4648180502331734\n",
      "train loss: 4.050147086324757\n",
      "train loss: 3.4532595616131996\n",
      "train loss: 3.5535696078379875\n",
      "train loss: 3.6777145057379195\n",
      "train loss: 3.470466006495045\n",
      "train loss: 3.486306790649619\n",
      "train loss: 3.9180423200146697\n",
      "train loss: 3.580420724145934\n",
      "=== epoch:32, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.5703579239997927\n",
      "train loss: 3.457666323098005\n",
      "train loss: 3.7539833293218288\n",
      "train loss: 3.3552979171830692\n",
      "train loss: 3.635297155777597\n",
      "train loss: 3.885073785424864\n",
      "train loss: 3.84101883597473\n",
      "train loss: 3.621542458491579\n",
      "train loss: 3.565108471872991\n",
      "=== epoch:33, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.4541920024474093\n",
      "train loss: 3.99984716184015\n",
      "train loss: 3.4652243783278847\n",
      "train loss: 3.4529766607868586\n",
      "train loss: 3.7509868658303405\n",
      "train loss: 3.684956682534098\n",
      "train loss: 3.4573616219787207\n",
      "train loss: 3.7012843110403097\n",
      "train loss: 3.7703828812334486\n",
      "=== epoch:34, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.7644442213096676\n",
      "train loss: 3.70027410854712\n",
      "train loss: 3.456224270407129\n",
      "train loss: 3.5512559213345325\n",
      "train loss: 3.6152303226673212\n",
      "train loss: 3.453477791393189\n",
      "train loss: 3.456913708111834\n",
      "train loss: 3.7342275872716844\n",
      "train loss: 3.4245161069238974\n",
      "=== epoch:35, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.535602083218392\n",
      "train loss: 3.770088794022962\n",
      "train loss: 3.723316199678599\n",
      "train loss: 3.8525867579548994\n",
      "train loss: 3.43749984334755\n",
      "train loss: 3.537770413391161\n",
      "train loss: 3.7384405091800557\n",
      "train loss: 3.8198515181876465\n",
      "train loss: 3.460555329872929\n",
      "=== epoch:36, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.5216284035886742\n",
      "train loss: 3.6093449522932564\n",
      "train loss: 3.9458152902696897\n",
      "train loss: 3.733668501868232\n",
      "train loss: 3.5228304736536096\n",
      "train loss: 3.8389730111609177\n",
      "train loss: 3.6454147006407656\n",
      "train loss: 3.504399288741861\n",
      "train loss: 3.7781473093503353\n",
      "=== epoch:37, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3506852410954773\n",
      "train loss: 3.7865036215312418\n",
      "train loss: 3.7145730477139387\n",
      "train loss: 3.6140281027743826\n",
      "train loss: 3.431689500434915\n",
      "train loss: 3.8805872870705937\n",
      "train loss: 3.740049597223879\n",
      "train loss: 3.5607600313473236\n",
      "train loss: 3.668539858246091\n",
      "=== epoch:38, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.415504405161418\n",
      "train loss: 3.387192283858061\n",
      "train loss: 3.57759119530173\n",
      "train loss: 3.39209559902559\n",
      "train loss: 3.482995918725411\n",
      "train loss: 3.4503565949531194\n",
      "train loss: 3.729581524708764\n",
      "train loss: 3.622466822939783\n",
      "train loss: 3.5376062405885067\n",
      "=== epoch:39, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.636816534568534\n",
      "train loss: 3.578996099021438\n",
      "train loss: 3.586435639691714\n",
      "train loss: 3.5706582553805886\n",
      "train loss: 3.4982688020863395\n",
      "train loss: 3.591414213831566\n",
      "train loss: 3.6071711838006615\n",
      "train loss: 3.868603875045357\n",
      "train loss: 3.4941923166080344\n",
      "=== epoch:40, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3604109242084186\n",
      "train loss: 3.394779739764673\n",
      "train loss: 3.422015305497087\n",
      "train loss: 3.6004311008313614\n",
      "train loss: 3.467609239822726\n",
      "train loss: 3.482703125118269\n",
      "train loss: 3.665720463155113\n",
      "train loss: 3.490545820002167\n",
      "train loss: 3.6999588543181665\n",
      "=== epoch:41, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.5679235611119617\n",
      "train loss: 3.6407916400473526\n",
      "train loss: 3.5159130795053675\n",
      "train loss: 3.446185842180339\n",
      "train loss: 3.3974814281079837\n",
      "train loss: 3.486083363225512\n",
      "train loss: 3.4448684460083734\n",
      "train loss: 3.4959137313618043\n",
      "train loss: 3.4640354774365196\n",
      "=== epoch:42, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.4746837777481114\n",
      "train loss: 3.5689707384449214\n",
      "train loss: 3.617908951909685\n",
      "train loss: 3.5378465885882147\n",
      "train loss: 3.3781446558885113\n",
      "train loss: 3.7671041227662543\n",
      "train loss: 3.469988976605086\n",
      "train loss: 3.482421779545899\n",
      "train loss: 3.447175513028824\n",
      "=== epoch:43, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.5156691745922064\n",
      "train loss: 3.3802830276000844\n",
      "train loss: 3.5448696901661028\n",
      "train loss: 3.4477912965102298\n",
      "train loss: 3.560109530950989\n",
      "train loss: 3.458176670529559\n",
      "train loss: 3.3229454356271986\n",
      "train loss: 3.627285419316734\n",
      "train loss: 3.4425466322812563\n",
      "=== epoch:44, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3158890236172565\n",
      "train loss: 3.6595213169611305\n",
      "train loss: 3.277174402801844\n",
      "train loss: 3.655033335884156\n",
      "train loss: 3.46341092518817\n",
      "train loss: 3.5302955803940796\n",
      "train loss: 3.621952968955773\n",
      "train loss: 3.7019332062939005\n",
      "train loss: 3.317764977969592\n",
      "=== epoch:45, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.475005897713308\n",
      "train loss: 3.5004547844733547\n",
      "train loss: 3.442392163353922\n",
      "train loss: 3.5049748482088674\n",
      "train loss: 3.5623660852522043\n",
      "train loss: 3.638504418329556\n",
      "train loss: 3.4784494446094962\n",
      "train loss: 3.5188025378581544\n",
      "train loss: 3.444487257803921\n",
      "=== epoch:46, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.4475615124006236\n",
      "train loss: 3.619114595202406\n",
      "train loss: 3.460443236109816\n",
      "train loss: 3.3471298970216843\n",
      "train loss: 3.4428108817194\n",
      "train loss: 3.3526377734013657\n",
      "train loss: 3.5187484029225784\n",
      "train loss: 3.364637723575581\n",
      "train loss: 3.45886620874871\n",
      "=== epoch:47, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.551054843882836\n",
      "train loss: 3.4087270139926016\n",
      "train loss: 3.366141189930272\n",
      "train loss: 3.6072203448075375\n",
      "train loss: 3.453885625907867\n",
      "train loss: 3.628115503876178\n",
      "train loss: 3.5377467679945096\n",
      "train loss: 3.5055228938159986\n",
      "train loss: 3.4712884636972823\n",
      "=== epoch:48, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.4066471414394432\n",
      "train loss: 3.3921110606929825\n",
      "train loss: 3.339592575852729\n",
      "train loss: 3.4871901079438112\n",
      "train loss: 3.337586607621439\n",
      "train loss: 3.512380072853013\n",
      "train loss: 3.4603345625762754\n",
      "train loss: 3.387978412015577\n",
      "train loss: 3.35211880694417\n",
      "=== epoch:49, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3606505331496295\n",
      "train loss: 3.428913055195533\n",
      "train loss: 3.4458109548499944\n",
      "train loss: 3.4285489722148776\n",
      "train loss: 3.3167153153288087\n",
      "train loss: 3.562385428717293\n",
      "train loss: 3.313927533885861\n",
      "train loss: 3.3477988449886995\n",
      "train loss: 3.3786377621743147\n",
      "=== epoch:50, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.6345909673964645\n",
      "train loss: 3.4282939014194564\n",
      "train loss: 3.474289940301909\n",
      "train loss: 3.4663008383293312\n",
      "train loss: 3.3475043340753765\n",
      "train loss: 3.643350381333917\n",
      "train loss: 3.466528687348966\n",
      "train loss: 3.6111848062945002\n",
      "train loss: 3.387690513132819\n",
      "=== epoch:51, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.442611441041718\n",
      "train loss: 3.4957573001419893\n",
      "train loss: 3.3277546042275334\n",
      "train loss: 3.21368893828758\n",
      "train loss: 3.376340375623973\n",
      "train loss: 3.3671401749341916\n",
      "train loss: 3.418640896578593\n",
      "train loss: 3.424272070244661\n",
      "train loss: 3.3108610241470844\n",
      "=== epoch:52, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.492992459574366\n",
      "train loss: 3.3956450513616065\n",
      "train loss: 3.4195216316943524\n",
      "train loss: 3.396769052128124\n",
      "train loss: 3.4173853303764434\n",
      "train loss: 3.3803864612489454\n",
      "train loss: 3.4605149287519392\n",
      "train loss: 3.4191609826386187\n",
      "train loss: 3.4519397975508017\n",
      "=== epoch:53, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3885818748566696\n",
      "train loss: 3.443324313216925\n",
      "train loss: 3.3378912179631186\n",
      "train loss: 3.2756505231566018\n",
      "train loss: 3.324216412586247\n",
      "train loss: 3.400062629428979\n",
      "train loss: 3.3178433969154058\n",
      "train loss: 3.467972200812533\n",
      "train loss: 3.4758374206788556\n",
      "=== epoch:54, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3644114817836113\n",
      "train loss: 3.293399516550989\n",
      "train loss: 3.393017306438\n",
      "train loss: 3.3534470179181883\n",
      "train loss: 3.420313150570969\n",
      "train loss: 3.2763931692679993\n",
      "train loss: 3.498906796435259\n",
      "train loss: 3.524237949592461\n",
      "train loss: 3.2186720916088944\n",
      "=== epoch:55, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.450806410493934\n",
      "train loss: 3.39612145175155\n",
      "train loss: 3.3656089001093026\n",
      "train loss: 3.4457561484497563\n",
      "train loss: 3.295461780495783\n",
      "train loss: 3.3418319564787007\n",
      "train loss: 3.510002400561217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.4463569975321375\n",
      "train loss: 3.325086734402477\n",
      "=== epoch:56, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.277962418775983\n",
      "train loss: 3.3431934095538662\n",
      "train loss: 3.385346020660415\n",
      "train loss: 3.364903768078516\n",
      "train loss: 3.219441009181574\n",
      "train loss: 3.3818564545140983\n",
      "train loss: 3.505575030378621\n",
      "train loss: 3.369422750376995\n",
      "train loss: 3.3477783777054553\n",
      "=== epoch:57, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3885855083472154\n",
      "train loss: 3.3552534229128037\n",
      "train loss: 3.2967142305705393\n",
      "train loss: 3.392413600373426\n",
      "train loss: 3.310503258058213\n",
      "train loss: 3.4054730106366176\n",
      "train loss: 3.2319200172064324\n",
      "train loss: 3.3452395182608288\n",
      "train loss: 3.4356656201754263\n",
      "=== epoch:58, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.4584880671370906\n",
      "train loss: 3.440695930264997\n",
      "train loss: 3.381224822159675\n",
      "train loss: 3.3663991386271306\n",
      "train loss: 3.3653085112858037\n",
      "train loss: 3.4238343974233643\n",
      "train loss: 3.377704985644587\n",
      "train loss: 3.2950931903189464\n",
      "train loss: 3.295264712710976\n",
      "=== epoch:59, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.387617240761931\n",
      "train loss: 3.3499902238534127\n",
      "train loss: 3.4283513656240534\n",
      "train loss: 3.289852952846184\n",
      "train loss: 3.4470542297884776\n",
      "train loss: 3.3726529055502206\n",
      "train loss: 3.368841763641199\n",
      "train loss: 3.2360164090160324\n",
      "train loss: 3.238092960990298\n",
      "=== epoch:60, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3051486188483348\n",
      "train loss: 3.264610619864375\n",
      "train loss: 3.2264541327840983\n",
      "train loss: 3.4262944849474186\n",
      "train loss: 3.3585491599194643\n",
      "train loss: 3.4111399563024727\n",
      "train loss: 3.54199984671167\n",
      "train loss: 3.315307037272911\n",
      "train loss: 3.218855601753881\n",
      "=== epoch:61, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.304189287674144\n",
      "train loss: 3.3914378000658623\n",
      "train loss: 3.3028994982813136\n",
      "train loss: 3.4422400666337043\n",
      "train loss: 3.2832421259225413\n",
      "train loss: 3.3318263665113244\n",
      "train loss: 3.294387860620665\n",
      "train loss: 3.306794040527242\n",
      "train loss: 3.4149589581750233\n",
      "=== epoch:62, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.2935953923224517\n",
      "train loss: 3.228865367300717\n",
      "train loss: 3.3588595874012905\n",
      "train loss: 3.2680470602461136\n",
      "train loss: 3.1963475688110727\n",
      "train loss: 3.190283449349721\n",
      "train loss: 3.2998379970710063\n",
      "train loss: 3.3450624708487857\n",
      "train loss: 3.2345973720815473\n",
      "=== epoch:63, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3129796081646385\n",
      "train loss: 3.3071182269117037\n",
      "train loss: 3.266411482293179\n",
      "train loss: 3.4113223369256342\n",
      "train loss: 3.3776042843643017\n",
      "train loss: 3.2806245580992863\n",
      "train loss: 3.2875447260228685\n",
      "train loss: 3.34430265122995\n",
      "train loss: 3.3013864856949207\n",
      "=== epoch:64, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.2044169411503862\n",
      "train loss: 3.2709410836667927\n",
      "train loss: 3.2925820375859023\n",
      "train loss: 3.3027999941119845\n",
      "train loss: 3.3022230194394866\n",
      "train loss: 3.216036781922952\n",
      "train loss: 3.3294369151812155\n",
      "train loss: 3.3807215993468795\n",
      "train loss: 3.2876658998483883\n",
      "=== epoch:65, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.263449011702332\n",
      "train loss: 3.2914697852166204\n",
      "train loss: 3.2707412837969576\n",
      "train loss: 3.355190405610144\n",
      "train loss: 3.2031561285532666\n",
      "train loss: 3.310931056841376\n",
      "train loss: 3.192508749973767\n",
      "train loss: 3.2878563849819837\n",
      "train loss: 3.2391248261526377\n",
      "=== epoch:66, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.3423939379322514\n",
      "train loss: 3.284033013824339\n",
      "train loss: 3.319338283927575\n",
      "train loss: 3.2770483971423725\n",
      "train loss: 3.353151652481584\n",
      "train loss: 3.2702352142124016\n",
      "train loss: 3.3431038063463996\n",
      "train loss: 3.307126438357074\n",
      "train loss: 3.2614085263352615\n",
      "=== epoch:67, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.317471375973383\n",
      "train loss: 3.2594962622382813\n",
      "train loss: 3.2258198253966883\n",
      "train loss: 3.2253554879347317\n",
      "train loss: 3.174130413741004\n",
      "train loss: 3.2682165275335975\n",
      "train loss: 3.331235089043734\n",
      "train loss: 3.270440438347798\n",
      "train loss: 3.206233665407648\n",
      "=== epoch:68, train acc:0.2777777777777778, test acc:0.43333333333333335 ===\n",
      "train loss: 3.2583621991680327\n",
      "train loss: 3.286482250158612\n",
      "train loss: 3.319171035579022\n",
      "train loss: 3.233029483287135\n",
      "train loss: 3.2132628293188272\n",
      "train loss: 3.2812003235774188\n",
      "train loss: 3.307129647932174\n",
      "train loss: 3.3521926441292598\n",
      "train loss: 3.251999939478211\n",
      "=== epoch:69, train acc:0.28888888888888886, test acc:0.43333333333333335 ===\n",
      "train loss: 3.2361031324588034\n",
      "train loss: 3.3075215146048893\n",
      "train loss: 3.258705126822494\n",
      "train loss: 3.234543821423057\n",
      "train loss: 3.332835954728666\n",
      "train loss: 3.179790583650991\n",
      "train loss: 3.241576118203789\n",
      "train loss: 3.160309717924294\n",
      "train loss: 3.2203912054135673\n",
      "=== epoch:70, train acc:0.37777777777777777, test acc:0.4 ===\n",
      "train loss: 3.2520681523150454\n",
      "train loss: 3.209591614933922\n",
      "train loss: 3.1801520543635453\n",
      "train loss: 3.256046332535789\n",
      "train loss: 3.2610746540193576\n",
      "train loss: 3.233145244846835\n",
      "train loss: 3.3138904346663423\n",
      "train loss: 3.315525323399835\n",
      "train loss: 3.2693047988679234\n",
      "=== epoch:71, train acc:0.4, test acc:0.4 ===\n",
      "train loss: 3.329645990347059\n",
      "train loss: 3.2569675044541357\n",
      "train loss: 3.1939471095252308\n",
      "train loss: 3.230545403372628\n",
      "train loss: 3.219517645318675\n",
      "train loss: 3.245069873819352\n",
      "train loss: 3.2006397055867044\n",
      "train loss: 3.264800151629481\n",
      "train loss: 3.246694398883369\n",
      "=== epoch:72, train acc:0.37777777777777777, test acc:0.4 ===\n",
      "train loss: 3.249358470036403\n",
      "train loss: 3.243989428950212\n",
      "train loss: 3.212820028661751\n",
      "train loss: 3.2408311516846706\n",
      "train loss: 3.2045501098507057\n",
      "train loss: 3.2668848087431455\n",
      "train loss: 3.2422138956314006\n",
      "train loss: 3.2551350067206593\n",
      "train loss: 3.285235177805792\n",
      "=== epoch:73, train acc:0.4777777777777778, test acc:0.4666666666666667 ===\n",
      "train loss: 3.210780815948401\n",
      "train loss: 3.2256112870095754\n",
      "train loss: 3.2194304771561866\n",
      "train loss: 3.197131265197448\n",
      "train loss: 3.2308233966203614\n",
      "train loss: 3.292336378683466\n",
      "train loss: 3.184836202734618\n",
      "train loss: 3.156532910137872\n",
      "train loss: 3.237281002707726\n",
      "=== epoch:74, train acc:0.4777777777777778, test acc:0.5666666666666667 ===\n",
      "train loss: 3.3021108957714604\n",
      "train loss: 3.1825242680617407\n",
      "train loss: 3.232336125669076\n",
      "train loss: 3.182171595605961\n",
      "train loss: 3.214046791574719\n",
      "train loss: 3.2345674669061037\n",
      "train loss: 3.210449374169283\n",
      "train loss: 3.203941426656911\n",
      "train loss: 3.2927174944553386\n",
      "=== epoch:75, train acc:0.5444444444444444, test acc:0.7 ===\n",
      "train loss: 3.151744808792935\n",
      "train loss: 3.2288147803433764\n",
      "train loss: 3.205747202268523\n",
      "train loss: 3.2538901168978627\n",
      "train loss: 3.1921685998495124\n",
      "train loss: 3.218107859517947\n",
      "train loss: 3.2509085183424307\n",
      "train loss: 3.173376690895477\n",
      "train loss: 3.2509757709118023\n",
      "=== epoch:76, train acc:0.4111111111111111, test acc:0.4 ===\n",
      "train loss: 3.216088161140124\n",
      "train loss: 3.249296210117336\n",
      "train loss: 3.219601225971382\n",
      "train loss: 3.203771426112905\n",
      "train loss: 3.209142973690972\n",
      "train loss: 3.3370990029305925\n",
      "train loss: 3.256005250853677\n",
      "train loss: 3.1943965822803078\n",
      "train loss: 3.193463896050515\n",
      "=== epoch:77, train acc:0.45555555555555555, test acc:0.43333333333333335 ===\n",
      "train loss: 3.2404094248067845\n",
      "train loss: 3.2324954260316154\n",
      "train loss: 3.2256699194173097\n",
      "train loss: 3.2994033170475143\n",
      "train loss: 3.1728358664030707\n",
      "train loss: 3.2299273900521377\n",
      "train loss: 3.203048957464521\n",
      "train loss: 3.2213114795137976\n",
      "train loss: 3.178561680318222\n",
      "=== epoch:78, train acc:0.4888888888888889, test acc:0.43333333333333335 ===\n",
      "train loss: 3.1733662019250826\n",
      "train loss: 3.1830067578354475\n",
      "train loss: 3.202490865729161\n",
      "train loss: 3.1682549482160827\n",
      "train loss: 3.157762649333271\n",
      "train loss: 3.1694889714250203\n",
      "train loss: 3.260656408099866\n",
      "train loss: 3.1757744172746234\n",
      "train loss: 3.2341601469001917\n",
      "=== epoch:79, train acc:0.5333333333333333, test acc:0.43333333333333335 ===\n",
      "train loss: 3.18905846139553\n",
      "train loss: 3.1934545577766476\n",
      "train loss: 3.166888174898926\n",
      "train loss: 3.1652228517392507\n",
      "train loss: 3.2241712782027148\n",
      "train loss: 3.1693567121159316\n",
      "train loss: 3.1752904253449197\n",
      "train loss: 3.220103610845777\n",
      "train loss: 3.265283619913096\n",
      "=== epoch:80, train acc:0.5333333333333333, test acc:0.4666666666666667 ===\n",
      "train loss: 3.15610329920107\n",
      "train loss: 3.2169963248979387\n",
      "train loss: 3.167616828029246\n",
      "train loss: 3.1606243206967837\n",
      "train loss: 3.122456185835149\n",
      "train loss: 3.1701467459480632\n",
      "train loss: 3.1701858662271474\n",
      "train loss: 3.1908246179856445\n",
      "train loss: 3.1507797084211795\n",
      "=== epoch:81, train acc:0.5333333333333333, test acc:0.5333333333333333 ===\n",
      "train loss: 3.2197804019571676\n",
      "train loss: 3.173589333528198\n",
      "train loss: 3.205053499978854\n",
      "train loss: 3.152414959232205\n",
      "train loss: 3.18440901159902\n",
      "train loss: 3.1280575429458715\n",
      "train loss: 3.201391876730689\n",
      "train loss: 3.1561551401199694\n",
      "train loss: 3.1678368502438867\n",
      "=== epoch:82, train acc:0.6, test acc:0.5333333333333333 ===\n",
      "train loss: 3.11397227902978\n",
      "train loss: 3.117232991786698\n",
      "train loss: 3.171144579394156\n",
      "train loss: 3.160927177126493\n",
      "train loss: 3.1920534840852652\n",
      "train loss: 3.179413446288768\n",
      "train loss: 3.0737111504071586\n",
      "train loss: 3.168906726550759\n",
      "train loss: 3.1987378229576198\n",
      "=== epoch:83, train acc:0.6222222222222222, test acc:0.5666666666666667 ===\n",
      "train loss: 3.1243868519461038\n",
      "train loss: 3.1777293596232643\n",
      "train loss: 3.145577198586767\n",
      "train loss: 3.1285861929275742\n",
      "train loss: 3.167665081308673\n",
      "train loss: 3.1636915013099918\n",
      "train loss: 3.1700502864687503\n",
      "train loss: 3.127413813201061\n",
      "train loss: 3.2071517153412197\n",
      "=== epoch:84, train acc:0.6444444444444445, test acc:0.5666666666666667 ===\n",
      "train loss: 3.159503738570331\n",
      "train loss: 3.1019164049177053\n",
      "train loss: 3.115594207244463\n",
      "train loss: 3.1678301233827\n",
      "train loss: 3.1667319142580093\n",
      "train loss: 3.110881409872712\n",
      "train loss: 3.13799983511227\n",
      "train loss: 3.1331777686273825\n",
      "train loss: 3.1102921015897005\n",
      "=== epoch:85, train acc:0.6444444444444445, test acc:0.5666666666666667 ===\n",
      "train loss: 3.1136518022131097\n",
      "train loss: 3.133542548312916\n",
      "train loss: 3.0971880081300722\n",
      "train loss: 3.142092097939078\n",
      "train loss: 3.1506685515416573\n",
      "train loss: 3.1237237146288317\n",
      "train loss: 3.056208843774693\n",
      "train loss: 3.140018780133841\n",
      "train loss: 3.166103753278733\n",
      "=== epoch:86, train acc:0.6666666666666666, test acc:0.5666666666666667 ===\n",
      "train loss: 3.2108895935519675\n",
      "train loss: 3.1487875208980673\n",
      "train loss: 3.104235042905449\n",
      "train loss: 3.1544668340872075\n",
      "train loss: 3.122510331706141\n",
      "train loss: 3.1220699090274784\n",
      "train loss: 3.165909052147181\n",
      "train loss: 3.1208556666571843\n",
      "train loss: 3.0154802560563523\n",
      "=== epoch:87, train acc:0.6777777777777778, test acc:0.5666666666666667 ===\n",
      "train loss: 3.0571615765809326\n",
      "train loss: 3.173364768031898\n",
      "train loss: 3.1162564562314854\n",
      "train loss: 3.0793271512538216\n",
      "train loss: 3.124615170892732\n",
      "train loss: 3.1605014867020356\n",
      "train loss: 3.1684247808276367\n",
      "train loss: 3.1788771529464235\n",
      "train loss: 3.120269779298003\n",
      "=== epoch:88, train acc:0.6888888888888889, test acc:0.5666666666666667 ===\n",
      "train loss: 2.9967413917603447\n",
      "train loss: 3.0926910713231455\n",
      "train loss: 3.123746771335667\n",
      "train loss: 3.136232169938989\n",
      "train loss: 3.166104068575948\n",
      "train loss: 3.1034510761755603\n",
      "train loss: 3.1011360106971795\n",
      "train loss: 3.100990428914628\n",
      "train loss: 3.114115748934935\n",
      "=== epoch:89, train acc:0.6888888888888889, test acc:0.5666666666666667 ===\n",
      "train loss: 3.12012833825145\n",
      "train loss: 3.1136529662151475\n",
      "train loss: 3.0425808957794813\n",
      "train loss: 3.178863101265699\n",
      "train loss: 3.121370226847215\n",
      "train loss: 3.088132469481339\n",
      "train loss: 3.0965681998050942\n",
      "train loss: 3.067727000721425\n",
      "train loss: 3.1445190164144154\n",
      "=== epoch:90, train acc:0.7, test acc:0.5666666666666667 ===\n",
      "train loss: 3.0476806653272743\n",
      "train loss: 3.16892080414566\n",
      "train loss: 3.1343074672053555\n",
      "train loss: 3.1204506963380165\n",
      "train loss: 3.0728712019039937\n",
      "train loss: 2.9500940691296407\n",
      "train loss: 3.108621011587599\n",
      "train loss: 3.0983966071639037\n",
      "train loss: 3.0289576622611967\n",
      "=== epoch:91, train acc:0.7, test acc:0.5666666666666667 ===\n",
      "train loss: 3.082695833742419\n",
      "train loss: 3.0453024433239846\n",
      "train loss: 3.1460915327026995\n",
      "train loss: 3.079877052855869\n",
      "train loss: 3.0608210026390124\n",
      "train loss: 3.089560994543605\n",
      "train loss: 2.9873127760016502\n",
      "train loss: 3.123170285009371\n",
      "train loss: 3.10389263068186\n",
      "=== epoch:92, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.1211327777633886\n",
      "train loss: 3.0910784656457\n",
      "train loss: 3.1729498544615877\n",
      "train loss: 3.1138951120521248\n",
      "train loss: 3.0646696780936846\n",
      "train loss: 3.054650500252837\n",
      "train loss: 3.0523593484140057\n",
      "train loss: 3.076971867145157\n",
      "train loss: 3.035355761525859\n",
      "=== epoch:93, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.1065534976088003\n",
      "train loss: 3.103438429772984\n",
      "train loss: 3.0774579112157916\n",
      "train loss: 3.064602645724846\n",
      "train loss: 3.1277481811020547\n",
      "train loss: 3.1103568467011855\n",
      "train loss: 3.1425698402927593\n",
      "train loss: 3.082957338831792\n",
      "train loss: 3.037225151070083\n",
      "=== epoch:94, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 2.980706343949341\n",
      "train loss: 3.083971947437301\n",
      "train loss: 3.0713235794990004\n",
      "train loss: 3.117021404229708\n",
      "train loss: 3.0673228483212305\n",
      "train loss: 3.0999497999298464\n",
      "train loss: 3.0363716029619354\n",
      "train loss: 3.065479798612874\n",
      "train loss: 3.0826978565049754\n",
      "=== epoch:95, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.0320129663854862\n",
      "train loss: 3.007382653184418\n",
      "train loss: 3.1467307002896416\n",
      "train loss: 3.067269530921574\n",
      "train loss: 3.0244246006311353\n",
      "train loss: 3.101078786479371\n",
      "train loss: 3.1525385412001867\n",
      "train loss: 3.1118128458304057\n",
      "train loss: 3.0761375838951146\n",
      "=== epoch:96, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.0919581837932677\n",
      "train loss: 2.9874000711791067\n",
      "train loss: 2.980812242659246\n",
      "train loss: 3.0922345005303304\n",
      "train loss: 2.912109914810798\n",
      "train loss: 2.9826768465034874\n",
      "train loss: 2.9218982926193195\n",
      "train loss: 2.952618825577842\n",
      "train loss: 2.9135961701060844\n",
      "=== epoch:97, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 2.985894059973478\n",
      "train loss: 3.048109261047587\n",
      "train loss: 3.0509076229712773\n",
      "train loss: 3.0513988459204833\n",
      "train loss: 3.075488261838018\n",
      "train loss: 3.000249285826805\n",
      "train loss: 3.082455231278946\n",
      "train loss: 3.0633979946204417\n",
      "train loss: 3.1662630870323922\n",
      "=== epoch:98, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.0304163517115716\n",
      "train loss: 3.0596471488336654\n",
      "train loss: 2.994432689868495\n",
      "train loss: 3.0919869186935327\n",
      "train loss: 3.0770570285310823\n",
      "train loss: 2.967236339202359\n",
      "train loss: 3.005537102517978\n",
      "train loss: 3.0178756771278707\n",
      "train loss: 3.091493322569016\n",
      "=== epoch:99, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.105006413016958\n",
      "train loss: 3.0027514618901807\n",
      "train loss: 3.0564168551711814\n",
      "train loss: 3.0466758041174566\n",
      "train loss: 3.0561524861086093\n",
      "train loss: 3.0875768410732376\n",
      "train loss: 3.0518039004031423\n",
      "train loss: 2.9412703440780903\n",
      "train loss: 2.954546757971118\n",
      "=== epoch:100, train acc:0.7111111111111111, test acc:0.5666666666666667 ===\n",
      "train loss: 3.0877981278805446\n",
      "train loss: 3.01217297257327\n",
      "train loss: 2.9876007414886123\n",
      "train loss: 2.9867861269848883\n",
      "train loss: 3.1332077182292704\n",
      "train loss: 3.0017518980164164\n",
      "train loss: 2.9836620571266304\n",
      "train loss: 2.9884739097360784\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.5666666666666667\n",
      "[4, 10, 3]\n",
      "train loss: 3.2957602155565744\n",
      "=== epoch:1, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.774230077555235\n",
      "train loss: 2.6153854832803862\n",
      "train loss: 5.0947763421878856\n",
      "train loss: 3.726414349911867\n",
      "train loss: 2.8549909023706395\n",
      "train loss: 4.074577655137255\n",
      "train loss: 3.797079115202979\n",
      "train loss: 3.414910190195197\n",
      "train loss: 4.21246638027414\n",
      "=== epoch:2, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 2.9239303109130215\n",
      "train loss: 4.5580420863525415\n",
      "train loss: 4.07128985535561\n",
      "train loss: 3.8637491134834376\n",
      "train loss: 3.3346068333093557\n",
      "train loss: 4.452504543944422\n",
      "train loss: 3.3604708263298786\n",
      "train loss: 3.8290378619510186\n",
      "train loss: 4.389035522777856\n",
      "=== epoch:3, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.9861252449041746\n",
      "train loss: 3.309452818857883\n",
      "train loss: 3.6986494458733112\n",
      "train loss: 3.672085342342895\n",
      "train loss: 3.013397829182821\n",
      "train loss: 3.961548937204474\n",
      "train loss: 2.9221914654837904\n",
      "train loss: 4.2358222286616805\n",
      "train loss: 3.6732726053202356\n",
      "=== epoch:4, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.9651254026791385\n",
      "train loss: 3.65035743053585\n",
      "train loss: 3.6926113894242176\n",
      "train loss: 3.8443400697184753\n",
      "train loss: 3.3744416607745356\n",
      "train loss: 3.8803989511593087\n",
      "train loss: 3.088052395309508\n",
      "train loss: 3.345768394155113\n",
      "train loss: 3.6193996274249782\n",
      "=== epoch:5, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.938216399513707\n",
      "train loss: 3.2963558079418696\n",
      "train loss: 3.5186382822626916\n",
      "train loss: 3.324064063565956\n",
      "train loss: 4.069355565577076\n",
      "train loss: 3.838863604856931\n",
      "train loss: 3.542500465925948\n",
      "train loss: 3.208481151986186\n",
      "train loss: 3.1992061278142643\n",
      "=== epoch:6, train acc:0.3333333333333333, test acc:0.3 ===\n",
      "train loss: 3.5277995404540685\n",
      "train loss: 3.524671373660225\n",
      "train loss: 3.3352354510432183\n",
      "train loss: 4.4965156840843905\n",
      "train loss: 4.476992115123056\n",
      "train loss: 3.040999107457897\n",
      "train loss: 3.884165687250562\n",
      "train loss: 3.000104064572557\n",
      "train loss: 3.233522028585773\n",
      "=== epoch:7, train acc:0.26666666666666666, test acc:0.26666666666666666 ===\n",
      "train loss: 2.991642491879854\n",
      "train loss: 3.471717137557549\n",
      "train loss: 3.2277079000351576\n",
      "train loss: 3.233437871953009\n",
      "train loss: 3.417940376820688\n",
      "train loss: 2.7809439663278623\n",
      "train loss: 3.155168039159994\n",
      "train loss: 3.505828058430278\n",
      "train loss: 3.6292763668033006\n",
      "=== epoch:8, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.9124243055061645\n",
      "train loss: 3.6122120877279547\n",
      "train loss: 3.6436752527482583\n",
      "train loss: 3.6552067938509447\n",
      "train loss: 3.7913948307152157\n",
      "train loss: 3.583084772612744\n",
      "train loss: 3.4019055770979536\n",
      "train loss: 3.9364899506036544\n",
      "train loss: 3.387989415929072\n",
      "=== epoch:9, train acc:0.28888888888888886, test acc:0.26666666666666666 ===\n",
      "train loss: 3.3735444158253767\n",
      "train loss: 3.216065175528665\n",
      "train loss: 3.496995817431863\n",
      "train loss: 3.3010842874916375\n",
      "train loss: 3.2769995400628016\n",
      "train loss: 3.5687568300042267\n",
      "train loss: 3.5568966056742086\n",
      "train loss: 2.9766981409752984\n",
      "train loss: 3.5974338712043794\n",
      "=== epoch:10, train acc:0.3333333333333333, test acc:0.3 ===\n",
      "train loss: 3.6928231691756226\n",
      "train loss: 3.3715019838838582\n",
      "train loss: 3.4435520353039886\n",
      "train loss: 3.089351179135898\n",
      "train loss: 3.2009596981793678\n",
      "train loss: 3.520597031626211\n",
      "train loss: 3.539881305748103\n",
      "train loss: 3.2222946951889107\n",
      "train loss: 3.4171783872358255\n",
      "=== epoch:11, train acc:0.3333333333333333, test acc:0.3 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 3.367533037090299\n",
      "train loss: 3.437448083450786\n",
      "train loss: 3.1927238286711694\n",
      "train loss: 3.221513870903616\n",
      "train loss: 3.2093601619425445\n",
      "train loss: 3.2956866569399095\n",
      "train loss: 3.450524240149499\n",
      "train loss: 3.1145915401838145\n",
      "train loss: 2.928843439824039\n",
      "=== epoch:12, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.135741971689355\n",
      "train loss: 2.9811340766165197\n",
      "train loss: 3.5395263133493726\n",
      "train loss: 3.4232220969905938\n",
      "train loss: 3.536216279124157\n",
      "train loss: 3.563260044159279\n",
      "train loss: 3.452793583490052\n",
      "train loss: 3.3078731204496297\n",
      "train loss: 3.107066947005971\n",
      "=== epoch:13, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.539967424208713\n",
      "train loss: 3.1383432172700148\n",
      "train loss: 3.139358505158576\n",
      "train loss: 2.72343381753147\n",
      "train loss: 3.247069992520824\n",
      "train loss: 3.6320024980003662\n",
      "train loss: 3.5447695205162937\n",
      "train loss: 3.0944705971106345\n",
      "train loss: 3.267723557440418\n",
      "=== epoch:14, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.362178857720505\n",
      "train loss: 3.719962983961425\n",
      "train loss: 2.9686087091418583\n",
      "train loss: 3.500418394110231\n",
      "train loss: 3.1942728617563882\n",
      "train loss: 2.9060485619924314\n",
      "train loss: 2.8401707173240327\n",
      "train loss: 3.017681953062584\n",
      "train loss: 3.10759333642094\n",
      "=== epoch:15, train acc:0.3333333333333333, test acc:0.26666666666666666 ===\n",
      "train loss: 3.1384928967906314\n",
      "train loss: 3.185116139542494\n",
      "train loss: 3.4124981970161343\n",
      "train loss: 3.452072796506655\n",
      "train loss: 3.2841229775606355\n",
      "train loss: 3.3769399713362196\n",
      "train loss: 3.411032794861266\n",
      "train loss: 2.946882946278908\n",
      "train loss: 2.8379261239341402\n",
      "=== epoch:16, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.5678198782528106\n",
      "train loss: 3.631583490650837\n",
      "train loss: 3.3810047615304977\n",
      "train loss: 2.8665635383465182\n",
      "train loss: 3.5548598192787177\n",
      "train loss: 3.2033722737201398\n",
      "train loss: 3.1763828892607613\n",
      "train loss: 3.193976977715522\n",
      "train loss: 2.9565524309227262\n",
      "=== epoch:17, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.1692077007372026\n",
      "train loss: 3.771210062680608\n",
      "train loss: 2.9195867653781433\n",
      "train loss: 3.3266897413342265\n",
      "train loss: 2.731990960536763\n",
      "train loss: 2.983206843103918\n",
      "train loss: 3.219672464053785\n",
      "train loss: 3.2791264290867104\n",
      "train loss: 3.1803462816651242\n",
      "=== epoch:18, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.265750001953614\n",
      "train loss: 3.183551654346218\n",
      "train loss: 3.3256160427845938\n",
      "train loss: 2.7498654715576576\n",
      "train loss: 3.129821896540994\n",
      "train loss: 3.146910007445011\n",
      "train loss: 3.4531096225384394\n",
      "train loss: 2.9966472978365566\n",
      "train loss: 3.441146988033233\n",
      "=== epoch:19, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 2.917252926165853\n",
      "train loss: 2.7069521779531924\n",
      "train loss: 3.0425465489860364\n",
      "train loss: 3.089861460302569\n",
      "train loss: 3.3751510323567246\n",
      "train loss: 3.1618475914977853\n",
      "train loss: 3.3836224474710543\n",
      "train loss: 3.049722523273148\n",
      "train loss: 3.3136799974394853\n",
      "=== epoch:20, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.0667203251517314\n",
      "train loss: 3.266975729862525\n",
      "train loss: 3.3213763889131593\n",
      "train loss: 2.8349076074822936\n",
      "train loss: 3.0007606721217104\n",
      "train loss: 3.188168595110419\n",
      "train loss: 3.3287897648410065\n",
      "train loss: 3.0201560559198195\n",
      "train loss: 3.0578348328108276\n",
      "=== epoch:21, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 2.951032665059886\n",
      "train loss: 3.0551210409084932\n",
      "train loss: 3.117018224733373\n",
      "train loss: 2.896810250824506\n",
      "train loss: 3.0377881128206523\n",
      "train loss: 3.122286889142158\n",
      "train loss: 3.0141976271249633\n",
      "train loss: 3.304045728927497\n",
      "train loss: 2.7609263211653277\n",
      "=== epoch:22, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.061449806131358\n",
      "train loss: 3.0189215847473916\n",
      "train loss: 3.1478259790992853\n",
      "train loss: 2.755035923039588\n",
      "train loss: 3.163846858174522\n",
      "train loss: 2.744249110784787\n",
      "train loss: 2.5311458847278385\n",
      "train loss: 2.7537488935190457\n",
      "train loss: 3.181231728301076\n",
      "=== epoch:23, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 2.667484441388333\n",
      "train loss: 2.6227637306921503\n",
      "train loss: 3.2646364042673928\n",
      "train loss: 3.8752353605458687\n",
      "train loss: 3.208954038808138\n",
      "train loss: 3.635411883834723\n",
      "train loss: 3.408183953872739\n",
      "train loss: 2.7886494452099786\n",
      "train loss: 2.993798096029667\n",
      "=== epoch:24, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 2.621134344786645\n",
      "train loss: 3.3794899979080277\n",
      "train loss: 3.1118222367231465\n",
      "train loss: 2.995290525885828\n",
      "train loss: 3.506917892749597\n",
      "train loss: 3.3242152696783296\n",
      "train loss: 2.539937758632568\n",
      "train loss: 3.2706815859690126\n",
      "train loss: 2.971569900436853\n",
      "=== epoch:25, train acc:0.35555555555555557, test acc:0.3 ===\n",
      "train loss: 3.140980054807347\n",
      "train loss: 3.0014961297152616\n",
      "train loss: 3.2380062527139106\n",
      "train loss: 2.9826650977361338\n",
      "train loss: 2.976951546705205\n",
      "train loss: 3.1919380675590308\n",
      "train loss: 2.888036310602702\n",
      "train loss: 3.1883762306446815\n",
      "train loss: 3.080513823281973\n",
      "=== epoch:26, train acc:0.36666666666666664, test acc:0.3 ===\n",
      "train loss: 3.002250289829716\n",
      "train loss: 3.124831319829081\n",
      "train loss: 2.9989453876345413\n",
      "train loss: 3.008164262280925\n",
      "train loss: 3.0704028360251034\n",
      "train loss: 2.997234107680724\n",
      "train loss: 2.9759456059056553\n",
      "train loss: 2.978843509594327\n",
      "train loss: 2.853769342775232\n",
      "=== epoch:27, train acc:0.34444444444444444, test acc:0.3 ===\n",
      "train loss: 2.817974217981261\n",
      "train loss: 3.2668680096526557\n",
      "train loss: 3.066751956067516\n",
      "train loss: 2.844851214619114\n",
      "train loss: 3.2159779027578375\n",
      "train loss: 2.9847481998444985\n",
      "train loss: 3.211862103550928\n",
      "train loss: 3.1052203430884817\n",
      "train loss: 2.8351764649368087\n",
      "=== epoch:28, train acc:0.34444444444444444, test acc:0.3 ===\n",
      "train loss: 2.919626538112724\n",
      "train loss: 3.0988364250574287\n",
      "train loss: 3.041857297633597\n",
      "train loss: 2.9265371037273296\n",
      "train loss: 3.197323384894095\n",
      "train loss: 2.921524975382371\n",
      "train loss: 3.118469155279156\n",
      "train loss: 3.086438612786596\n",
      "train loss: 2.982963363968982\n",
      "=== epoch:29, train acc:0.6333333333333333, test acc:0.7 ===\n",
      "train loss: 2.9493835192849827\n",
      "train loss: 2.9153442303267543\n",
      "train loss: 2.9789286079563424\n",
      "train loss: 3.022592733983474\n",
      "train loss: 2.9406179218423953\n",
      "train loss: 2.996895344514797\n",
      "train loss: 3.0098758185071706\n",
      "train loss: 2.9829716665207617\n",
      "train loss: 2.9613827539821416\n",
      "=== epoch:30, train acc:0.6222222222222222, test acc:0.6666666666666666 ===\n",
      "train loss: 2.9065979108961515\n",
      "train loss: 2.9658484769110607\n",
      "train loss: 2.937594381987963\n",
      "train loss: 2.9593383802319737\n",
      "train loss: 2.9279102416279286\n",
      "train loss: 2.957078137971611\n",
      "train loss: 2.805106016266261\n",
      "train loss: 2.9830953038473655\n",
      "train loss: 2.6762669853666106\n",
      "=== epoch:31, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.56681973318525\n",
      "train loss: 3.220338031777235\n",
      "train loss: 2.9800188521516437\n",
      "train loss: 3.144933695395154\n",
      "train loss: 2.858636706421478\n",
      "train loss: 2.98493844544051\n",
      "train loss: 2.4006294120883243\n",
      "train loss: 2.710346586962342\n",
      "train loss: 2.828683025078871\n",
      "=== epoch:32, train acc:0.5666666666666667, test acc:0.5 ===\n",
      "train loss: 3.065312668228976\n",
      "train loss: 2.5030911884959095\n",
      "train loss: 3.2540530880947696\n",
      "train loss: 3.0675690611025717\n",
      "train loss: 2.904646259974717\n",
      "train loss: 2.8075139003701186\n",
      "train loss: 2.860852533497762\n",
      "train loss: 2.733261836892093\n",
      "train loss: 3.12005311849299\n",
      "=== epoch:33, train acc:0.4, test acc:0.36666666666666664 ===\n",
      "train loss: 3.0339088866404404\n",
      "train loss: 3.1814801861765747\n",
      "train loss: 2.864293415780375\n",
      "train loss: 3.220771155027895\n",
      "train loss: 2.9937839733810234\n",
      "train loss: 3.0767613634425297\n",
      "train loss: 2.734268448390524\n",
      "train loss: 2.885731442694808\n",
      "train loss: 2.7927460398544293\n",
      "=== epoch:34, train acc:0.5666666666666667, test acc:0.4666666666666667 ===\n",
      "train loss: 2.9836268332594313\n",
      "train loss: 2.8914851274350526\n",
      "train loss: 2.914857475457699\n",
      "train loss: 2.950755817516418\n",
      "train loss: 2.8852812665747942\n",
      "train loss: 2.8143656290918595\n",
      "train loss: 2.888643940353095\n",
      "train loss: 2.932035690725252\n",
      "train loss: 2.8871672137827034\n",
      "=== epoch:35, train acc:0.6333333333333333, test acc:0.7 ===\n",
      "train loss: 2.8572941437807096\n",
      "train loss: 2.937447322747528\n",
      "train loss: 2.80326061504851\n",
      "train loss: 2.8710983802874934\n",
      "train loss: 2.8224656519556306\n",
      "train loss: 2.9346374164035884\n",
      "train loss: 2.827326563668854\n",
      "train loss: 2.7831100868717047\n",
      "train loss: 2.7668077376318814\n",
      "=== epoch:36, train acc:0.6444444444444445, test acc:0.7 ===\n",
      "train loss: 2.8821504938215026\n",
      "train loss: 2.8762829786216333\n",
      "train loss: 2.8808770542000732\n",
      "train loss: 2.8172280794831774\n",
      "train loss: 2.813451561917555\n",
      "train loss: 2.8314993453039867\n",
      "train loss: 2.8278925136459527\n",
      "train loss: 2.7813925338597913\n",
      "train loss: 2.76620497500585\n",
      "=== epoch:37, train acc:0.6444444444444445, test acc:0.6666666666666666 ===\n",
      "train loss: 2.814694287604637\n",
      "train loss: 2.7833579060725357\n",
      "train loss: 2.8250401117594275\n",
      "train loss: 2.9784162719902607\n",
      "train loss: 2.7195348897848395\n",
      "train loss: 2.741407960726923\n",
      "train loss: 2.8962197156787406\n",
      "train loss: 2.6443462314700104\n",
      "train loss: 2.7325840342924677\n",
      "=== epoch:38, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.709721682594878\n",
      "train loss: 2.6410704801319596\n",
      "train loss: 2.727447847186215\n",
      "train loss: 3.1358399865354483\n",
      "train loss: 2.5542241869869997\n",
      "train loss: 2.8122085167214097\n",
      "train loss: 2.9439617166218697\n",
      "train loss: 2.666140226867611\n",
      "train loss: 2.637856403361141\n",
      "=== epoch:39, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.3785077373827845\n",
      "train loss: 2.4642108609102142\n",
      "train loss: 2.969439496570715\n",
      "train loss: 2.2774617210436636\n",
      "train loss: 2.846270014385399\n",
      "train loss: 2.6281937119860146\n",
      "train loss: 2.8965632859953123\n",
      "train loss: 2.6139313302542777\n",
      "train loss: 2.044153687136045\n",
      "=== epoch:40, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.6436386708961392\n",
      "train loss: 2.7250630627281307\n",
      "train loss: 2.7355454993322517\n",
      "train loss: 2.2984622154451584\n",
      "train loss: 3.12559608531487\n",
      "train loss: 2.5272096052723945\n",
      "train loss: 2.3742482516362813\n",
      "train loss: 2.846985606933298\n",
      "train loss: 3.1459654702299247\n",
      "=== epoch:41, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.998113513969038\n",
      "train loss: 2.883077175664451\n",
      "train loss: 3.1541126642491704\n",
      "train loss: 2.199164303821174\n",
      "train loss: 2.6537276200358124\n",
      "train loss: 2.5514041936594176\n",
      "train loss: 2.5600485332437\n",
      "train loss: 3.2558889325998255\n",
      "train loss: 2.36182200215972\n",
      "=== epoch:42, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.2361021556094487\n",
      "train loss: 2.6984686029816447\n",
      "train loss: 2.421894130688025\n",
      "train loss: 2.4623654749473007\n",
      "train loss: 3.121243646345853\n",
      "train loss: 2.8590983131115872\n",
      "train loss: 2.6575856792880472\n",
      "train loss: 2.570640825313892\n",
      "train loss: 2.754336065178519\n",
      "=== epoch:43, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 3.059406526111409\n",
      "train loss: 2.9109553812942512\n",
      "train loss: 2.7520055067186737\n",
      "train loss: 2.8569294832526047\n",
      "train loss: 2.6681566236933842\n",
      "train loss: 2.8796456434646185\n",
      "train loss: 2.6504453482496233\n",
      "train loss: 2.7006187360047065\n",
      "train loss: 2.7296571977520174\n",
      "=== epoch:44, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.5136890942063177\n",
      "train loss: 2.5270454886140863\n",
      "train loss: 2.531518135390141\n",
      "train loss: 2.517496056417969\n",
      "train loss: 2.7053351768145846\n",
      "train loss: 2.713383138265733\n",
      "train loss: 2.5623708291976683\n",
      "train loss: 2.4047915074252275\n",
      "train loss: 2.52603356379037\n",
      "=== epoch:45, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.7557654146212305\n",
      "train loss: 2.635186120590236\n",
      "train loss: 2.6992151177895987\n",
      "train loss: 2.639560348117566\n",
      "train loss: 2.638134986594295\n",
      "train loss: 2.728509206108086\n",
      "train loss: 2.8337887452251516\n",
      "train loss: 2.48593377322582\n",
      "train loss: 2.4537489420087657\n",
      "=== epoch:46, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.6693632534878295\n",
      "train loss: 2.5857495132579387\n",
      "train loss: 2.564677386166224\n",
      "train loss: 2.5533993048743713\n",
      "train loss: 3.0037489179726498\n",
      "train loss: 2.556211522740174\n",
      "train loss: 2.5948671588864274\n",
      "train loss: 2.68426007103627\n",
      "train loss: 2.5279973862011724\n",
      "=== epoch:47, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.5291721307902226\n",
      "train loss: 2.4262517043018264\n",
      "train loss: 2.471068081428315\n",
      "train loss: 2.7601288299704767\n",
      "train loss: 2.3257744603489168\n",
      "train loss: 2.5512862498658144\n",
      "train loss: 2.4058218116432095\n",
      "train loss: 2.8271647241033486\n",
      "train loss: 2.3015638298631145\n",
      "=== epoch:48, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.897311055485563\n",
      "train loss: 2.8326357372305515\n",
      "train loss: 2.774522916073569\n",
      "train loss: 2.6163867754846373\n",
      "train loss: 2.6251895616644814\n",
      "train loss: 2.8424121723016893\n",
      "train loss: 2.7278088169731967\n",
      "train loss: 2.7237842390253957\n",
      "train loss: 2.53347551637887\n",
      "=== epoch:49, train acc:0.6666666666666666, test acc:0.6666666666666666 ===\n",
      "train loss: 2.5671248788541496\n",
      "train loss: 2.47747725268211\n",
      "train loss: 2.4843206808550415\n",
      "train loss: 2.4156458943836547\n",
      "train loss: 2.290031025970082\n",
      "train loss: 2.498329486852188\n",
      "train loss: 2.3533886625618488\n",
      "train loss: 2.347245185735538\n",
      "train loss: 2.4937957340317243\n",
      "=== epoch:50, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.6099356673661935\n",
      "train loss: 2.4418082301384354\n",
      "train loss: 2.5063334097859467\n",
      "train loss: 2.8686541335517224\n",
      "train loss: 2.5780091338643105\n",
      "train loss: 2.658303529788585\n",
      "train loss: 2.2773950637156606\n",
      "train loss: 2.3794781634999187\n",
      "train loss: 2.512005548213496\n",
      "=== epoch:51, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.547727947057188\n",
      "train loss: 2.843982413191744\n",
      "train loss: 2.519809459955718\n",
      "train loss: 2.2962069054263945\n",
      "train loss: 2.636630904758558\n",
      "train loss: 2.217843546692717\n",
      "train loss: 2.338180914396947\n",
      "train loss: 2.460776374934417\n",
      "train loss: 2.3410942750831483\n",
      "=== epoch:52, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.757314633305507\n",
      "train loss: 2.062642611146556\n",
      "train loss: 2.5168859466372857\n",
      "train loss: 2.2593111188640265\n",
      "train loss: 2.587691309486472\n",
      "train loss: 2.613111982039197\n",
      "train loss: 2.4274364093214063\n",
      "train loss: 2.8505536961953437\n",
      "train loss: 2.373007396687234\n",
      "=== epoch:53, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.7580406257668324\n",
      "train loss: 2.70512112212205\n",
      "train loss: 2.467020885003719\n",
      "train loss: 2.623204958511741\n",
      "train loss: 2.4327856567593873\n",
      "train loss: 2.2890462565906655\n",
      "train loss: 2.4738062449310965\n",
      "train loss: 2.5553631270548105\n",
      "train loss: 2.2733342014665494\n",
      "=== epoch:54, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.6433190224850676\n",
      "train loss: 2.3362755701468054\n",
      "train loss: 2.664043494597123\n",
      "train loss: 2.4951935295189216\n",
      "train loss: 2.232536025550308\n",
      "train loss: 2.4122646439082116\n",
      "train loss: 2.2746289224132346\n",
      "train loss: 2.3775524277437405\n",
      "train loss: 2.7222782229908744\n",
      "=== epoch:55, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.4271463184613475\n",
      "train loss: 2.5113144807037906\n",
      "train loss: 2.376163005835206\n",
      "train loss: 2.6552327177199544\n",
      "train loss: 2.227020886599534\n",
      "train loss: 2.439381393999134\n",
      "train loss: 2.194133985679759\n",
      "train loss: 2.875463205055037\n",
      "train loss: 2.222731754088794\n",
      "=== epoch:56, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.3901969576072113\n",
      "train loss: 2.3379942679986123\n",
      "train loss: 2.5737205514785515\n",
      "train loss: 2.176865417757007\n",
      "train loss: 2.933761825222603\n",
      "train loss: 2.589756100915681\n",
      "train loss: 2.4998340704376556\n",
      "train loss: 2.2369137370560983\n",
      "train loss: 2.17386320351353\n",
      "=== epoch:57, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.3900689491993665\n",
      "train loss: 2.8620789694772237\n",
      "train loss: 2.25249368857475\n",
      "train loss: 2.2188155932830544\n",
      "train loss: 2.5152889099511717\n",
      "train loss: 2.2580130029368175\n",
      "train loss: 2.4186459614283944\n",
      "train loss: 2.1111515206130678\n",
      "train loss: 2.0039188931500123\n",
      "=== epoch:58, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.216042170807916\n",
      "train loss: 1.9546062039542527\n",
      "train loss: 2.430197895888977\n",
      "train loss: 2.366607263756752\n",
      "train loss: 2.721883575975854\n",
      "train loss: 2.531330234958417\n",
      "train loss: 2.697998818410537\n",
      "train loss: 1.90718045212847\n",
      "train loss: 2.4597023216891185\n",
      "=== epoch:59, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.1489448862180573\n",
      "train loss: 2.4764044831213368\n",
      "train loss: 2.3675611045262133\n",
      "train loss: 2.4768112297884475\n",
      "train loss: 2.3235499236897494\n",
      "train loss: 2.128218737355972\n",
      "train loss: 2.2482584155101155\n",
      "train loss: 2.4720382764479822\n",
      "train loss: 2.62688104157543\n",
      "=== epoch:60, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.472239281117584\n",
      "train loss: 2.4926212924912425\n",
      "train loss: 2.457954413216104\n",
      "train loss: 2.2463987390253037\n",
      "train loss: 2.1554713481686325\n",
      "train loss: 2.114100125970103\n",
      "train loss: 2.4715315416979946\n",
      "train loss: 2.2843948418133015\n",
      "train loss: 2.4667251215925163\n",
      "=== epoch:61, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.020663586885174\n",
      "train loss: 2.3583586554194054\n",
      "train loss: 2.0930346320293056\n",
      "train loss: 2.023235885420059\n",
      "train loss: 2.394655718479406\n",
      "train loss: 1.9737357219147227\n",
      "train loss: 2.233616223937086\n",
      "train loss: 2.584059780643428\n",
      "train loss: 2.1264577366150372\n",
      "=== epoch:62, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.334194837493049\n",
      "train loss: 2.6745039062881\n",
      "train loss: 1.9625488376183713\n",
      "train loss: 1.9662715562837998\n",
      "train loss: 2.2795842701650373\n",
      "train loss: 2.666192783658498\n",
      "train loss: 2.4581940427286653\n",
      "train loss: 2.431957508228362\n",
      "train loss: 1.7688820974624664\n",
      "=== epoch:63, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.902493669499072\n",
      "train loss: 2.0210924763748004\n",
      "train loss: 1.8498890964944374\n",
      "train loss: 2.4420852585126434\n",
      "train loss: 2.9179477778961678\n",
      "train loss: 1.9985444374527446\n",
      "train loss: 2.0108395367627883\n",
      "train loss: 1.7026034505902239\n",
      "train loss: 2.0932005351212144\n",
      "=== epoch:64, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.5284342996426608\n",
      "train loss: 2.3042972554073384\n",
      "train loss: 2.037167427679094\n",
      "train loss: 2.2437538084960793\n",
      "train loss: 2.179278880905304\n",
      "train loss: 2.4669925906547294\n",
      "train loss: 1.994003915328877\n",
      "train loss: 2.3297309918980247\n",
      "train loss: 1.947449001027531\n",
      "=== epoch:65, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.9574785465987157\n",
      "train loss: 2.1655583162883416\n",
      "train loss: 2.0015582636081377\n",
      "train loss: 2.7859353005811913\n",
      "train loss: 2.2782449779686056\n",
      "train loss: 2.4859536815016314\n",
      "train loss: 2.555569948354099\n",
      "train loss: 1.950196106869362\n",
      "train loss: 2.0148075237268186\n",
      "=== epoch:66, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.1190521152168293\n",
      "train loss: 1.6060838644116167\n",
      "train loss: 2.2561952280190125\n",
      "train loss: 2.1453375316393575\n",
      "train loss: 1.9771474385179153\n",
      "train loss: 1.9712521037889696\n",
      "train loss: 2.694298513715868\n",
      "train loss: 2.4157883447860553\n",
      "train loss: 2.4279733751285835\n",
      "=== epoch:67, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.204069923020529\n",
      "train loss: 2.1859672037412006\n",
      "train loss: 1.8711922445948705\n",
      "train loss: 2.1572937786300166\n",
      "train loss: 1.9371240088779134\n",
      "train loss: 2.1001193706043813\n",
      "train loss: 2.1242637749310886\n",
      "train loss: 2.650982152257036\n",
      "train loss: 2.676316378525477\n",
      "=== epoch:68, train acc:0.6666666666666666, test acc:0.7 ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.228066839868448\n",
      "train loss: 2.0701517026115357\n",
      "train loss: 2.067506541242998\n",
      "train loss: 2.2843946386352467\n",
      "train loss: 2.2257233698303924\n",
      "train loss: 2.862773278503085\n",
      "train loss: 2.349272004222945\n",
      "train loss: 1.7645852957087516\n",
      "train loss: 2.5305581775825203\n",
      "=== epoch:69, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.9939766791741969\n",
      "train loss: 2.3574460876784977\n",
      "train loss: 1.9527646613144465\n",
      "train loss: 2.4944782834380517\n",
      "train loss: 1.9022171658787386\n",
      "train loss: 2.1177507072195274\n",
      "train loss: 2.265387681218716\n",
      "train loss: 1.936456875578134\n",
      "train loss: 2.226540385091985\n",
      "=== epoch:70, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.3078691089161376\n",
      "train loss: 2.155525793972581\n",
      "train loss: 2.6571871687223796\n",
      "train loss: 2.2132765174525884\n",
      "train loss: 2.5659285382847288\n",
      "train loss: 2.4886605611144947\n",
      "train loss: 2.209621360473104\n",
      "train loss: 1.8282720579204736\n",
      "train loss: 2.2752402708372235\n",
      "=== epoch:71, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.5379603169996363\n",
      "train loss: 1.8841020116220801\n",
      "train loss: 2.15418869874032\n",
      "train loss: 2.2068287358818592\n",
      "train loss: 1.9396310223235005\n",
      "train loss: 2.2495800021450028\n",
      "train loss: 2.217358579446038\n",
      "train loss: 2.4160013048100155\n",
      "train loss: 1.9227945717058943\n",
      "=== epoch:72, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.815693229592839\n",
      "train loss: 1.903785239777843\n",
      "train loss: 1.9851487999918012\n",
      "train loss: 2.9291787220258256\n",
      "train loss: 2.580712038502198\n",
      "train loss: 2.197551954453296\n",
      "train loss: 1.689573223258366\n",
      "train loss: 2.406612307042348\n",
      "train loss: 2.0940911609191026\n",
      "=== epoch:73, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.7335299307495673\n",
      "train loss: 2.3557290758630014\n",
      "train loss: 2.3974673549862384\n",
      "train loss: 2.15498020394369\n",
      "train loss: 2.6169064391700383\n",
      "train loss: 2.3987788482860637\n",
      "train loss: 1.3246971459744055\n",
      "train loss: 2.3485342241954315\n",
      "train loss: 1.7416666488747932\n",
      "=== epoch:74, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.0752527469317865\n",
      "train loss: 1.6378053601042497\n",
      "train loss: 2.6517214603491186\n",
      "train loss: 1.9077083978533058\n",
      "train loss: 2.3900097481346965\n",
      "train loss: 1.580087768768845\n",
      "train loss: 2.340292939715118\n",
      "train loss: 2.2215460016146857\n",
      "train loss: 1.810697005196062\n",
      "=== epoch:75, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.3505286595773986\n",
      "train loss: 2.194853366478743\n",
      "train loss: 1.9858976470002128\n",
      "train loss: 2.191627745206193\n",
      "train loss: 1.9513546135993616\n",
      "train loss: 1.7831326646539156\n",
      "train loss: 1.7516451161295201\n",
      "train loss: 2.2220101396742398\n",
      "train loss: 2.094516092846966\n",
      "=== epoch:76, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.3377056493539126\n",
      "train loss: 1.9216517442146044\n",
      "train loss: 2.0128618986685747\n",
      "train loss: 1.625626477912075\n",
      "train loss: 2.3829520707556684\n",
      "train loss: 2.459635819979375\n",
      "train loss: 1.9163452739564468\n",
      "train loss: 2.3300385892307625\n",
      "train loss: 1.7552623544732182\n",
      "=== epoch:77, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.402518180615369\n",
      "train loss: 1.9675099340066866\n",
      "train loss: 2.347783397733988\n",
      "train loss: 1.8586901442328685\n",
      "train loss: 1.9838895661075429\n",
      "train loss: 1.9751575649560886\n",
      "train loss: 2.4417954580397825\n",
      "train loss: 1.8230464229211474\n",
      "train loss: 2.326373407427967\n",
      "=== epoch:78, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.470142153753264\n",
      "train loss: 2.3303133409758976\n",
      "train loss: 2.272106594732348\n",
      "train loss: 1.9458821765187888\n",
      "train loss: 2.1300847364336826\n",
      "train loss: 2.0279103840792487\n",
      "train loss: 2.0900809891322463\n",
      "train loss: 2.3525678651348807\n",
      "train loss: 2.4057057309039074\n",
      "=== epoch:79, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.2619506753796497\n",
      "train loss: 1.985953766579726\n",
      "train loss: 2.183513375026953\n",
      "train loss: 2.171135239496353\n",
      "train loss: 1.9748478051220701\n",
      "train loss: 2.470486382741203\n",
      "train loss: 2.6628975181394337\n",
      "train loss: 2.2330842135379383\n",
      "train loss: 2.206106750428821\n",
      "=== epoch:80, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.8363739216051043\n",
      "train loss: 1.852292560340931\n",
      "train loss: 2.3388146096450195\n",
      "train loss: 2.241820538891623\n",
      "train loss: 2.5070463399030825\n",
      "train loss: 2.0139801385813763\n",
      "train loss: 2.2474562991139453\n",
      "train loss: 2.546578925610152\n",
      "train loss: 2.059915998832982\n",
      "=== epoch:81, train acc:0.7222222222222222, test acc:0.7333333333333333 ===\n",
      "train loss: 2.248285643815622\n",
      "train loss: 1.7997210035938673\n",
      "train loss: 1.7625140290246437\n",
      "train loss: 2.152201843250343\n",
      "train loss: 2.094502219444507\n",
      "train loss: 1.8042347945662516\n",
      "train loss: 2.0926219696882256\n",
      "train loss: 1.970647378789106\n",
      "train loss: 2.231996225648394\n",
      "=== epoch:82, train acc:0.6888888888888889, test acc:0.7 ===\n",
      "train loss: 2.298973775290546\n",
      "train loss: 2.0352766630260417\n",
      "train loss: 2.2527736402070646\n",
      "train loss: 2.0977372526365095\n",
      "train loss: 1.8235618186920783\n",
      "train loss: 1.944193315164182\n",
      "train loss: 1.872043313439661\n",
      "train loss: 1.8537569504226292\n",
      "train loss: 1.8485868596307298\n",
      "=== epoch:83, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.8525849010491584\n",
      "train loss: 2.4206703775283342\n",
      "train loss: 1.254358469583754\n",
      "train loss: 2.3083692856666795\n",
      "train loss: 2.674836739802865\n",
      "train loss: 1.5417149925677782\n",
      "train loss: 2.3574798094466662\n",
      "train loss: 1.7747348205057927\n",
      "train loss: 1.9931920079825465\n",
      "=== epoch:84, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.375634280589485\n",
      "train loss: 2.3993804320731114\n",
      "train loss: 2.2261961159223067\n",
      "train loss: 2.381916857224903\n",
      "train loss: 2.4787985726785933\n",
      "train loss: 2.0039096737301425\n",
      "train loss: 1.9971303094403898\n",
      "train loss: 1.5744691220692384\n",
      "train loss: 2.4694666730798702\n",
      "=== epoch:85, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.5401893039556085\n",
      "train loss: 1.8040798010880175\n",
      "train loss: 2.3348748500296757\n",
      "train loss: 1.6925318761224797\n",
      "train loss: 2.290584149259211\n",
      "train loss: 2.4995105325712377\n",
      "train loss: 2.094477981413721\n",
      "train loss: 2.0552840981698677\n",
      "train loss: 1.7996231647831622\n",
      "=== epoch:86, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.1736396718302675\n",
      "train loss: 2.295167613873834\n",
      "train loss: 2.9465581831943837\n",
      "train loss: 1.8848739603301556\n",
      "train loss: 1.7894293266692685\n",
      "train loss: 2.283688204122924\n",
      "train loss: 1.5598273642315124\n",
      "train loss: 2.2734252395438053\n",
      "train loss: 1.6378034277343836\n",
      "=== epoch:87, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.6339533050849573\n",
      "train loss: 2.1226610984451963\n",
      "train loss: 1.5384305982078448\n",
      "train loss: 2.10202410098069\n",
      "train loss: 1.997601820878693\n",
      "train loss: 3.0292808578485357\n",
      "train loss: 2.0867109602147482\n",
      "train loss: 2.413934636457318\n",
      "train loss: 1.7371604556529632\n",
      "=== epoch:88, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.6699750332465353\n",
      "train loss: 1.5495496900521228\n",
      "train loss: 2.096565574197786\n",
      "train loss: 1.7401861971758132\n",
      "train loss: 2.6888492587174944\n",
      "train loss: 1.9424630631546302\n",
      "train loss: 2.316460850943053\n",
      "train loss: 2.153870561134267\n",
      "train loss: 2.5774171684917695\n",
      "=== epoch:89, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.209193788887864\n",
      "train loss: 1.7362574005358828\n",
      "train loss: 2.2045177621676606\n",
      "train loss: 1.9379681872345333\n",
      "train loss: 2.100004054793382\n",
      "train loss: 2.066657457817829\n",
      "train loss: 2.1126432386635323\n",
      "train loss: 1.9427954026394565\n",
      "train loss: 1.4839149215770535\n",
      "=== epoch:90, train acc:0.6888888888888889, test acc:0.7 ===\n",
      "train loss: 1.356640992067965\n",
      "train loss: 1.662619360392048\n",
      "train loss: 1.7414117304839132\n",
      "train loss: 1.7572561213183657\n",
      "train loss: 1.9172924127044197\n",
      "train loss: 1.8620339413697504\n",
      "train loss: 1.5068553042072563\n",
      "train loss: 2.163440717477872\n",
      "train loss: 2.2894857904413546\n",
      "=== epoch:91, train acc:0.6888888888888889, test acc:0.7 ===\n",
      "train loss: 2.4508264753107274\n",
      "train loss: 1.3801095924294677\n",
      "train loss: 2.636636613810086\n",
      "train loss: 2.106139344737906\n",
      "train loss: 1.6295729756601727\n",
      "train loss: 1.5722475202561672\n",
      "train loss: 1.30685183824276\n",
      "train loss: 2.261621304792687\n",
      "train loss: 1.8276725767964965\n",
      "=== epoch:92, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.8481915825579334\n",
      "train loss: 1.9850154104319797\n",
      "train loss: 1.3664441143530606\n",
      "train loss: 1.9876936135470316\n",
      "train loss: 1.6824533110994317\n",
      "train loss: 2.2860678952783418\n",
      "train loss: 2.2331975779919535\n",
      "train loss: 1.8581897025033163\n",
      "train loss: 1.1072117739381895\n",
      "=== epoch:93, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.6470990136251216\n",
      "train loss: 2.2176390748249637\n",
      "train loss: 2.077584559016751\n",
      "train loss: 1.755295851357155\n",
      "train loss: 2.2975958249574675\n",
      "train loss: 2.1349824572626086\n",
      "train loss: 2.1080680097773214\n",
      "train loss: 1.9104909236297285\n",
      "train loss: 2.5730216022900967\n",
      "=== epoch:94, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.8638362094900662\n",
      "train loss: 1.7780711121943285\n",
      "train loss: 2.277926072670189\n",
      "train loss: 2.079938869094119\n",
      "train loss: 1.2829626755398809\n",
      "train loss: 1.9226891453326456\n",
      "train loss: 2.2357691849815917\n",
      "train loss: 2.0303836477503494\n",
      "train loss: 1.9421933111690386\n",
      "=== epoch:95, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 2.0687278961007247\n",
      "train loss: 2.156635686671078\n",
      "train loss: 2.7376284348627826\n",
      "train loss: 1.7403387448975383\n",
      "train loss: 2.154887835338589\n",
      "train loss: 1.552932521157576\n",
      "train loss: 1.7364325127377267\n",
      "train loss: 1.5719714730635623\n",
      "train loss: 1.7860219062833123\n",
      "=== epoch:96, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.6066990964020518\n",
      "train loss: 1.7169610431257845\n",
      "train loss: 1.8175468573788773\n",
      "train loss: 2.102169194118664\n",
      "train loss: 1.5005552792538541\n",
      "train loss: 2.747967188869764\n",
      "train loss: 1.6169817067393064\n",
      "train loss: 1.2534301103996566\n",
      "train loss: 1.9567049261541725\n",
      "=== epoch:97, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.9909802471099556\n",
      "train loss: 1.583443263380372\n",
      "train loss: 1.7958217128543843\n",
      "train loss: 2.3751412821626823\n",
      "train loss: 2.513699841995934\n",
      "train loss: 1.874140220048215\n",
      "train loss: 1.6558579906137416\n",
      "train loss: 2.4441362772851116\n",
      "train loss: 2.048404251736167\n",
      "=== epoch:98, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.982911092764945\n",
      "train loss: 1.4644832402468244\n",
      "train loss: 2.273916725630956\n",
      "train loss: 2.1789201180796645\n",
      "train loss: 1.9431328949097546\n",
      "train loss: 2.5347275789841954\n",
      "train loss: 2.046057641957837\n",
      "train loss: 2.6520645049884077\n",
      "train loss: 1.747966632327755\n",
      "=== epoch:99, train acc:0.6666666666666666, test acc:0.7 ===\n",
      "train loss: 1.8938541535431035\n",
      "train loss: 1.8410609604393258\n",
      "train loss: 1.999716115471682\n",
      "train loss: 2.3259229419968293\n",
      "train loss: 2.305208545882188\n",
      "train loss: 2.3330780636994635\n",
      "train loss: 1.7838776592796752\n",
      "train loss: 2.222193568087907\n",
      "train loss: 1.5609246590647436\n",
      "=== epoch:100, train acc:0.8111111111111111, test acc:0.8333333333333334 ===\n",
      "train loss: 2.3523196069963923\n",
      "train loss: 1.710247671313454\n",
      "train loss: 1.7021487827583803\n",
      "train loss: 2.3180674322441828\n",
      "train loss: 2.1880538613988145\n",
      "train loss: 2.459522750318288\n",
      "train loss: 2.2640559241644644\n",
      "train loss: 1.9749626374538753\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.7333333333333333\n",
      "[4, 10, 3]\n",
      "train loss: 6.412313984795964\n",
      "=== epoch:1, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.729590772757814\n",
      "train loss: 2.8184140216006\n",
      "train loss: 4.586311526158829\n",
      "train loss: 6.410250692266607\n",
      "train loss: 3.758347454304656\n",
      "train loss: 5.483902059347855\n",
      "train loss: 7.25746481077868\n",
      "train loss: 7.391343357585088\n",
      "train loss: 5.451121416396455\n",
      "=== epoch:2, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 7.342441394793776\n",
      "train loss: 2.7675883660541443\n",
      "train loss: 4.573802171258616\n",
      "train loss: 5.44456490755129\n",
      "train loss: 5.43398366231595\n",
      "train loss: 6.357641528526328\n",
      "train loss: 4.5516976884746745\n",
      "train loss: 6.426101047244936\n",
      "train loss: 5.521543957679586\n",
      "=== epoch:3, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 6.486710909273425\n",
      "train loss: 5.380827660506341\n",
      "train loss: 5.322599443120766\n",
      "train loss: 8.070993362564696\n",
      "train loss: 7.167770422983732\n",
      "train loss: 6.081430325467991\n",
      "train loss: 5.332148070579165\n",
      "train loss: 4.456566724526701\n",
      "train loss: 6.096224553505426\n",
      "=== epoch:4, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 6.706643565290505\n",
      "train loss: 3.5856815427032878\n",
      "train loss: 4.542775966712585\n",
      "train loss: 6.1630718667210385\n",
      "train loss: 5.940461953784774\n",
      "train loss: 6.6901152726315365\n",
      "train loss: 4.416850419129675\n",
      "train loss: 5.8212752800269\n",
      "train loss: 7.089915854833157\n",
      "=== epoch:5, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 5.770036677643235\n",
      "train loss: 6.25370441115843\n",
      "train loss: 3.8252841143451977\n",
      "train loss: 2.6445060771103153\n",
      "train loss: 5.172022636430672\n",
      "train loss: 4.873584867840219\n",
      "train loss: 4.1644068404696855\n",
      "train loss: 3.4250773379440855\n",
      "train loss: 5.338753220316411\n",
      "=== epoch:6, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 5.636518252852983\n",
      "train loss: 7.375080170818053\n",
      "train loss: 5.058756424462416\n",
      "train loss: 5.220445167949492\n",
      "train loss: 5.787588416052236\n",
      "train loss: 5.368924424692345\n",
      "train loss: 3.661344430311882\n",
      "train loss: 6.495746521944824\n",
      "train loss: 3.975300591154627\n",
      "=== epoch:7, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.4573342614379317\n",
      "train loss: 5.8566162674464195\n",
      "train loss: 4.844806839838687\n",
      "train loss: 6.418223064489441\n",
      "train loss: 3.6700250348507666\n",
      "train loss: 2.898271744207635\n",
      "train loss: 5.2956343818678615\n",
      "train loss: 5.526774939213053\n",
      "train loss: 5.996113816177497\n",
      "=== epoch:8, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.7768988637141794\n",
      "train loss: 4.727643633777268\n",
      "train loss: 4.75880065167379\n",
      "train loss: 4.268888820377597\n",
      "train loss: 4.7131603581807715\n",
      "train loss: 6.299774661421946\n",
      "train loss: 5.76659270243267\n",
      "train loss: 4.732470377496755\n",
      "train loss: 5.973450356474658\n",
      "=== epoch:9, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 5.297481646998133\n",
      "train loss: 4.725265287329541\n",
      "train loss: 4.157680517811118\n",
      "train loss: 6.230080696556547\n",
      "train loss: 5.432040191911737\n",
      "train loss: 6.462601097278075\n",
      "train loss: 3.2236954766320225\n",
      "train loss: 3.4055371850118137\n",
      "train loss: 5.944913574656674\n",
      "=== epoch:10, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 2.1941645518648882\n",
      "train loss: 3.6628032754791016\n",
      "train loss: 5.435245395054271\n",
      "train loss: 5.182570385258555\n",
      "train loss: 4.048841266414221\n",
      "train loss: 6.395347263085392\n",
      "train loss: 4.655307152182136\n",
      "train loss: 4.885260360699164\n",
      "train loss: 3.893878778872463\n",
      "=== epoch:11, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 4.572190301930984\n",
      "train loss: 4.169402597399896\n",
      "train loss: 6.5006466659857365\n",
      "train loss: 4.838187962002539\n",
      "train loss: 5.007816103713849\n",
      "train loss: 3.7030146509114292\n",
      "train loss: 5.35782053204905\n",
      "train loss: 5.439642384970145\n",
      "train loss: 5.12193752146391\n",
      "=== epoch:12, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 6.097588569724156\n",
      "train loss: 6.04157250049299\n",
      "train loss: 4.6631856050423615\n",
      "train loss: 5.792136919009389\n",
      "train loss: 5.751152506677406\n",
      "train loss: 5.762214583068436\n",
      "train loss: 6.8103429683540115\n",
      "train loss: 4.825538746226807\n",
      "train loss: 3.3467948363496904\n",
      "=== epoch:13, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.595007315408362\n",
      "train loss: 3.0531335558676056\n",
      "train loss: 1.6661615567736638\n",
      "train loss: 3.516679090425125\n",
      "train loss: 4.181327824031163\n",
      "train loss: 2.805175581317834\n",
      "train loss: 4.875333096934183\n",
      "train loss: 4.271776694371058\n",
      "train loss: 4.41064279142345\n",
      "=== epoch:14, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 5.0982145669506265\n",
      "train loss: 4.589134828018105\n",
      "train loss: 5.686443073156759\n",
      "train loss: 5.400135761570725\n",
      "train loss: 5.378773418894131\n",
      "train loss: 5.642263495585838\n",
      "train loss: 3.564145265188825\n",
      "train loss: 5.082483623244801\n",
      "train loss: 4.031408705062532\n",
      "=== epoch:15, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 4.27866575892207\n",
      "train loss: 5.636550710043382\n",
      "train loss: 4.50005797996056\n",
      "train loss: 3.333894288902615\n",
      "train loss: 5.15823301206245\n",
      "train loss: 5.8522403732003845\n",
      "train loss: 4.550348236957253\n",
      "train loss: 4.227121384515548\n",
      "train loss: 5.476893600658258\n",
      "=== epoch:16, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 6.604092939668156\n",
      "train loss: 6.400409154964243\n",
      "train loss: 3.981964490111848\n",
      "train loss: 4.753331515728363\n",
      "train loss: 5.075922191129091\n",
      "train loss: 3.344371538644449\n",
      "train loss: 3.676949235373907\n",
      "train loss: 4.9656250332594905\n",
      "train loss: 4.448130219062488\n",
      "=== epoch:17, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 4.478099601996708\n",
      "train loss: 5.376128644593709\n",
      "train loss: 2.8280917411077096\n",
      "train loss: 4.003041441642415\n",
      "train loss: 2.732433362757117\n",
      "train loss: 5.474668329708438\n",
      "train loss: 4.343400383718561\n",
      "train loss: 3.88777944500872\n",
      "train loss: 5.252769175806274\n",
      "=== epoch:18, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 5.4703415189313045\n",
      "train loss: 4.836935673355935\n",
      "train loss: 5.205095184240793\n",
      "train loss: 4.85501815826017\n",
      "train loss: 2.4997298641059182\n",
      "train loss: 4.654641163919221\n",
      "train loss: 5.091624011307865\n",
      "train loss: 5.453260249302894\n",
      "train loss: 3.8336083551395217\n",
      "=== epoch:19, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 4.374127944894552\n",
      "train loss: 4.588604635817383\n",
      "train loss: 3.869727083501001\n",
      "train loss: 5.12672396844629\n",
      "train loss: 4.689382426269869\n",
      "train loss: 5.261914855922212\n",
      "train loss: 5.129165143216197\n",
      "train loss: 5.290596031996039\n",
      "train loss: 4.7859281387696635\n",
      "=== epoch:20, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.991480302384637\n",
      "train loss: 4.2297473556197085\n",
      "train loss: 5.4605408055434665\n",
      "train loss: 2.809546143746491\n",
      "train loss: 4.644479729239013\n",
      "train loss: 4.185836949441653\n",
      "train loss: 5.5299800187536565\n",
      "train loss: 4.019610441968413\n",
      "train loss: 3.7427275394157693\n",
      "=== epoch:21, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.3405296680952317\n",
      "train loss: 3.371882237704669\n",
      "train loss: 4.648552392500228\n",
      "train loss: 3.7100001058607575\n",
      "train loss: 3.299642565074131\n",
      "train loss: 5.189184363047981\n",
      "train loss: 3.110008120135458\n",
      "train loss: 2.396889972318505\n",
      "train loss: 3.149966317045732\n",
      "=== epoch:22, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 4.717124344255415\n",
      "train loss: 4.699861267042236\n",
      "train loss: 4.464988795240581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 4.085668439550658\n",
      "train loss: 3.5915017974122017\n",
      "train loss: 3.0981320939268406\n",
      "train loss: 3.6167460598962466\n",
      "train loss: 3.094650513857144\n",
      "train loss: 4.50283438762785\n",
      "=== epoch:23, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 4.066669623957529\n",
      "train loss: 5.421253744721167\n",
      "train loss: 4.066529157614315\n",
      "train loss: 3.0433009503571844\n",
      "train loss: 3.082108918545182\n",
      "train loss: 4.488054243004507\n",
      "train loss: 3.4457060863796958\n",
      "train loss: 3.0195956713162833\n",
      "train loss: 2.5703297547730553\n",
      "=== epoch:24, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 2.571201012517285\n",
      "train loss: 3.4577425640419976\n",
      "train loss: 2.9267531694504045\n",
      "train loss: 5.215113708310211\n",
      "train loss: 4.359700407406258\n",
      "train loss: 4.344960485086465\n",
      "train loss: 2.362493239419203\n",
      "train loss: 5.633973477883242\n",
      "train loss: 4.147499160740513\n",
      "=== epoch:25, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.2349270678357582\n",
      "train loss: 4.575861871634752\n",
      "train loss: 2.8878512607922313\n",
      "train loss: 4.478198137286981\n",
      "train loss: 3.260185659573522\n",
      "train loss: 3.629987264738409\n",
      "train loss: 3.527549879457957\n",
      "train loss: 3.9544006112917165\n",
      "train loss: 3.8103933635977016\n",
      "=== epoch:26, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 3.7290441283941878\n",
      "train loss: 3.7606198930300554\n",
      "train loss: 3.6469814569591374\n",
      "train loss: 3.8599525469812725\n",
      "train loss: 3.252013568543628\n",
      "train loss: 3.9933913191470114\n",
      "train loss: 3.3387336169301545\n",
      "train loss: 2.7205754997970346\n",
      "train loss: 3.182052110966419\n",
      "=== epoch:27, train acc:0.37777777777777777, test acc:0.26666666666666666 ===\n",
      "train loss: 2.8886756998097267\n",
      "train loss: 3.4569705677151945\n",
      "train loss: 3.1221579964072133\n",
      "train loss: 3.038141755673414\n",
      "train loss: 2.828373241426191\n",
      "train loss: 3.178184979331836\n",
      "train loss: 2.7878101752764186\n",
      "train loss: 2.8465686253549842\n",
      "train loss: 3.329247091120994\n",
      "=== epoch:28, train acc:0.5333333333333333, test acc:0.4 ===\n",
      "train loss: 2.6272159780682838\n",
      "train loss: 2.79276612571726\n",
      "train loss: 2.5345558902411542\n",
      "train loss: 2.5231962003464967\n",
      "train loss: 3.212982700586666\n",
      "train loss: 2.329058425779824\n",
      "train loss: 2.469795978219489\n",
      "train loss: 2.748118547420308\n",
      "train loss: 2.909186724732085\n",
      "=== epoch:29, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.8003986844745374\n",
      "train loss: 2.216082747701246\n",
      "train loss: 2.536276645358677\n",
      "train loss: 3.1656408848509643\n",
      "train loss: 2.6692665025083615\n",
      "train loss: 2.513074275507891\n",
      "train loss: 2.930585552070764\n",
      "train loss: 3.1096137120522966\n",
      "train loss: 2.1211287666525607\n",
      "=== epoch:30, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.7245319789503313\n",
      "train loss: 3.0570079073181806\n",
      "train loss: 3.0638814131016288\n",
      "train loss: 2.1460880035082774\n",
      "train loss: 2.7503257648669694\n",
      "train loss: 2.7412320342981875\n",
      "train loss: 2.9321171932218593\n",
      "train loss: 2.66400665701359\n",
      "train loss: 2.752203067147762\n",
      "=== epoch:31, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.8721031291124692\n",
      "train loss: 2.8575873895970156\n",
      "train loss: 2.6729839618849933\n",
      "train loss: 3.273436202714443\n",
      "train loss: 2.590396181383722\n",
      "train loss: 2.597969313629859\n",
      "train loss: 2.5550042676396814\n",
      "train loss: 2.9112195048829625\n",
      "train loss: 2.8680501483415703\n",
      "=== epoch:32, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.414282881632948\n",
      "train loss: 2.92060695562457\n",
      "train loss: 2.874799402812533\n",
      "train loss: 2.4719329250682636\n",
      "train loss: 2.615992893710552\n",
      "train loss: 2.5984533811833384\n",
      "train loss: 2.7091813503285738\n",
      "train loss: 2.6498455236565834\n",
      "train loss: 2.8174301348490665\n",
      "=== epoch:33, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.7354422713724134\n",
      "train loss: 2.7757732918197924\n",
      "train loss: 2.586487405068963\n",
      "train loss: 3.05225302924999\n",
      "train loss: 2.772426928718926\n",
      "train loss: 2.505964460830564\n",
      "train loss: 2.749936111110192\n",
      "train loss: 2.8796884705967103\n",
      "train loss: 2.9124211583865423\n",
      "=== epoch:34, train acc:0.37777777777777777, test acc:0.23333333333333334 ===\n",
      "train loss: 2.7040737529066723\n",
      "train loss: 3.1174998648264323\n",
      "train loss: 2.8431175540676827\n",
      "train loss: 3.017764006019782\n",
      "train loss: 2.610101771869017\n",
      "train loss: 2.6008825172577477\n",
      "train loss: 2.3634707485646884\n",
      "train loss: 2.6444534288065706\n",
      "train loss: 3.104589305283002\n",
      "=== epoch:35, train acc:0.4111111111111111, test acc:0.26666666666666666 ===\n",
      "train loss: 2.717056168568689\n",
      "train loss: 2.593954691612695\n",
      "train loss: 2.602673401490762\n",
      "train loss: 2.9929753061465223\n",
      "train loss: 2.8324703028721734\n",
      "train loss: 2.3401964776665696\n",
      "train loss: 2.6472404339024234\n",
      "train loss: 2.4511862889115172\n",
      "train loss: 2.576318908078765\n",
      "=== epoch:36, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 3.016344075554537\n",
      "train loss: 2.914959572212601\n",
      "train loss: 2.694917350828755\n",
      "train loss: 2.587480240136556\n",
      "train loss: 2.4856980605289185\n",
      "train loss: 2.4971989372148395\n",
      "train loss: 2.7606941089533508\n",
      "train loss: 2.959564397575382\n",
      "train loss: 2.7555738378267227\n",
      "=== epoch:37, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.643280223957107\n",
      "train loss: 1.9517381896272306\n",
      "train loss: 3.0749201811873985\n",
      "train loss: 2.7642132470507055\n",
      "train loss: 2.3912421010912253\n",
      "train loss: 3.04111825317349\n",
      "train loss: 2.586610313315057\n",
      "train loss: 2.862355356512168\n",
      "train loss: 3.0661462090632225\n",
      "=== epoch:38, train acc:0.36666666666666664, test acc:0.23333333333333334 ===\n",
      "train loss: 2.80313527867737\n",
      "train loss: 2.4535380935008804\n",
      "train loss: 2.2821868129853646\n",
      "train loss: 2.8991568887564423\n",
      "train loss: 2.4509800361065013\n",
      "train loss: 2.606275377046232\n",
      "train loss: 2.4913502251513604\n",
      "train loss: 2.6926519973725886\n",
      "train loss: 2.590171405216428\n",
      "=== epoch:39, train acc:0.3888888888888889, test acc:0.3333333333333333 ===\n",
      "train loss: 1.977582307692647\n",
      "train loss: 2.740010749534824\n",
      "train loss: 2.534122068360664\n",
      "train loss: 2.7838062003454356\n",
      "train loss: 2.821240200085237\n",
      "train loss: 2.688836515259482\n",
      "train loss: 2.4670321786105776\n",
      "train loss: 3.05980164924941\n",
      "train loss: 2.3406541210226917\n",
      "=== epoch:40, train acc:0.4666666666666667, test acc:0.4666666666666667 ===\n",
      "train loss: 2.8564738310851525\n",
      "train loss: 2.875319504045762\n",
      "train loss: 2.288606358058657\n",
      "train loss: 2.8577697216856968\n",
      "train loss: 3.0079816223515605\n",
      "train loss: 2.4376813146546157\n",
      "train loss: 2.876550811395986\n",
      "train loss: 2.6858926442858606\n",
      "train loss: 2.593337205180966\n",
      "=== epoch:41, train acc:0.4111111111111111, test acc:0.36666666666666664 ===\n",
      "train loss: 2.7729032139579317\n",
      "train loss: 2.736274467988337\n",
      "train loss: 2.159795324500103\n",
      "train loss: 2.8873907452209226\n",
      "train loss: 2.715322208632901\n",
      "train loss: 2.4720683400529695\n",
      "train loss: 2.7937422960030283\n",
      "train loss: 2.350766526582087\n",
      "train loss: 2.76256278298857\n",
      "=== epoch:42, train acc:0.4111111111111111, test acc:0.36666666666666664 ===\n",
      "train loss: 2.5586474251687314\n",
      "train loss: 2.6801309850247494\n",
      "train loss: 2.7869206254676864\n",
      "train loss: 2.7600585485705347\n",
      "train loss: 2.7964063447707663\n",
      "train loss: 2.8228000329549263\n",
      "train loss: 2.515345337593166\n",
      "train loss: 2.598113478063102\n",
      "train loss: 2.666187828894706\n",
      "=== epoch:43, train acc:0.5666666666666667, test acc:0.5333333333333333 ===\n",
      "train loss: 2.4331472885649523\n",
      "train loss: 2.744480648732811\n",
      "train loss: 2.7078086542143356\n",
      "train loss: 2.4089489079335946\n",
      "train loss: 2.4625587958428876\n",
      "train loss: 2.5885712754522823\n",
      "train loss: 2.7057785172886786\n",
      "train loss: 2.4822219942808634\n",
      "train loss: 2.724500655805723\n",
      "=== epoch:44, train acc:0.6, test acc:0.6 ===\n",
      "train loss: 2.642519899522733\n",
      "train loss: 2.5480894761245567\n",
      "train loss: 2.3092604787614435\n",
      "train loss: 2.6660068015010783\n",
      "train loss: 2.640452738386148\n",
      "train loss: 2.512101501015911\n",
      "train loss: 2.82427701665933\n",
      "train loss: 2.458017392517461\n",
      "train loss: 2.6593792290473006\n",
      "=== epoch:45, train acc:0.5222222222222223, test acc:0.5333333333333333 ===\n",
      "train loss: 2.511572995590338\n",
      "train loss: 2.453701467209172\n",
      "train loss: 2.5743831332256772\n",
      "train loss: 2.642992764107286\n",
      "train loss: 2.6687592506683186\n",
      "train loss: 2.206265897867133\n",
      "train loss: 2.2786412422733977\n",
      "train loss: 2.325862218800349\n",
      "train loss: 2.1183586582753913\n",
      "=== epoch:46, train acc:0.5555555555555556, test acc:0.6333333333333333 ===\n",
      "train loss: 2.4390050653082103\n",
      "train loss: 2.5968334779765154\n",
      "train loss: 2.6630185086772244\n",
      "train loss: 2.506728927414773\n",
      "train loss: 2.310471397043991\n",
      "train loss: 2.713141365046469\n",
      "train loss: 2.3265829955363757\n",
      "train loss: 2.153406453362417\n",
      "train loss: 2.506349315892261\n",
      "=== epoch:47, train acc:0.5777777777777777, test acc:0.6666666666666666 ===\n",
      "train loss: 2.7868075009109337\n",
      "train loss: 2.5594570356224646\n",
      "train loss: 2.2161127155073714\n",
      "train loss: 2.5312419099418104\n",
      "train loss: 2.684691931520127\n",
      "train loss: 2.5345520078519193\n",
      "train loss: 3.070186067435807\n",
      "train loss: 2.4910304575543165\n",
      "train loss: 2.346158470507881\n",
      "=== epoch:48, train acc:0.5666666666666667, test acc:0.6333333333333333 ===\n",
      "train loss: 2.57524874032631\n",
      "train loss: 2.347608496568745\n",
      "train loss: 2.577049657323384\n",
      "train loss: 2.4134689626617982\n",
      "train loss: 2.279283430794765\n",
      "train loss: 2.6369261220607223\n",
      "train loss: 2.7511049053735457\n",
      "train loss: 2.6036867266607877\n",
      "train loss: 2.495037659130082\n",
      "=== epoch:49, train acc:0.5333333333333333, test acc:0.5666666666666667 ===\n",
      "train loss: 2.4515510745010927\n",
      "train loss: 2.5322071474104346\n",
      "train loss: 2.4035674829970417\n",
      "train loss: 2.552938422771874\n",
      "train loss: 2.674333059541445\n",
      "train loss: 2.4873668343512425\n",
      "train loss: 2.3264314928286156\n",
      "train loss: 2.3788575851927916\n",
      "train loss: 2.44897577961581\n",
      "=== epoch:50, train acc:0.5444444444444444, test acc:0.5666666666666667 ===\n",
      "train loss: 2.4322904796271754\n",
      "train loss: 2.6549627898855506\n",
      "train loss: 2.5683513691484112\n",
      "train loss: 2.379457609915785\n",
      "train loss: 2.6807783666786182\n",
      "train loss: 2.534724403769637\n",
      "train loss: 2.5252189807822267\n",
      "train loss: 2.290601789509543\n",
      "train loss: 2.5023193532679584\n",
      "=== epoch:51, train acc:0.5888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 2.554887143516045\n",
      "train loss: 2.51213582457576\n",
      "train loss: 2.3571723388885233\n",
      "train loss: 2.545928875301965\n",
      "train loss: 2.602138995109921\n",
      "train loss: 2.507756175968519\n",
      "train loss: 2.6812603012599396\n",
      "train loss: 2.482093182214638\n",
      "train loss: 2.431935291127103\n",
      "=== epoch:52, train acc:0.6222222222222222, test acc:0.7333333333333333 ===\n",
      "train loss: 2.5022727683306916\n",
      "train loss: 2.4835239452357842\n",
      "train loss: 2.3292896615202294\n",
      "train loss: 2.578819611364528\n",
      "train loss: 2.4664795083313336\n",
      "train loss: 2.243871874220918\n",
      "train loss: 2.653188396835089\n",
      "train loss: 2.2812412428419804\n",
      "train loss: 2.637564263690483\n",
      "=== epoch:53, train acc:0.6333333333333333, test acc:0.7333333333333333 ===\n",
      "train loss: 2.383098544357598\n",
      "train loss: 2.654762642987647\n",
      "train loss: 2.6319921856164297\n",
      "train loss: 2.5210219973114985\n",
      "train loss: 2.4363925584524244\n",
      "train loss: 2.5386600423922916\n",
      "train loss: 2.5880383989484472\n",
      "train loss: 2.531793411665812\n",
      "train loss: 2.5040198097817026\n",
      "=== epoch:54, train acc:0.8444444444444444, test acc:0.9666666666666667 ===\n",
      "Reached accuracy_limit\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.9666666666666667\n",
      "[4, 10, 3]\n",
      "train loss: 6.390406595702151\n",
      "=== epoch:1, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.831677285313079\n",
      "train loss: 7.7696509664841695\n",
      "train loss: 7.356472334999945\n",
      "train loss: 5.578330122581232\n",
      "train loss: 6.796419518814122\n",
      "train loss: 5.579489475638885\n",
      "train loss: 6.168859795802066\n",
      "train loss: 6.2150903512046245\n",
      "train loss: 5.043136401641659\n",
      "=== epoch:2, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.3815957649208634\n",
      "train loss: 4.976020403601506\n",
      "train loss: 6.064901085688598\n",
      "train loss: 5.091021673488943\n",
      "train loss: 6.51174468155805\n",
      "train loss: 5.578599698239565\n",
      "train loss: 7.724356558794884\n",
      "train loss: 5.468678274739368\n",
      "train loss: 7.337075764594346\n",
      "=== epoch:3, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.9449831398011606\n",
      "train loss: 5.656347123333956\n",
      "train loss: 5.611665046053764\n",
      "train loss: 5.147252416957872\n",
      "train loss: 5.111887690000755\n",
      "train loss: 6.038689084328625\n",
      "train loss: 4.892582210165761\n",
      "train loss: 6.100402112303845\n",
      "train loss: 6.539453535692842\n",
      "=== epoch:4, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.397390449330353\n",
      "train loss: 6.053045966473421\n",
      "train loss: 6.593688955685014\n",
      "train loss: 5.3531221844960575\n",
      "train loss: 5.527730142295349\n",
      "train loss: 6.257035677463987\n",
      "train loss: 7.19900175547794\n",
      "train loss: 5.322231902070261\n",
      "train loss: 5.37422875797772\n",
      "=== epoch:5, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.247339480876458\n",
      "train loss: 7.711587072382238\n",
      "train loss: 6.2140354705267224\n",
      "train loss: 4.979037796435011\n",
      "train loss: 7.2929943348800865\n",
      "train loss: 6.461075209741819\n",
      "train loss: 6.537258915218024\n",
      "train loss: 9.136901448342048\n",
      "train loss: 6.175503743078068\n",
      "=== epoch:6, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.462709488884668\n",
      "train loss: 5.560423304727715\n",
      "train loss: 6.402131400284913\n",
      "train loss: 6.591692057192928\n",
      "train loss: 6.681131558752034\n",
      "train loss: 6.653556246091885\n",
      "train loss: 5.473299311034631\n",
      "train loss: 5.002860339867937\n",
      "train loss: 6.482120025124722\n",
      "=== epoch:7, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.600872760974798\n",
      "train loss: 5.901383691019918\n",
      "train loss: 7.250722768040764\n",
      "train loss: 5.454804892387703\n",
      "train loss: 5.452915810940939\n",
      "train loss: 6.658871394870336\n",
      "train loss: 6.414149260336094\n",
      "train loss: 4.8293191419502435\n",
      "train loss: 6.7707730368552435\n",
      "=== epoch:8, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.970145128784729\n",
      "train loss: 5.369804939193894\n",
      "train loss: 6.07401276266439\n",
      "train loss: 7.6565584372525715\n",
      "train loss: 4.913164377148027\n",
      "train loss: 5.141194795139185\n",
      "train loss: 6.169063759715494\n",
      "train loss: 4.8785701340032706\n",
      "train loss: 5.1055322380792285\n",
      "=== epoch:9, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.502784576538059\n",
      "train loss: 6.515695981856157\n",
      "train loss: 5.518341208426516\n",
      "train loss: 5.789034030319805\n",
      "train loss: 4.805237830058008\n",
      "train loss: 6.152318891089972\n",
      "train loss: 5.5445904658985725\n",
      "train loss: 5.460073871933517\n",
      "train loss: 7.172305178612337\n",
      "=== epoch:10, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.532250088598736\n",
      "train loss: 8.789433980859643\n",
      "train loss: 7.139158735458085\n",
      "train loss: 5.855262174443069\n",
      "train loss: 5.313528179096018\n",
      "train loss: 4.695588258557322\n",
      "train loss: 5.592449903406976\n",
      "train loss: 6.861511755599358\n",
      "train loss: 6.044242398276214\n",
      "=== epoch:11, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 7.620966072874985\n",
      "train loss: 4.8626540119606485\n",
      "train loss: 5.366982778592728\n",
      "train loss: 4.7806560826991324\n",
      "train loss: 7.18425492354717\n",
      "train loss: 6.995208261786332\n",
      "train loss: 5.277253025628796\n",
      "train loss: 5.493759275754678\n",
      "train loss: 5.835026921901343\n",
      "=== epoch:12, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.450874562674717\n",
      "train loss: 4.379365503132021\n",
      "train loss: 5.869118695457476\n",
      "train loss: 6.623180361369926\n",
      "train loss: 5.721268374376315\n",
      "train loss: 5.5801540905614635\n",
      "train loss: 4.713800978746728\n",
      "train loss: 4.795866188366474\n",
      "train loss: 5.5072947227716424\n",
      "=== epoch:13, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.486945014811342\n",
      "train loss: 5.208307859214595\n",
      "train loss: 5.755094090061908\n",
      "train loss: 6.253189561032937\n",
      "train loss: 7.156070831671003\n",
      "train loss: 4.707390325746123\n",
      "train loss: 5.942361995947569\n",
      "train loss: 5.9816819597632875\n",
      "train loss: 5.326230475528684\n",
      "=== epoch:14, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.014081084743073\n",
      "train loss: 5.830271705734324\n",
      "train loss: 5.327384298901178\n",
      "train loss: 5.203117984420388\n",
      "train loss: 6.339394945937961\n",
      "train loss: 7.6143486313213735\n",
      "train loss: 6.465179929317752\n",
      "train loss: 5.2667170562054695\n",
      "train loss: 6.5406804241313665\n",
      "=== epoch:15, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.5277420108487405\n",
      "train loss: 6.355309296214502\n",
      "train loss: 5.270053616557595\n",
      "train loss: 5.143318221138368\n",
      "train loss: 5.499029163286848\n",
      "train loss: 6.54679667331772\n",
      "train loss: 5.755278653952545\n",
      "train loss: 6.376298820312726\n",
      "train loss: 7.067878562036787\n",
      "=== epoch:16, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.242043563579378\n",
      "train loss: 7.198350876006998\n",
      "train loss: 5.061246478398985\n",
      "train loss: 5.516310839316551\n",
      "train loss: 5.948676411863741\n",
      "train loss: 5.387774368228319\n",
      "train loss: 5.786217755754585\n",
      "train loss: 5.857540248596439\n",
      "train loss: 5.718836784050416\n",
      "=== epoch:17, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.166101460888768\n",
      "train loss: 5.445082439490775\n",
      "train loss: 5.314738719161297\n",
      "train loss: 6.5053345190150145\n",
      "train loss: 5.487127651246532\n",
      "train loss: 5.7943184716201905\n",
      "train loss: 5.681795328037083\n",
      "train loss: 4.836595121095578\n",
      "train loss: 6.34270012120562\n",
      "=== epoch:18, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.899428938394019\n",
      "train loss: 5.232952796409319\n",
      "train loss: 6.353796419250106\n",
      "train loss: 5.42847960226265\n",
      "train loss: 5.115502947408625\n",
      "train loss: 6.248763476995106\n",
      "train loss: 5.321465449782284\n",
      "train loss: 6.114087103744646\n",
      "train loss: 4.814648856480226\n",
      "=== epoch:19, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.906504789450268\n",
      "train loss: 6.522946319030634\n",
      "train loss: 6.907662467045069\n",
      "train loss: 5.612749654554366\n",
      "train loss: 5.960340845753287\n",
      "train loss: 5.937479390948406\n",
      "train loss: 5.36832884074558\n",
      "train loss: 6.254563015214876\n",
      "train loss: 5.4439985135659485\n",
      "=== epoch:20, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.753157014222687\n",
      "train loss: 4.82618314795554\n",
      "train loss: 6.559322553172026\n",
      "train loss: 4.8077976655824415\n",
      "train loss: 6.85009363796547\n",
      "train loss: 5.541962422373144\n",
      "train loss: 5.221232301944116\n",
      "train loss: 6.443064486418477\n",
      "train loss: 5.288485806083622\n",
      "=== epoch:21, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.084187101523234\n",
      "train loss: 4.6590386591846356\n",
      "train loss: 6.196503621986872\n",
      "train loss: 8.48771383993388\n",
      "train loss: 5.462981748031742\n",
      "train loss: 6.8777448695851815\n",
      "train loss: 6.714125792366075\n",
      "train loss: 6.755775969053876\n",
      "train loss: 5.25917738115507\n",
      "=== epoch:22, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.750939010756988\n",
      "train loss: 6.9650546203605606\n",
      "train loss: 5.657708465873522\n",
      "train loss: 6.397755650508766\n",
      "train loss: 5.712199933904244\n",
      "train loss: 6.83218890189817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 5.2936734106228895\n",
      "train loss: 5.909272206256262\n",
      "train loss: 6.4576537663490905\n",
      "=== epoch:23, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.838801397223208\n",
      "train loss: 5.3777051578023585\n",
      "train loss: 6.348842276774049\n",
      "train loss: 6.385614282505326\n",
      "train loss: 5.156307896735003\n",
      "train loss: 4.610890219474337\n",
      "train loss: 5.1199866216171985\n",
      "train loss: 5.154770139218348\n",
      "train loss: 5.132266351950656\n",
      "=== epoch:24, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.231630360081154\n",
      "train loss: 5.528410946205489\n",
      "train loss: 5.614204641345243\n",
      "train loss: 6.422554868069181\n",
      "train loss: 4.68574099135462\n",
      "train loss: 6.525123819455902\n",
      "train loss: 4.600874021151638\n",
      "train loss: 6.2479975900253555\n",
      "train loss: 5.632434140908694\n",
      "=== epoch:25, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.733712454754052\n",
      "train loss: 5.115702098979123\n",
      "train loss: 4.7841047219296815\n",
      "train loss: 5.176273985108456\n",
      "train loss: 5.13979870602998\n",
      "train loss: 5.11004431935429\n",
      "train loss: 4.070858084204085\n",
      "train loss: 5.935198985577846\n",
      "train loss: 5.835807034241186\n",
      "=== epoch:26, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.319218285712292\n",
      "train loss: 5.674219006142492\n",
      "train loss: 4.005320014657959\n",
      "train loss: 4.723213100285598\n",
      "train loss: 5.852940863628327\n",
      "train loss: 5.053829043238567\n",
      "train loss: 5.708448500259607\n",
      "train loss: 6.86030408717966\n",
      "train loss: 5.106513010083086\n",
      "=== epoch:27, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.175876999236743\n",
      "train loss: 5.1210404074417335\n",
      "train loss: 5.042054780392195\n",
      "train loss: 5.698400785036235\n",
      "train loss: 5.558940334253061\n",
      "train loss: 4.647335019241164\n",
      "train loss: 6.391758516030626\n",
      "train loss: 4.231324840191225\n",
      "train loss: 5.006247606214377\n",
      "=== epoch:28, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.177284931309723\n",
      "train loss: 4.631196322492951\n",
      "train loss: 4.526749173930774\n",
      "train loss: 6.1749545348932315\n",
      "train loss: 4.803950150784968\n",
      "train loss: 6.133869706394019\n",
      "train loss: 6.3731887861335\n",
      "train loss: 4.925527940163885\n",
      "train loss: 5.005575573379668\n",
      "=== epoch:29, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.268621701000412\n",
      "train loss: 7.126868868661304\n",
      "train loss: 5.697914050562262\n",
      "train loss: 4.369457507773353\n",
      "train loss: 5.767532432870808\n",
      "train loss: 6.832276602702775\n",
      "train loss: 6.284223230141466\n",
      "train loss: 5.279909683474739\n",
      "train loss: 4.581355458262455\n",
      "=== epoch:30, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.737171048318223\n",
      "train loss: 5.767437802127792\n",
      "train loss: 4.446655486891422\n",
      "train loss: 5.484550773960396\n",
      "train loss: 6.306943375307749\n",
      "train loss: 7.235968618201995\n",
      "train loss: 6.1184581578071\n",
      "train loss: 6.19945709113956\n",
      "train loss: 4.603993839930196\n",
      "=== epoch:31, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.725248432353433\n",
      "train loss: 5.28666450909687\n",
      "train loss: 4.637648342634662\n",
      "train loss: 5.609729306491128\n",
      "train loss: 4.367141418274738\n",
      "train loss: 4.99796059598015\n",
      "train loss: 6.1752820974165425\n",
      "train loss: 3.8964458788417047\n",
      "train loss: 6.245129914924359\n",
      "=== epoch:32, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.013047622649106\n",
      "train loss: 7.526016415946205\n",
      "train loss: 4.749274382803056\n",
      "train loss: 5.236771247911272\n",
      "train loss: 5.210646527852915\n",
      "train loss: 4.716537125241958\n",
      "train loss: 5.611436982159956\n",
      "train loss: 7.367470282173457\n",
      "train loss: 6.11428519256582\n",
      "=== epoch:33, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.655758725292538\n",
      "train loss: 5.616114672787893\n",
      "train loss: 7.454087244404556\n",
      "train loss: 5.629674545619465\n",
      "train loss: 6.335760607786608\n",
      "train loss: 5.038707346832884\n",
      "train loss: 4.607927892112928\n",
      "train loss: 5.2281612458515125\n",
      "train loss: 6.247140517001552\n",
      "=== epoch:34, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.988422148548232\n",
      "train loss: 5.193497522861503\n",
      "train loss: 4.638673048287578\n",
      "train loss: 5.541218387306491\n",
      "train loss: 4.811303186955524\n",
      "train loss: 4.320216610147299\n",
      "train loss: 4.0844145559217\n",
      "train loss: 4.47189043501293\n",
      "train loss: 6.061637604171993\n",
      "=== epoch:35, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.758472187555012\n",
      "train loss: 4.970153970354735\n",
      "train loss: 4.9855066649643\n",
      "train loss: 7.978965427864654\n",
      "train loss: 6.666354024189771\n",
      "train loss: 5.140900749458842\n",
      "train loss: 6.773779916661518\n",
      "train loss: 7.16631767691806\n",
      "train loss: 6.197968792831466\n",
      "=== epoch:36, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.704146809122311\n",
      "train loss: 4.430552348682641\n",
      "train loss: 4.963089863194659\n",
      "train loss: 4.577523046046782\n",
      "train loss: 5.759959266965532\n",
      "train loss: 4.9681850843979944\n",
      "train loss: 5.551519182772943\n",
      "train loss: 5.2017642661721855\n",
      "train loss: 6.386198275299606\n",
      "=== epoch:37, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.04797504590763\n",
      "train loss: 6.220446397369814\n",
      "train loss: 5.597249035939182\n",
      "train loss: 4.247393503183947\n",
      "train loss: 5.187475841564444\n",
      "train loss: 6.050170818145284\n",
      "train loss: 3.948006979940862\n",
      "train loss: 6.07677554048661\n",
      "train loss: 3.6533146323919032\n",
      "=== epoch:38, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.9881497112619\n",
      "train loss: 5.912690413003245\n",
      "train loss: 4.647392356881501\n",
      "train loss: 5.10362831482974\n",
      "train loss: 5.061078514010138\n",
      "train loss: 4.943828237651074\n",
      "train loss: 6.21613110653092\n",
      "train loss: 5.640391330645704\n",
      "train loss: 5.647762646040267\n",
      "=== epoch:39, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.502465002233515\n",
      "train loss: 6.721093670763216\n",
      "train loss: 5.5192775084006565\n",
      "train loss: 6.326019083856945\n",
      "train loss: 5.091645544882146\n",
      "train loss: 5.028685784506145\n",
      "train loss: 5.507912972028332\n",
      "train loss: 5.922113243606879\n",
      "train loss: 5.5879672694297176\n",
      "=== epoch:40, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 3.971364922882237\n",
      "train loss: 5.555038856359705\n",
      "train loss: 6.186022080711775\n",
      "train loss: 4.3907462316453785\n",
      "train loss: 4.466724663766158\n",
      "train loss: 5.209899632167594\n",
      "train loss: 5.038561016530412\n",
      "train loss: 5.803749971347068\n",
      "train loss: 5.446390627909968\n",
      "=== epoch:41, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.075438397020948\n",
      "train loss: 4.028348944436773\n",
      "train loss: 6.564825899334324\n",
      "train loss: 4.801912735883282\n",
      "train loss: 5.707133315762088\n",
      "train loss: 4.717945879511618\n",
      "train loss: 6.02767388310163\n",
      "train loss: 6.20113081293071\n",
      "train loss: 4.428522923801859\n",
      "=== epoch:42, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.215263479697122\n",
      "train loss: 6.137144946065095\n",
      "train loss: 5.039466999764377\n",
      "train loss: 5.490165678414369\n",
      "train loss: 5.054435028386751\n",
      "train loss: 6.554797458792237\n",
      "train loss: 3.7679193073398434\n",
      "train loss: 6.684849572472847\n",
      "train loss: 4.521929573413155\n",
      "=== epoch:43, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.834890872589632\n",
      "train loss: 6.08701255457255\n",
      "train loss: 5.299884929932228\n",
      "train loss: 4.3880390357216355\n",
      "train loss: 5.572718971663059\n",
      "train loss: 6.10868233808865\n",
      "train loss: 5.438879075442823\n",
      "train loss: 3.5803852294348104\n",
      "train loss: 6.201602371820481\n",
      "=== epoch:44, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.454998012663659\n",
      "train loss: 5.29960252165851\n",
      "train loss: 5.871636966560946\n",
      "train loss: 4.824504084498274\n",
      "train loss: 5.508917047113642\n",
      "train loss: 4.851018126024844\n",
      "train loss: 5.085073600715476\n",
      "train loss: 5.411826439818392\n",
      "train loss: 3.7498535110528763\n",
      "=== epoch:45, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.692121860316586\n",
      "train loss: 6.235477947885891\n",
      "train loss: 4.095388517595173\n",
      "train loss: 4.888730884131108\n",
      "train loss: 4.47704669548878\n",
      "train loss: 6.090146369537209\n",
      "train loss: 5.569165018501605\n",
      "train loss: 5.855979002622073\n",
      "train loss: 5.481814399524274\n",
      "=== epoch:46, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 7.18725827640925\n",
      "train loss: 5.6828564398855645\n",
      "train loss: 5.860355533194784\n",
      "train loss: 5.481974404456046\n",
      "train loss: 4.589245225770749\n",
      "train loss: 5.210916804231073\n",
      "train loss: 6.094324046836182\n",
      "train loss: 6.723596464736707\n",
      "train loss: 6.755802407948701\n",
      "=== epoch:47, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.541078765064396\n",
      "train loss: 7.2485018880868894\n",
      "train loss: 4.451865030939636\n",
      "train loss: 5.432968934910174\n",
      "train loss: 7.291558730031971\n",
      "train loss: 5.581626784627668\n",
      "train loss: 7.653847826993055\n",
      "train loss: 6.105161414313176\n",
      "train loss: 6.1190349490685385\n",
      "=== epoch:48, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.94618441345202\n",
      "train loss: 6.540604306825509\n",
      "train loss: 5.665917166082221\n",
      "train loss: 4.837519850051669\n",
      "train loss: 6.5352981924700595\n",
      "train loss: 6.15777040944999\n",
      "train loss: 4.399289814178034\n",
      "train loss: 6.688202988273414\n",
      "train loss: 5.633040080646371\n",
      "=== epoch:49, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 3.594400505554863\n",
      "train loss: 5.097386028964011\n",
      "train loss: 4.971902920890377\n",
      "train loss: 5.044987014982453\n",
      "train loss: 6.558034287958346\n",
      "train loss: 5.418942224250231\n",
      "train loss: 5.057466386674088\n",
      "train loss: 5.370327197319502\n",
      "train loss: 4.455674972288013\n",
      "=== epoch:50, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.39660416897795\n",
      "train loss: 4.213805970827993\n",
      "train loss: 6.714887496331235\n",
      "train loss: 4.4051176168220865\n",
      "train loss: 5.439795254347226\n",
      "train loss: 4.633809594620448\n",
      "train loss: 4.515362898486359\n",
      "train loss: 4.988999334201422\n",
      "train loss: 4.281529061733816\n",
      "=== epoch:51, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.0045613447417425\n",
      "train loss: 6.489721752574832\n",
      "train loss: 4.9488727616662676\n",
      "train loss: 6.624570883072259\n",
      "train loss: 6.455077425558688\n",
      "train loss: 6.286154701220104\n",
      "train loss: 4.96997789224338\n",
      "train loss: 5.41704179376733\n",
      "train loss: 5.367292588697447\n",
      "=== epoch:52, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.99159015905156\n",
      "train loss: 5.727524131604828\n",
      "train loss: 6.453736232282317\n",
      "train loss: 6.093877633140749\n",
      "train loss: 4.148178370688961\n",
      "train loss: 4.703875833394277\n",
      "train loss: 5.980614143052672\n",
      "train loss: 6.027649026966078\n",
      "train loss: 5.074324466502308\n",
      "=== epoch:53, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.335271237873006\n",
      "train loss: 6.525644310984096\n",
      "train loss: 5.156147660892929\n",
      "train loss: 6.741359180668717\n",
      "train loss: 5.521520467952274\n",
      "train loss: 4.837062519293107\n",
      "train loss: 4.440474243516856\n",
      "train loss: 6.186704232893854\n",
      "train loss: 5.9831379438222365\n",
      "=== epoch:54, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.9427072964308705\n",
      "train loss: 4.855390804057441\n",
      "train loss: 4.265863231676815\n",
      "train loss: 4.433118906567316\n",
      "train loss: 5.55103880148019\n",
      "train loss: 5.277170919535392\n",
      "train loss: 5.896542469798144\n",
      "train loss: 6.139732446795396\n",
      "train loss: 4.869794870313038\n",
      "=== epoch:55, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.3744051728231925\n",
      "train loss: 4.406096919328229\n",
      "train loss: 5.1658339805155435\n",
      "train loss: 5.86912357884654\n",
      "train loss: 4.172293964984062\n",
      "train loss: 4.8185933434915595\n",
      "train loss: 6.203665868986751\n",
      "train loss: 3.7601005956900515\n",
      "train loss: 4.083366698684528\n",
      "=== epoch:56, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.7896746694635794\n",
      "train loss: 4.474154495209773\n",
      "train loss: 6.28493188288201\n",
      "train loss: 4.975680430838989\n",
      "train loss: 5.447888322670751\n",
      "train loss: 4.8838877523633615\n",
      "train loss: 5.7817496091543115\n",
      "train loss: 6.730111339385832\n",
      "train loss: 5.517241255052902\n",
      "=== epoch:57, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.551599479882724\n",
      "train loss: 5.009126274818585\n",
      "train loss: 5.332677343366859\n",
      "train loss: 6.322255957190113\n",
      "train loss: 6.979464624898118\n",
      "train loss: 5.244390817879426\n",
      "train loss: 4.405531699807558\n",
      "train loss: 5.00447640782987\n",
      "train loss: 5.059066481008047\n",
      "=== epoch:58, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.392582015799877\n",
      "train loss: 6.874940697744062\n",
      "train loss: 4.501681738035777\n",
      "train loss: 4.7407221328285205\n",
      "train loss: 5.694950114422624\n",
      "train loss: 5.397818814729086\n",
      "train loss: 5.968933981493828\n",
      "train loss: 4.736691038330855\n",
      "train loss: 6.691498994502761\n",
      "=== epoch:59, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.022257344093662\n",
      "train loss: 5.506545945594374\n",
      "train loss: 5.4772214643414205\n",
      "train loss: 4.486468521904676\n",
      "train loss: 4.714662859579115\n",
      "train loss: 5.842071288756101\n",
      "train loss: 4.686210816289797\n",
      "train loss: 4.660118287248407\n",
      "train loss: 4.305497976643154\n",
      "=== epoch:60, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.540352526698108\n",
      "train loss: 4.337750172618666\n",
      "train loss: 5.587185164121558\n",
      "train loss: 5.4797389939861425\n",
      "train loss: 5.090530331999175\n",
      "train loss: 4.5610922718023135\n",
      "train loss: 6.612991674311287\n",
      "train loss: 5.479616220405294\n",
      "train loss: 4.170609064736408\n",
      "=== epoch:61, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.131796313828499\n",
      "train loss: 6.1533305878435165\n",
      "train loss: 4.9368401396760815\n",
      "train loss: 5.385274445043487\n",
      "train loss: 5.758283689889224\n",
      "train loss: 5.8075681995087285\n",
      "train loss: 4.629172368344735\n",
      "train loss: 4.233117719190631\n",
      "train loss: 7.44473508777168\n",
      "=== epoch:62, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.193596367446381\n",
      "train loss: 5.943470513241907\n",
      "train loss: 4.180899256387543\n",
      "train loss: 4.817550884493167\n",
      "train loss: 3.662379200955172\n",
      "train loss: 6.099829869798779\n",
      "train loss: 4.84146643295547\n",
      "train loss: 5.36133397492959\n",
      "train loss: 5.297028843223957\n",
      "=== epoch:63, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.888197339881474\n",
      "train loss: 6.975593596383887\n",
      "train loss: 4.724275691372729\n",
      "train loss: 5.831678022533379\n",
      "train loss: 5.249198053887427\n",
      "train loss: 4.210096299191723\n",
      "train loss: 5.424934067856096\n",
      "train loss: 5.055234943593542\n",
      "train loss: 6.603932176845162\n",
      "=== epoch:64, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.875214742181963\n",
      "train loss: 5.36497679387035\n",
      "train loss: 4.997523830756625\n",
      "train loss: 5.073927038260007\n",
      "train loss: 5.865516403618286\n",
      "train loss: 6.4011199964950185\n",
      "train loss: 4.675980139528738\n",
      "train loss: 5.478231150588425\n",
      "train loss: 4.7847065373149436\n",
      "=== epoch:65, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.029210371901484\n",
      "train loss: 4.853611610319081\n",
      "train loss: 4.78411333845229\n",
      "train loss: 4.745269510574996\n",
      "train loss: 5.8287780226401305\n",
      "train loss: 5.861797725306789\n",
      "train loss: 5.296069142106978\n",
      "train loss: 5.346660234551402\n",
      "train loss: 5.226622794190112\n",
      "=== epoch:66, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.199178762719827\n",
      "train loss: 4.794089848226217\n",
      "train loss: 5.421269436286132\n",
      "train loss: 4.043235595620194\n",
      "train loss: 4.710458048772606\n",
      "train loss: 4.009225042428929\n",
      "train loss: 4.1491519769400576\n",
      "train loss: 4.215564848483929\n",
      "train loss: 5.800795657291544\n",
      "=== epoch:67, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.893322056361895\n",
      "train loss: 4.341220992776887\n",
      "train loss: 5.78823855903155\n",
      "train loss: 5.964543231685903\n",
      "train loss: 4.897434822269885\n",
      "train loss: 5.184871039138408\n",
      "train loss: 5.372788589918532\n",
      "train loss: 4.6981564594366985\n",
      "train loss: 5.851110335557541\n",
      "=== epoch:68, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.470973380344753\n",
      "train loss: 6.556116188284516\n",
      "train loss: 5.266206235388616\n",
      "train loss: 5.916864062267751\n",
      "train loss: 5.490545443527847\n",
      "train loss: 4.705249082592095\n",
      "train loss: 5.3072466815981345\n",
      "train loss: 4.269439122105149\n",
      "train loss: 5.327099327101276\n",
      "=== epoch:69, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.855865388009305\n",
      "train loss: 4.7946868508298515\n",
      "train loss: 5.210057768092913\n",
      "train loss: 5.43071221612349\n",
      "train loss: 5.283017859889884\n",
      "train loss: 5.315006310708897\n",
      "train loss: 6.611724541608297\n",
      "train loss: 5.1392169022939385\n",
      "train loss: 5.58291180549336\n",
      "=== epoch:70, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.618567012639016\n",
      "train loss: 6.851270038328959\n",
      "train loss: 4.915430552964935\n",
      "train loss: 5.148526605564986\n",
      "train loss: 4.306506836035782\n",
      "train loss: 5.329700891617814\n",
      "train loss: 4.222413045052526\n",
      "train loss: 4.934786992310674\n",
      "train loss: 4.768563369442602\n",
      "=== epoch:71, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.433105123360319\n",
      "train loss: 5.179585940330071\n",
      "train loss: 4.694799924252943\n",
      "train loss: 4.7461522397562055\n",
      "train loss: 5.4787192956315325\n",
      "train loss: 4.1697532132665\n",
      "train loss: 4.5619272778108355\n",
      "train loss: 4.979789122624099\n",
      "train loss: 6.357663507290333\n",
      "=== epoch:72, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.1698766751279335\n",
      "train loss: 4.5217963058285875\n",
      "train loss: 5.2687991670208\n",
      "train loss: 6.312872641934831\n",
      "train loss: 5.765933409075638\n",
      "train loss: 6.190287376452\n",
      "train loss: 4.78180301589751\n",
      "train loss: 4.011274953872821\n",
      "train loss: 5.25079689175644\n",
      "=== epoch:73, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.618971407802266\n",
      "train loss: 5.6417428082978045\n",
      "train loss: 4.917412853496038\n",
      "train loss: 5.195768739155269\n",
      "train loss: 7.11365377871907\n",
      "train loss: 3.975904613330373\n",
      "train loss: 4.549337008112381\n",
      "train loss: 5.975009336292248\n",
      "train loss: 4.725256721545531\n",
      "=== epoch:74, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.852424230677583\n",
      "train loss: 5.620651483824972\n",
      "train loss: 4.740794845858214\n",
      "train loss: 5.326625392420366\n",
      "train loss: 5.341066230599064\n",
      "train loss: 6.154853572163071\n",
      "train loss: 5.620900486848654\n",
      "train loss: 4.234218190975336\n",
      "train loss: 6.351320794975842\n",
      "=== epoch:75, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.877397628621069\n",
      "train loss: 4.165514067491959\n",
      "train loss: 4.708449513801017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 4.062273005600618\n",
      "train loss: 5.1906859247546295\n",
      "train loss: 5.350979186199648\n",
      "train loss: 5.0370609795267445\n",
      "train loss: 4.640046798851782\n",
      "train loss: 4.695599677476969\n",
      "=== epoch:76, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.160148516978773\n",
      "train loss: 5.920299359789551\n",
      "train loss: 5.228955612657229\n",
      "train loss: 6.307787775611578\n",
      "train loss: 5.901207577842284\n",
      "train loss: 4.368336996599749\n",
      "train loss: 4.579415010675761\n",
      "train loss: 4.751135432921681\n",
      "train loss: 4.651342378056031\n",
      "=== epoch:77, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.850482926610382\n",
      "train loss: 4.161750856372719\n",
      "train loss: 3.7346212198491724\n",
      "train loss: 4.6435983545283435\n",
      "train loss: 5.264018772470343\n",
      "train loss: 6.5905496572412305\n",
      "train loss: 3.9148746165727486\n",
      "train loss: 5.723081826983468\n",
      "train loss: 5.239976531144882\n",
      "=== epoch:78, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.0541860418357984\n",
      "train loss: 5.1907536349598695\n",
      "train loss: 5.773772296016222\n",
      "train loss: 5.788655922603463\n",
      "train loss: 4.801380339631174\n",
      "train loss: 4.018853701850646\n",
      "train loss: 5.328336521524681\n",
      "train loss: 4.21611320456534\n",
      "train loss: 5.152554823190897\n",
      "=== epoch:79, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.709575474929493\n",
      "train loss: 5.878208336839953\n",
      "train loss: 4.786187992658723\n",
      "train loss: 5.660618996536994\n",
      "train loss: 3.8289253547953566\n",
      "train loss: 3.597682941311993\n",
      "train loss: 6.5085341304325555\n",
      "train loss: 4.888275895320873\n",
      "train loss: 4.1486336738211715\n",
      "=== epoch:80, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.746657138154352\n",
      "train loss: 4.736969228201481\n",
      "train loss: 5.251784612470971\n",
      "train loss: 5.859892669726309\n",
      "train loss: 4.53866818993314\n",
      "train loss: 5.763991734899021\n",
      "train loss: 6.827576313975482\n",
      "train loss: 4.766149925286098\n",
      "train loss: 5.575869315716318\n",
      "=== epoch:81, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.6547393294404875\n",
      "train loss: 6.4598273825363455\n",
      "train loss: 4.822379480617918\n",
      "train loss: 7.271071070655257\n",
      "train loss: 5.75381678286613\n",
      "train loss: 4.432951012433186\n",
      "train loss: 5.467907449343856\n",
      "train loss: 5.093414232374307\n",
      "train loss: 6.254620732770299\n",
      "=== epoch:82, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.386716248400937\n",
      "train loss: 5.061273580352673\n",
      "train loss: 5.135045861247048\n",
      "train loss: 4.69862511609832\n",
      "train loss: 4.246309838340095\n",
      "train loss: 4.9204372222830814\n",
      "train loss: 4.596496049098968\n",
      "train loss: 5.285702194054339\n",
      "train loss: 3.866878646203957\n",
      "=== epoch:83, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.787060245677439\n",
      "train loss: 4.130653838891145\n",
      "train loss: 4.304863862764619\n",
      "train loss: 5.20166785440554\n",
      "train loss: 5.1053505769766545\n",
      "train loss: 6.297316318424858\n",
      "train loss: 5.619764788231176\n",
      "train loss: 4.88148999244511\n",
      "train loss: 5.231136123177755\n",
      "=== epoch:84, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.6779720382940235\n",
      "train loss: 4.434030372481771\n",
      "train loss: 5.073813658392435\n",
      "train loss: 4.783029003107037\n",
      "train loss: 4.110706067435777\n",
      "train loss: 5.708108637479917\n",
      "train loss: 6.22534842688319\n",
      "train loss: 4.145770903605209\n",
      "train loss: 6.292584100748538\n",
      "=== epoch:85, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.800748957658137\n",
      "train loss: 5.581851076387411\n",
      "train loss: 4.158749022233938\n",
      "train loss: 5.917174026951065\n",
      "train loss: 6.198504470883814\n",
      "train loss: 7.148173681468086\n",
      "train loss: 5.060704650647398\n",
      "train loss: 3.999335838794903\n",
      "train loss: 4.2564357382728355\n",
      "=== epoch:86, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 6.322134941874129\n",
      "train loss: 4.660221748199675\n",
      "train loss: 4.468860162472179\n",
      "train loss: 4.951820072873907\n",
      "train loss: 4.4505868738217975\n",
      "train loss: 6.240916366349884\n",
      "train loss: 4.147668616927659\n",
      "train loss: 6.457188667121669\n",
      "train loss: 4.400853920645333\n",
      "=== epoch:87, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.373226759643314\n",
      "train loss: 5.195906903774448\n",
      "train loss: 4.324931244804877\n",
      "train loss: 6.871227071234207\n",
      "train loss: 4.7336288679370915\n",
      "train loss: 4.644621635422346\n",
      "train loss: 5.621483084271827\n",
      "train loss: 4.402368449372353\n",
      "train loss: 4.616814830492415\n",
      "=== epoch:88, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.115489600765366\n",
      "train loss: 5.883751628307326\n",
      "train loss: 5.752373874889984\n",
      "train loss: 5.5879624099430325\n",
      "train loss: 3.560062930838204\n",
      "train loss: 4.636687329023289\n",
      "train loss: 5.0176784545718\n",
      "train loss: 4.730235626504579\n",
      "train loss: 4.011510020752187\n",
      "=== epoch:89, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.874420281227472\n",
      "train loss: 5.072123616525238\n",
      "train loss: 4.691600681847969\n",
      "train loss: 3.9341581122991878\n",
      "train loss: 4.186797241592201\n",
      "train loss: 5.064067626711821\n",
      "train loss: 6.203217569014024\n",
      "train loss: 5.029409698393441\n",
      "train loss: 6.917780979134804\n",
      "=== epoch:90, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 3.4921337773170045\n",
      "train loss: 6.273676218692938\n",
      "train loss: 5.643090296421979\n",
      "train loss: 5.139117262973101\n",
      "train loss: 3.9752110067856896\n",
      "train loss: 3.9993381435356308\n",
      "train loss: 5.604415037214122\n",
      "train loss: 4.35551885505758\n",
      "train loss: 6.230189427207775\n",
      "=== epoch:91, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.801004957315727\n",
      "train loss: 5.610480244517808\n",
      "train loss: 5.148537819666234\n",
      "train loss: 5.687861772245718\n",
      "train loss: 4.518773414515335\n",
      "train loss: 5.748318803727341\n",
      "train loss: 4.492314537142317\n",
      "train loss: 6.171744527256216\n",
      "train loss: 5.087522046040473\n",
      "=== epoch:92, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.567168093662503\n",
      "train loss: 5.736683787955801\n",
      "train loss: 4.6280334691683525\n",
      "train loss: 4.440545704375708\n",
      "train loss: 5.702979241214038\n",
      "train loss: 4.743128372419993\n",
      "train loss: 5.3750648061930075\n",
      "train loss: 6.028493933208925\n",
      "train loss: 4.497355832915129\n",
      "=== epoch:93, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.778022734083541\n",
      "train loss: 5.590936066981449\n",
      "train loss: 4.542104291029678\n",
      "train loss: 3.9823088605125894\n",
      "train loss: 5.059466115656979\n",
      "train loss: 5.167327117487995\n",
      "train loss: 4.5455074394298896\n",
      "train loss: 5.8172196032796\n",
      "train loss: 4.403169771135542\n",
      "=== epoch:94, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.101577460254598\n",
      "train loss: 4.163864680828496\n",
      "train loss: 4.508820277598769\n",
      "train loss: 5.155755730253307\n",
      "train loss: 3.829149003558469\n",
      "train loss: 4.8898591980298916\n",
      "train loss: 4.209659200823299\n",
      "train loss: 5.085199105995004\n",
      "train loss: 4.221783157647453\n",
      "=== epoch:95, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.752036213591556\n",
      "train loss: 3.8583541850977285\n",
      "train loss: 4.995301669211896\n",
      "train loss: 4.646853461903934\n",
      "train loss: 4.136997856388942\n",
      "train loss: 4.39831634179732\n",
      "train loss: 3.8306281896684786\n",
      "train loss: 4.143793840004298\n",
      "train loss: 4.486852743260934\n",
      "=== epoch:96, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.621747161583961\n",
      "train loss: 6.310127760295217\n",
      "train loss: 4.772741293010681\n",
      "train loss: 4.2809010537938805\n",
      "train loss: 4.793554818489723\n",
      "train loss: 4.483677605036294\n",
      "train loss: 4.08211210498521\n",
      "train loss: 5.521924199928433\n",
      "train loss: 4.109806120897178\n",
      "=== epoch:97, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.552153151339988\n",
      "train loss: 6.14089708084465\n",
      "train loss: 4.433102111887395\n",
      "train loss: 4.962986310298251\n",
      "train loss: 4.968280093008519\n",
      "train loss: 4.4588768082157895\n",
      "train loss: 6.053250664336315\n",
      "train loss: 4.164371090447303\n",
      "train loss: 4.730383988217432\n",
      "=== epoch:98, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.676249298928847\n",
      "train loss: 5.171837383314691\n",
      "train loss: 5.451734654384466\n",
      "train loss: 6.232167205486976\n",
      "train loss: 3.637017460389005\n",
      "train loss: 5.567767772615737\n",
      "train loss: 5.608503532928074\n",
      "train loss: 4.863013796409111\n",
      "train loss: 5.3705034681631085\n",
      "=== epoch:99, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 5.492143551915878\n",
      "train loss: 6.388319576116513\n",
      "train loss: 4.105562723043613\n",
      "train loss: 5.9447990874892875\n",
      "train loss: 5.731590428694725\n",
      "train loss: 5.142728951268433\n",
      "train loss: 5.5904810900131405\n",
      "train loss: 5.341502347128588\n",
      "train loss: 4.828603380528888\n",
      "=== epoch:100, train acc:0.32222222222222224, test acc:0.3 ===\n",
      "train loss: 4.596544813623932\n",
      "train loss: 4.6879231802118255\n",
      "train loss: 3.923381975119818\n",
      "train loss: 6.476279867926616\n",
      "train loss: 5.187849317065681\n",
      "train loss: 4.046584415145756\n",
      "train loss: 6.552717848592658\n",
      "train loss: 5.065819224645439\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.3\n",
      "[4, 10, 3]\n",
      "train loss: 3.7918531553828116\n",
      "=== epoch:1, train acc:0.36666666666666664, test acc:0.36666666666666664 ===\n",
      "train loss: 3.4747607499054314\n",
      "train loss: 5.152613717988942\n",
      "train loss: 3.0195977874058704\n",
      "train loss: 4.453401877421654\n",
      "train loss: 3.3182686611282675\n",
      "train loss: 2.811050182905299\n",
      "train loss: 4.239490195036649\n",
      "train loss: 4.716207556446564\n",
      "train loss: 3.8969463146309904\n",
      "=== epoch:2, train acc:0.36666666666666664, test acc:0.36666666666666664 ===\n",
      "train loss: 3.6794279931107923\n",
      "train loss: 3.721098833415108\n",
      "train loss: 4.274018617821914\n",
      "train loss: 4.0405150330099495\n",
      "train loss: 3.227642276429624\n",
      "train loss: 3.593239379357972\n",
      "train loss: 4.365785164855358\n",
      "train loss: 3.59528807494691\n",
      "train loss: 3.06792022818851\n",
      "=== epoch:3, train acc:0.36666666666666664, test acc:0.36666666666666664 ===\n",
      "train loss: 3.2727695724114785\n",
      "train loss: 2.609246229931788\n",
      "train loss: 4.252923182952761\n",
      "train loss: 3.355165552134596\n",
      "train loss: 3.46548299471122\n",
      "train loss: 2.946305429050775\n",
      "train loss: 2.9733416479220423\n",
      "train loss: 2.789024895071205\n",
      "train loss: 2.5206047208303084\n",
      "=== epoch:4, train acc:0.6333333333333333, test acc:0.5333333333333333 ===\n",
      "train loss: 2.9639777007704273\n",
      "train loss: 3.0299770390795477\n",
      "train loss: 2.889678686990842\n",
      "train loss: 2.6369743985945915\n",
      "train loss: 2.9348441894405637\n",
      "train loss: 2.848361865520003\n",
      "train loss: 2.056579424783034\n",
      "train loss: 2.434391107142531\n",
      "train loss: 2.2114748244954985\n",
      "=== epoch:5, train acc:0.6888888888888889, test acc:0.5666666666666667 ===\n",
      "train loss: 3.17404929819687\n",
      "train loss: 2.8091268669875724\n",
      "train loss: 2.5759992163942282\n",
      "train loss: 2.757853197372562\n",
      "train loss: 2.98424266766031\n",
      "train loss: 2.793926506731363\n",
      "train loss: 2.5499395755933105\n",
      "train loss: 2.619365519432799\n",
      "train loss: 2.8269192759504014\n",
      "=== epoch:6, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.710609593067865\n",
      "train loss: 2.8916531160757835\n",
      "train loss: 2.5304227006980136\n",
      "train loss: 2.4637623023240622\n",
      "train loss: 2.6993273814580077\n",
      "train loss: 2.877680192389502\n",
      "train loss: 2.5292717461367293\n",
      "train loss: 2.1727243911558163\n",
      "train loss: 2.7262520262684755\n",
      "=== epoch:7, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 3.01866799793865\n",
      "train loss: 2.4100992345428676\n",
      "train loss: 2.300159895280648\n",
      "train loss: 2.636114718931978\n",
      "train loss: 2.3158594935135235\n",
      "train loss: 2.493608430422451\n",
      "train loss: 2.658437873017656\n",
      "train loss: 2.57122546151122\n",
      "train loss: 2.425094763242315\n",
      "=== epoch:8, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.530230893396617\n",
      "train loss: 2.0314969049990097\n",
      "train loss: 2.7792189557044673\n",
      "train loss: 2.7577424901836625\n",
      "train loss: 2.4617241796551506\n",
      "train loss: 2.4446286043827894\n",
      "train loss: 2.48181295425345\n",
      "train loss: 1.9486979727276759\n",
      "train loss: 2.6604047840657823\n",
      "=== epoch:9, train acc:0.6555555555555556, test acc:0.6333333333333333 ===\n",
      "train loss: 2.208511066486794\n",
      "train loss: 2.0559842306633125\n",
      "train loss: 1.9643912949695939\n",
      "train loss: 2.467103524071096\n",
      "train loss: 2.3852142827212828\n",
      "train loss: 1.9067027513689232\n",
      "train loss: 2.3145134148049205\n",
      "train loss: 2.5648093009513415\n",
      "train loss: 1.8629250559100963\n",
      "=== epoch:10, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.1801641287256963\n",
      "train loss: 1.924262833683669\n",
      "train loss: 2.6345575140867767\n",
      "train loss: 2.3735981833532587\n",
      "train loss: 2.425496464143075\n",
      "train loss: 2.40217843294928\n",
      "train loss: 2.3651817642870068\n",
      "train loss: 2.5275049041937216\n",
      "train loss: 2.1761951162997213\n",
      "=== epoch:11, train acc:0.6777777777777778, test acc:0.6333333333333333 ===\n",
      "train loss: 2.254216986976946\n",
      "train loss: 2.036781460353227\n",
      "train loss: 2.609292817175688\n",
      "train loss: 2.4348661511019905\n",
      "train loss: 1.9678094702924278\n",
      "train loss: 2.1633152230467374\n",
      "train loss: 2.1347230308078045\n",
      "train loss: 2.4606697093192405\n",
      "train loss: 2.217815589948919\n",
      "=== epoch:12, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.002911077610056\n",
      "train loss: 1.8171920459544406\n",
      "train loss: 2.000999460006145\n",
      "train loss: 2.2766384522490535\n",
      "train loss: 1.9309764231476603\n",
      "train loss: 2.429509028289603\n",
      "train loss: 2.167914688390113\n",
      "train loss: 1.7591470026364948\n",
      "train loss: 2.7305352648043715\n",
      "=== epoch:13, train acc:0.6777777777777778, test acc:0.6333333333333333 ===\n",
      "train loss: 2.0066001021115953\n",
      "train loss: 2.2225091398492776\n",
      "train loss: 2.393594530596745\n",
      "train loss: 1.942823010668787\n",
      "train loss: 1.800388019828742\n",
      "train loss: 1.7736841932858358\n",
      "train loss: 1.4035998452462026\n",
      "train loss: 2.4186826622050437\n",
      "train loss: 1.8702413415702435\n",
      "=== epoch:14, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.1804484749324513\n",
      "train loss: 1.8744746114736899\n",
      "train loss: 1.738859363560618\n",
      "train loss: 2.3146460669357567\n",
      "train loss: 1.851321198712353\n",
      "train loss: 1.7196866809592632\n",
      "train loss: 1.4042852815118307\n",
      "train loss: 1.798745725550286\n",
      "train loss: 2.77185529402764\n",
      "=== epoch:15, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.253523625634878\n",
      "train loss: 1.8824959575036881\n",
      "train loss: 2.9719009820218494\n",
      "train loss: 2.4902624105852396\n",
      "train loss: 2.303212614289401\n",
      "train loss: 2.323932043374434\n",
      "train loss: 2.401876881460354\n",
      "train loss: 2.454098987361552\n",
      "train loss: 1.872601648805895\n",
      "=== epoch:16, train acc:0.6777777777777778, test acc:0.6333333333333333 ===\n",
      "train loss: 1.4473499443116338\n",
      "train loss: 1.2482957437805167\n",
      "train loss: 1.8713381006062972\n",
      "train loss: 1.8851086502868424\n",
      "train loss: 2.1264791007354673\n",
      "train loss: 2.0498510108493226\n",
      "train loss: 1.5373089514418394\n",
      "train loss: 1.9856189739595989\n",
      "train loss: 2.7151365255718023\n",
      "=== epoch:17, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.4877558406466251\n",
      "train loss: 1.811667317025665\n",
      "train loss: 1.9508351994435131\n",
      "train loss: 2.852076336438367\n",
      "train loss: 2.044752432491002\n",
      "train loss: 2.5932151625176765\n",
      "train loss: 1.6063468856123166\n",
      "train loss: 2.267162639576392\n",
      "train loss: 2.075715030324435\n",
      "=== epoch:18, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.822290565010823\n",
      "train loss: 2.3277824544823984\n",
      "train loss: 2.7504602226326487\n",
      "train loss: 1.9387319606416216\n",
      "train loss: 2.4037620213612065\n",
      "train loss: 2.3528924580368415\n",
      "train loss: 1.7959116956752235\n",
      "train loss: 2.1749422946917423\n",
      "train loss: 2.3243174391877335\n",
      "=== epoch:19, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 2.291532269486725\n",
      "train loss: 2.028968734561203\n",
      "train loss: 1.755351189185858\n",
      "train loss: 1.6143351674477442\n",
      "train loss: 1.808987073362567\n",
      "train loss: 1.6282747574108312\n",
      "train loss: 2.0289093644656995\n",
      "train loss: 2.0420544578218074\n",
      "train loss: 1.9336549977008128\n",
      "=== epoch:20, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.006885854428177\n",
      "train loss: 2.485949897125274\n",
      "train loss: 2.0834536947124582\n",
      "train loss: 1.6945190588302752\n",
      "train loss: 2.058025886175326\n",
      "train loss: 1.6647745374720753\n",
      "train loss: 1.568104400825918\n",
      "train loss: 2.148175835647847\n",
      "train loss: 2.115514497130849\n",
      "=== epoch:21, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.143943288441833\n",
      "train loss: 1.4095505734333604\n",
      "train loss: 3.008761544167632\n",
      "train loss: 1.9167984823961841\n",
      "train loss: 2.147705263760286\n",
      "train loss: 1.9394790077113138\n",
      "train loss: 1.868512466343994\n",
      "train loss: 1.9581068667428077\n",
      "train loss: 1.7888960257083704\n",
      "=== epoch:22, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.812228991900415\n",
      "train loss: 1.4960739357601784\n",
      "train loss: 2.075906788003842\n",
      "train loss: 2.339678136138629\n",
      "train loss: 1.6731866561970403\n",
      "train loss: 1.0209288189443435\n",
      "train loss: 1.9055928847804189\n",
      "train loss: 2.288495944351247\n",
      "train loss: 2.003081735487566\n",
      "=== epoch:23, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.0280258592646416\n",
      "train loss: 2.284450400013881\n",
      "train loss: 2.183188540548718\n",
      "train loss: 2.1044412836278807\n",
      "train loss: 1.4353335675492753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.1272439648360653\n",
      "train loss: 1.4837327708783805\n",
      "train loss: 2.154074859159987\n",
      "train loss: 2.358630226608206\n",
      "=== epoch:24, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.970693831267096\n",
      "train loss: 1.74645830130211\n",
      "train loss: 1.5243746061570287\n",
      "train loss: 2.2787464973825946\n",
      "train loss: 2.0023625000300953\n",
      "train loss: 2.2349453667810035\n",
      "train loss: 1.1724091239564847\n",
      "train loss: 1.9971266805666361\n",
      "train loss: 1.485219206712813\n",
      "=== epoch:25, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.0432831321700586\n",
      "train loss: 1.7326509059919992\n",
      "train loss: 2.6391258669179054\n",
      "train loss: 1.626093450575236\n",
      "train loss: 2.3174080674772846\n",
      "train loss: 2.3331156510831974\n",
      "train loss: 1.8311920504589414\n",
      "train loss: 2.520570812217662\n",
      "train loss: 1.7457415946830293\n",
      "=== epoch:26, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.997171372326181\n",
      "train loss: 1.7966696112962643\n",
      "train loss: 1.8327328717695905\n",
      "train loss: 1.589234151885532\n",
      "train loss: 2.4890411643028294\n",
      "train loss: 1.914105091680284\n",
      "train loss: 1.6919098531158694\n",
      "train loss: 1.7238059378714221\n",
      "train loss: 1.8947495892024344\n",
      "=== epoch:27, train acc:0.6888888888888889, test acc:0.7 ===\n",
      "train loss: 1.9836944231570552\n",
      "train loss: 2.0692421828677516\n",
      "train loss: 1.901246779817863\n",
      "train loss: 2.3966325760127627\n",
      "train loss: 1.7822020474042042\n",
      "train loss: 1.9205060087067833\n",
      "train loss: 1.8007985559898723\n",
      "train loss: 1.8300145625344486\n",
      "train loss: 1.9595546105877493\n",
      "=== epoch:28, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.6826858268867346\n",
      "train loss: 2.1317278271378095\n",
      "train loss: 1.9161089974138266\n",
      "train loss: 2.342314685031039\n",
      "train loss: 2.152865132955127\n",
      "train loss: 2.400360346458494\n",
      "train loss: 1.8833709398568241\n",
      "train loss: 1.5632627328274784\n",
      "train loss: 1.2335122637413725\n",
      "=== epoch:29, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.9577772871385206\n",
      "train loss: 1.4292621742440539\n",
      "train loss: 2.318457238479021\n",
      "train loss: 1.8290534342676488\n",
      "train loss: 2.225846684543223\n",
      "train loss: 2.2786229754406797\n",
      "train loss: 1.507130981606334\n",
      "train loss: 1.694569128188981\n",
      "train loss: 1.8200435913177078\n",
      "=== epoch:30, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.28307256994333\n",
      "train loss: 2.222631462088932\n",
      "train loss: 1.9007238095353673\n",
      "train loss: 1.5616309300768079\n",
      "train loss: 1.755863074162159\n",
      "train loss: 1.705348216054912\n",
      "train loss: 2.517476678509488\n",
      "train loss: 1.3581959118849176\n",
      "train loss: 1.8471163772469361\n",
      "=== epoch:31, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.0826760722351128\n",
      "train loss: 2.644654218972451\n",
      "train loss: 1.995603056661962\n",
      "train loss: 1.4337369191459368\n",
      "train loss: 1.5201482303088691\n",
      "train loss: 1.944170929442847\n",
      "train loss: 1.8352249011940478\n",
      "train loss: 1.715299738829794\n",
      "train loss: 2.1584784445449112\n",
      "=== epoch:32, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.7086714093629878\n",
      "train loss: 1.9747708554708066\n",
      "train loss: 2.153718144522981\n",
      "train loss: 2.333362004575869\n",
      "train loss: 1.6544973392904068\n",
      "train loss: 1.8033300179050884\n",
      "train loss: 1.6536314671491563\n",
      "train loss: 1.972309058480674\n",
      "train loss: 2.256660636610924\n",
      "=== epoch:33, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.6270386383490891\n",
      "train loss: 1.2358664328017313\n",
      "train loss: 1.7230221605747789\n",
      "train loss: 2.0147334641907553\n",
      "train loss: 1.798380686753108\n",
      "train loss: 1.78322954023449\n",
      "train loss: 2.356379014615286\n",
      "train loss: 1.8287165225482411\n",
      "train loss: 1.7206309739552903\n",
      "=== epoch:34, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.3602820245406044\n",
      "train loss: 1.9947902830694122\n",
      "train loss: 1.5578468321206156\n",
      "train loss: 2.3736870954724485\n",
      "train loss: 1.6658422930576602\n",
      "train loss: 1.3471179197266236\n",
      "train loss: 2.0153198464059443\n",
      "train loss: 0.7921632901304042\n",
      "train loss: 0.9591257745534405\n",
      "=== epoch:35, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.9596084258044426\n",
      "train loss: 1.7881500861852009\n",
      "train loss: 1.8001945828612604\n",
      "train loss: 1.8616941679278238\n",
      "train loss: 1.1269039670460774\n",
      "train loss: 2.524954702030641\n",
      "train loss: 1.1667707003291359\n",
      "train loss: 1.1267410977691246\n",
      "train loss: 1.6940581001917894\n",
      "=== epoch:36, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.954403469530802\n",
      "train loss: 1.4943988755863067\n",
      "train loss: 2.385438295011234\n",
      "train loss: 1.4323520715472762\n",
      "train loss: 2.353865129351866\n",
      "train loss: 2.692325284705157\n",
      "train loss: 1.3498734591195194\n",
      "train loss: 1.4619881640120718\n",
      "train loss: 1.2405337161906562\n",
      "=== epoch:37, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.550318492288758\n",
      "train loss: 1.7864166731136801\n",
      "train loss: 1.5290878595408688\n",
      "train loss: 1.4121174326817394\n",
      "train loss: 1.8832134079827705\n",
      "train loss: 2.592823382599082\n",
      "train loss: 1.3853818150420767\n",
      "train loss: 1.0255506142013813\n",
      "train loss: 1.5263482783139177\n",
      "=== epoch:38, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.4523502469690097\n",
      "train loss: 1.6414349757661513\n",
      "train loss: 1.5285262676247555\n",
      "train loss: 1.9689042112897641\n",
      "train loss: 1.5056750131696526\n",
      "train loss: 1.4742198655974885\n",
      "train loss: 1.719351374591264\n",
      "train loss: 2.048929688585091\n",
      "train loss: 0.5504036794271732\n",
      "=== epoch:39, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.2008785532895287\n",
      "train loss: 1.0530672701364345\n",
      "train loss: 1.1240703854956997\n",
      "train loss: 1.950120186822623\n",
      "train loss: 1.3877756000378172\n",
      "train loss: 1.9960718166249007\n",
      "train loss: 2.0842286025057692\n",
      "train loss: 1.481773466478999\n",
      "train loss: 1.3521377439143363\n",
      "=== epoch:40, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.9819656403233499\n",
      "train loss: 1.7789279474762851\n",
      "train loss: 2.4414053138194403\n",
      "train loss: 2.0522812769717524\n",
      "train loss: 1.4352267517543025\n",
      "train loss: 1.7695646015658655\n",
      "train loss: 0.90275206197711\n",
      "train loss: 1.3474865943494512\n",
      "train loss: 1.9417558220621922\n",
      "=== epoch:41, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.9125726200196436\n",
      "train loss: 2.3417743185050224\n",
      "train loss: 1.9220186100313315\n",
      "train loss: 1.2139502842732104\n",
      "train loss: 0.6388238972284445\n",
      "train loss: 2.3251870258593432\n",
      "train loss: 1.424229343795107\n",
      "train loss: 1.627951233451567\n",
      "train loss: 2.294010459945595\n",
      "=== epoch:42, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.232826590953846\n",
      "train loss: 2.159227163276659\n",
      "train loss: 1.8684638047932876\n",
      "train loss: 1.920904109392852\n",
      "train loss: 1.687903041253458\n",
      "train loss: 1.9476140368553476\n",
      "train loss: 1.3788754899491413\n",
      "train loss: 1.4424940643621995\n",
      "train loss: 1.8862929887741378\n",
      "=== epoch:43, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.8639268952515373\n",
      "train loss: 1.6842835955352315\n",
      "train loss: 1.7404831942378582\n",
      "train loss: 1.3548407204849326\n",
      "train loss: 1.5892875707113474\n",
      "train loss: 2.445486706084035\n",
      "train loss: 1.6721658176826997\n",
      "train loss: 2.1916959712978907\n",
      "train loss: 1.4298983761593302\n",
      "=== epoch:44, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.6209905661130604\n",
      "train loss: 1.9329781269344712\n",
      "train loss: 1.5020876373624723\n",
      "train loss: 1.4683768412163036\n",
      "train loss: 1.52709962333398\n",
      "train loss: 1.9438146597214145\n",
      "train loss: 1.9223738295863004\n",
      "train loss: 1.1196738448490238\n",
      "train loss: 1.4468015300404253\n",
      "=== epoch:45, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.9490612302259744\n",
      "train loss: 1.039682650300916\n",
      "train loss: 1.7969154189884253\n",
      "train loss: 0.869931644415821\n",
      "train loss: 2.2492014765216104\n",
      "train loss: 1.6223453650300845\n",
      "train loss: 2.018408407772491\n",
      "train loss: 2.023697614458727\n",
      "train loss: 1.9642897595281352\n",
      "=== epoch:46, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.427521154115835\n",
      "train loss: 1.888601131235884\n",
      "train loss: 1.7712167656766642\n",
      "train loss: 1.8337976069412463\n",
      "train loss: 1.9999491878118498\n",
      "train loss: 1.0356361262426037\n",
      "train loss: 1.0559379611113968\n",
      "train loss: 2.189795970234869\n",
      "train loss: 1.8081543365606838\n",
      "=== epoch:47, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.9645804164326575\n",
      "train loss: 1.461697081232653\n",
      "train loss: 2.5657341583298368\n",
      "train loss: 1.5937937225920193\n",
      "train loss: 1.3862398469917467\n",
      "train loss: 1.9087843711089825\n",
      "train loss: 1.590880692904499\n",
      "train loss: 1.8639905148990807\n",
      "train loss: 1.9122185370642646\n",
      "=== epoch:48, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.7786726296531377\n",
      "train loss: 1.4959854617583634\n",
      "train loss: 1.7547050842782954\n",
      "train loss: 0.8413290917230809\n",
      "train loss: 1.7752032310016956\n",
      "train loss: 2.398832180551313\n",
      "train loss: 2.2776455455611906\n",
      "train loss: 2.3718684327608175\n",
      "train loss: 1.816130348328087\n",
      "=== epoch:49, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.6333289136581686\n",
      "train loss: 2.434389331080941\n",
      "train loss: 1.6233489244928814\n",
      "train loss: 2.1012210693645708\n",
      "train loss: 1.9223881414458472\n",
      "train loss: 1.9110834659249814\n",
      "train loss: 2.3480804049484862\n",
      "train loss: 1.4712276783982354\n",
      "train loss: 1.719242329941767\n",
      "=== epoch:50, train acc:0.7, test acc:0.7 ===\n",
      "train loss: 0.9355179547661112\n",
      "train loss: 0.9843685484373943\n",
      "train loss: 1.466189833072993\n",
      "train loss: 1.7058880893059487\n",
      "train loss: 2.280778868579455\n",
      "train loss: 1.3052074267452551\n",
      "train loss: 2.231319061615666\n",
      "train loss: 1.8839330500425013\n",
      "train loss: 2.0018662526828557\n",
      "=== epoch:51, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.5450223546191417\n",
      "train loss: 1.9909266691694882\n",
      "train loss: 1.582509098684535\n",
      "train loss: 2.383510548452143\n",
      "train loss: 1.6614270835706089\n",
      "train loss: 2.1374515022087306\n",
      "train loss: 1.7987409794396196\n",
      "train loss: 1.1902698391753093\n",
      "train loss: 1.962513180064788\n",
      "=== epoch:52, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.8407297353642404\n",
      "train loss: 0.8949309061075347\n",
      "train loss: 1.1744426102194323\n",
      "train loss: 1.7040892336336129\n",
      "train loss: 2.747511514740322\n",
      "train loss: 1.7744784096068922\n",
      "train loss: 1.571600662955709\n",
      "train loss: 1.4378354779742846\n",
      "train loss: 1.866015705606047\n",
      "=== epoch:53, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.4190224966462734\n",
      "train loss: 2.1281606932711745\n",
      "train loss: 1.804799653703461\n",
      "train loss: 1.4155057691224002\n",
      "train loss: 1.7221665554543928\n",
      "train loss: 1.707973012593289\n",
      "train loss: 1.2595723443218665\n",
      "train loss: 1.3506769115991608\n",
      "train loss: 1.1719665601684133\n",
      "=== epoch:54, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.535170253548089\n",
      "train loss: 2.129141937006956\n",
      "train loss: 2.3235263766341827\n",
      "train loss: 1.6793527379317195\n",
      "train loss: 1.9946352219373769\n",
      "train loss: 0.9165143358754758\n",
      "train loss: 2.600749886167771\n",
      "train loss: 2.129046524315375\n",
      "train loss: 1.443626733539676\n",
      "=== epoch:55, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 2.0022932259901114\n",
      "train loss: 1.9411534063548153\n",
      "train loss: 1.3901083134871814\n",
      "train loss: 1.7107783850536622\n",
      "train loss: 1.3185406154779817\n",
      "train loss: 1.4622204374232939\n",
      "train loss: 2.1726028432318065\n",
      "train loss: 1.9950541229348682\n",
      "train loss: 2.0237736193136477\n",
      "=== epoch:56, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.8464866482421842\n",
      "train loss: 1.3591718466485163\n",
      "train loss: 1.7219131979506481\n",
      "train loss: 1.512597162454621\n",
      "train loss: 2.0283814143175256\n",
      "train loss: 2.0821104827869927\n",
      "train loss: 2.3114606299885896\n",
      "train loss: 1.6820717625449666\n",
      "train loss: 1.417126995379868\n",
      "=== epoch:57, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.1861382284553592\n",
      "train loss: 1.2053722150457569\n",
      "train loss: 1.8854789151045142\n",
      "train loss: 1.524613730241734\n",
      "train loss: 0.7773740640394408\n",
      "train loss: 1.1752492635416547\n",
      "train loss: 1.223230528572128\n",
      "train loss: 2.0856183316094143\n",
      "train loss: 1.418388005327078\n",
      "=== epoch:58, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.5245121281563845\n",
      "train loss: 1.0574603777905511\n",
      "train loss: 0.5069993929418553\n",
      "train loss: 0.9733059213077739\n",
      "train loss: 1.5589066761948394\n",
      "train loss: 0.9310350482028809\n",
      "train loss: 1.2453310005781661\n",
      "train loss: 1.80110681742682\n",
      "train loss: 1.2794891167396547\n",
      "=== epoch:59, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.8321328170710776\n",
      "train loss: 1.4440120380279837\n",
      "train loss: 1.5268914000357086\n",
      "train loss: 0.8675700183015826\n",
      "train loss: 1.9604953414249455\n",
      "train loss: 1.5084218943508318\n",
      "train loss: 1.771145673921235\n",
      "train loss: 2.57567954508161\n",
      "train loss: 2.1173662662331734\n",
      "=== epoch:60, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.2578815490277875\n",
      "train loss: 2.2998932407421195\n",
      "train loss: 1.3875985439509977\n",
      "train loss: 0.9501133347834357\n",
      "train loss: 1.5447039370844848\n",
      "train loss: 1.6526083712547024\n",
      "train loss: 1.3292806521801883\n",
      "train loss: 2.1414248017138204\n",
      "train loss: 2.0554439104938735\n",
      "=== epoch:61, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.125835647771383\n",
      "train loss: 0.7429014493634446\n",
      "train loss: 2.3019094486139426\n",
      "train loss: 0.9949049137962619\n",
      "train loss: 1.4058424930486573\n",
      "train loss: 1.4618361883067879\n",
      "train loss: 2.293101683482545\n",
      "train loss: 1.9840311298679967\n",
      "train loss: 1.6962893031700046\n",
      "=== epoch:62, train acc:0.7, test acc:0.7 ===\n",
      "train loss: 1.569549352101212\n",
      "train loss: 1.645416489551141\n",
      "train loss: 1.325678847666587\n",
      "train loss: 1.728671899013386\n",
      "train loss: 1.2701773347325513\n",
      "train loss: 1.7054122071438935\n",
      "train loss: 2.1650220087811634\n",
      "train loss: 1.089780598010798\n",
      "train loss: 1.0824838469707747\n",
      "=== epoch:63, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.9808875399202241\n",
      "train loss: 2.163548122634673\n",
      "train loss: 1.7559768393540565\n",
      "train loss: 1.4375222168525854\n",
      "train loss: 1.5652365524119012\n",
      "train loss: 1.447343944686707\n",
      "train loss: 1.6292004056262275\n",
      "train loss: 2.3999681700114643\n",
      "train loss: 1.904124500784946\n",
      "=== epoch:64, train acc:0.7, test acc:0.7 ===\n",
      "train loss: 1.8141077956105827\n",
      "train loss: 1.5690411893367984\n",
      "train loss: 1.913567379821245\n",
      "train loss: 1.7670332205173391\n",
      "train loss: 2.1131326996513766\n",
      "train loss: 1.3329884216535137\n",
      "train loss: 1.6827588361767958\n",
      "train loss: 2.0667016968390834\n",
      "train loss: 1.7101976299914374\n",
      "=== epoch:65, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.3757471163172392\n",
      "train loss: 1.0206131799825255\n",
      "train loss: 1.3890723412141976\n",
      "train loss: 2.240827855705163\n",
      "train loss: 1.8174124285987174\n",
      "train loss: 1.8690602109990337\n",
      "train loss: 1.695262257645471\n",
      "train loss: 2.0251883672491555\n",
      "train loss: 1.3061369833664058\n",
      "=== epoch:66, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.1722506310623384\n",
      "train loss: 1.5190533523787686\n",
      "train loss: 1.7149205515985044\n",
      "train loss: 1.8628402858485145\n",
      "train loss: 1.4733349599663759\n",
      "train loss: 1.550032114628688\n",
      "train loss: 1.5042079237404495\n",
      "train loss: 1.2140221752361058\n",
      "train loss: 1.8339683611083293\n",
      "=== epoch:67, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 0.8223620195050629\n",
      "train loss: 1.9092791816866803\n",
      "train loss: 1.2858803413023019\n",
      "train loss: 1.6967678865852376\n",
      "train loss: 0.6976128541527598\n",
      "train loss: 0.8170887028451703\n",
      "train loss: 1.0939997113828526\n",
      "train loss: 1.4047011532004412\n",
      "train loss: 1.4658794494387108\n",
      "=== epoch:68, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 0.8853379084521856\n",
      "train loss: 1.698335645442412\n",
      "train loss: 1.7303783195038656\n",
      "train loss: 0.7308269985811469\n",
      "train loss: 1.4585989969109974\n",
      "train loss: 1.61129985597536\n",
      "train loss: 1.7249068266404135\n",
      "train loss: 1.5565094636243135\n",
      "train loss: 1.2828937868845522\n",
      "=== epoch:69, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.2761648053426138\n",
      "train loss: 1.3100300630541282\n",
      "train loss: 1.575873128058966\n",
      "train loss: 2.1429223030201943\n",
      "train loss: 1.7484541248332426\n",
      "train loss: 1.5981530081363138\n",
      "train loss: 1.578880574018503\n",
      "train loss: 2.1320066644076823\n",
      "train loss: 1.4408582391466003\n",
      "=== epoch:70, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.8069875333946732\n",
      "train loss: 1.3842903855877542\n",
      "train loss: 1.4565445779993602\n",
      "train loss: 0.9704547679932154\n",
      "train loss: 1.7294631649419734\n",
      "train loss: 0.8931620614483522\n",
      "train loss: 0.6133000049495361\n",
      "train loss: 1.4898589526182517\n",
      "train loss: 1.3133479093865323\n",
      "=== epoch:71, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.7000678848943775\n",
      "train loss: 2.277101431220847\n",
      "train loss: 1.719626333661791\n",
      "train loss: 1.8934113092770155\n",
      "train loss: 0.9516032695164868\n",
      "train loss: 1.3756762968597727\n",
      "train loss: 1.1145558425632145\n",
      "train loss: 2.284060989440667\n",
      "train loss: 1.8282468057088375\n",
      "=== epoch:72, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.036716910950435\n",
      "train loss: 2.115006799350973\n",
      "train loss: 1.7880494795260153\n",
      "train loss: 1.4112808093013116\n",
      "train loss: 1.8934462587622356\n",
      "train loss: 1.3799560338175838\n",
      "train loss: 1.6351849131517406\n",
      "train loss: 2.2606831687028808\n",
      "train loss: 0.6849429352612616\n",
      "=== epoch:73, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.2603007086169338\n",
      "train loss: 1.1499068167499\n",
      "train loss: 1.4988317769615382\n",
      "train loss: 1.43173618207952\n",
      "train loss: 0.683126119609509\n",
      "train loss: 1.2990825898691376\n",
      "train loss: 1.4250499674046113\n",
      "train loss: 1.631136831896715\n",
      "train loss: 2.3475032646131755\n",
      "=== epoch:74, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.017552594850093\n",
      "train loss: 2.017699184332523\n",
      "train loss: 2.024789799257503\n",
      "train loss: 1.9021922349755434\n",
      "train loss: 1.3883649666400788\n",
      "train loss: 1.3468361371960371\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.0453660848606297\n",
      "train loss: 1.8744749589351561\n",
      "train loss: 1.718049622473072\n",
      "=== epoch:75, train acc:0.7, test acc:0.7 ===\n",
      "train loss: 2.026554889923746\n",
      "train loss: 1.5237234296428408\n",
      "train loss: 1.1999088861649896\n",
      "train loss: 1.5837541762296066\n",
      "train loss: 1.4387762270298199\n",
      "train loss: 1.2787118960976833\n",
      "train loss: 1.193239323065394\n",
      "train loss: 1.8236594799500905\n",
      "train loss: 1.3984168225532765\n",
      "=== epoch:76, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.5432330288075757\n",
      "train loss: 2.0452397836770295\n",
      "train loss: 2.297507203041451\n",
      "train loss: 1.684259105049402\n",
      "train loss: 1.4029879067765156\n",
      "train loss: 1.906548517505347\n",
      "train loss: 1.9504333614025744\n",
      "train loss: 1.9357822154298097\n",
      "train loss: 1.3842700615328736\n",
      "=== epoch:77, train acc:0.7111111111111111, test acc:0.7333333333333333 ===\n",
      "train loss: 1.613144158488454\n",
      "train loss: 1.525390287639118\n",
      "train loss: 1.9510399493018369\n",
      "train loss: 1.2723573518130784\n",
      "train loss: 2.1607314310490313\n",
      "train loss: 1.5623109767404804\n",
      "train loss: 1.4354187502227398\n",
      "train loss: 1.8119941248697473\n",
      "train loss: 1.3592211175514661\n",
      "=== epoch:78, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.4502618467520894\n",
      "train loss: 1.3470904796700847\n",
      "train loss: 1.2070647672137673\n",
      "train loss: 1.738420856578012\n",
      "train loss: 1.6259177289306495\n",
      "train loss: 1.6218406167114658\n",
      "train loss: 1.2153942954348487\n",
      "train loss: 0.8470215316867286\n",
      "train loss: 2.1452802881628994\n",
      "=== epoch:79, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.0298503951157307\n",
      "train loss: 1.8038465484618325\n",
      "train loss: 1.2433214906650814\n",
      "train loss: 1.7566152564384145\n",
      "train loss: 1.9080439281415706\n",
      "train loss: 1.4908043535894218\n",
      "train loss: 1.6536237389608834\n",
      "train loss: 1.2181172561103313\n",
      "train loss: 1.2509227433623387\n",
      "=== epoch:80, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.3939026041312843\n",
      "train loss: 1.6421315319839613\n",
      "train loss: 1.84432270303626\n",
      "train loss: 2.162153824798449\n",
      "train loss: 1.7429550796089144\n",
      "train loss: 1.9138421822211993\n",
      "train loss: 1.5568254319187735\n",
      "train loss: 0.9505573470382573\n",
      "train loss: 1.2395109234255424\n",
      "=== epoch:81, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.6084173917421483\n",
      "train loss: 2.1664557571479612\n",
      "train loss: 1.8830995614310715\n",
      "train loss: 1.2380123152103923\n",
      "train loss: 1.3361284420007227\n",
      "train loss: 1.2419211852019383\n",
      "train loss: 1.488997564211205\n",
      "train loss: 1.7199354937649198\n",
      "train loss: 2.056144889562653\n",
      "=== epoch:82, train acc:0.7555555555555555, test acc:0.8 ===\n",
      "train loss: 1.5140082048934387\n",
      "train loss: 1.4393636556182492\n",
      "train loss: 1.4738468380013372\n",
      "train loss: 1.8147004967943932\n",
      "train loss: 2.1253307486932114\n",
      "train loss: 1.5846809194107914\n",
      "train loss: 1.561201124752029\n",
      "train loss: 1.2012343583848253\n",
      "train loss: 1.5919041767515423\n",
      "=== epoch:83, train acc:0.7111111111111111, test acc:0.7333333333333333 ===\n",
      "train loss: 1.788238930612118\n",
      "train loss: 1.5658714555034512\n",
      "train loss: 1.197450242247526\n",
      "train loss: 1.6123396960054481\n",
      "train loss: 1.2723521524683092\n",
      "train loss: 1.6939864518660024\n",
      "train loss: 1.6484378886497562\n",
      "train loss: 2.131145686769165\n",
      "train loss: 1.8752792509396552\n",
      "=== epoch:84, train acc:0.7, test acc:0.7 ===\n",
      "train loss: 2.0774009039160672\n",
      "train loss: 2.1378020793456214\n",
      "train loss: 1.2783500503583207\n",
      "train loss: 1.6633271709625403\n",
      "train loss: 1.3751449710050998\n",
      "train loss: 1.813232719436812\n",
      "train loss: 1.4913314008181513\n",
      "train loss: 1.3692356563334072\n",
      "train loss: 1.5481233383585598\n",
      "=== epoch:85, train acc:0.6888888888888889, test acc:0.7 ===\n",
      "train loss: 1.9173336329292825\n",
      "train loss: 1.383127942955752\n",
      "train loss: 1.2206214989685686\n",
      "train loss: 1.1316589535850858\n",
      "train loss: 0.8411649713760022\n",
      "train loss: 1.6334381247516006\n",
      "train loss: 0.645081329292227\n",
      "train loss: 1.1292222063075938\n",
      "train loss: 1.484991971850233\n",
      "=== epoch:86, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 2.0211640003464515\n",
      "train loss: 0.870925204852139\n",
      "train loss: 1.1640845224464142\n",
      "train loss: 0.5095060043276435\n",
      "train loss: 0.6454044749570131\n",
      "train loss: 0.9471004205947468\n",
      "train loss: 2.1153723706330467\n",
      "train loss: 1.1464853120225569\n",
      "train loss: 0.7385133455963514\n",
      "=== epoch:87, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 0.7898531423514592\n",
      "train loss: 2.176953134040881\n",
      "train loss: 2.344431143329046\n",
      "train loss: 0.8417721857273578\n",
      "train loss: 1.7066435945784815\n",
      "train loss: 1.3081438896513267\n",
      "train loss: 1.5724609830895404\n",
      "train loss: 2.637650827449096\n",
      "train loss: 1.0258371266286832\n",
      "=== epoch:88, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.144119558117124\n",
      "train loss: 1.4441940347432347\n",
      "train loss: 1.7819375310696293\n",
      "train loss: 1.0585351523009716\n",
      "train loss: 2.109240862622484\n",
      "train loss: 1.6049458552832887\n",
      "train loss: 1.1552780774061921\n",
      "train loss: 0.9191131805953416\n",
      "train loss: 0.8828531084490103\n",
      "=== epoch:89, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.7216387485830762\n",
      "train loss: 1.4322891871558898\n",
      "train loss: 1.7935561254367711\n",
      "train loss: 1.2698682799498642\n",
      "train loss: 1.1591269766870493\n",
      "train loss: 1.5194080135957415\n",
      "train loss: 1.357321602463658\n",
      "train loss: 1.2027915564542335\n",
      "train loss: 1.3169896942604513\n",
      "=== epoch:90, train acc:0.6888888888888889, test acc:0.6 ===\n",
      "train loss: 1.7945074300374853\n",
      "train loss: 1.3052338577829046\n",
      "train loss: 1.7385092182489088\n",
      "train loss: 1.5009631170086468\n",
      "train loss: 1.7742303112271225\n",
      "train loss: 1.6666337645171425\n",
      "train loss: 0.7010982314495137\n",
      "train loss: 1.3928799383069121\n",
      "train loss: 2.746471533738734\n",
      "=== epoch:91, train acc:0.6888888888888889, test acc:0.6333333333333333 ===\n",
      "train loss: 1.8633591610931062\n",
      "train loss: 1.8983715325754966\n",
      "train loss: 1.4984739565422613\n",
      "train loss: 1.1881476466958059\n",
      "train loss: 1.2646265578082017\n",
      "train loss: 2.37539506968731\n",
      "train loss: 1.832809197462391\n",
      "train loss: 1.6266926708114626\n",
      "train loss: 1.8908125259709951\n",
      "=== epoch:92, train acc:0.9, test acc:0.9333333333333333 ===\n",
      "Reached accuracy_limit\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.9333333333333333\n",
      "[4, 10, 3]\n",
      "train loss: 7.7922543599103635\n",
      "=== epoch:1, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 7.689280310682809\n",
      "train loss: 7.269285428317134\n",
      "train loss: 6.247735443936485\n",
      "train loss: 7.24488989142956\n",
      "train loss: 5.3067163561133075\n",
      "train loss: 7.419148245817693\n",
      "train loss: 7.4688937152678445\n",
      "train loss: 6.598114270944202\n",
      "train loss: 6.626298416644721\n",
      "=== epoch:2, train acc:0.3333333333333333, test acc:0.3333333333333333 ===\n",
      "train loss: 6.470975643952398\n",
      "train loss: 5.007414257312819\n",
      "train loss: 5.045528674547619\n",
      "train loss: 6.135982101109541\n",
      "train loss: 6.6959153078268425\n",
      "train loss: 5.5928614890954815\n",
      "train loss: 6.421114180125279\n",
      "train loss: 5.889067213313773\n",
      "train loss: 4.6198438231499335\n",
      "=== epoch:3, train acc:0.3333333333333333, test acc:0.3333333333333333 ===\n",
      "train loss: 5.987945401403739\n",
      "train loss: 5.812681491690607\n",
      "train loss: 5.344561693016256\n",
      "train loss: 4.697751709786266\n",
      "train loss: 6.707311395770032\n",
      "train loss: 4.88574293886485\n",
      "train loss: 4.453579453718438\n",
      "train loss: 4.6196689911533\n",
      "train loss: 4.641216333849238\n",
      "=== epoch:4, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 6.085175107877859\n",
      "train loss: 5.959149282416266\n",
      "train loss: 4.852906354465497\n",
      "train loss: 6.036472571212339\n",
      "train loss: 4.888907317159545\n",
      "train loss: 4.953425345275284\n",
      "train loss: 5.2503353607776635\n",
      "train loss: 5.1546333557397\n",
      "train loss: 4.341107582059071\n",
      "=== epoch:5, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 6.033370890948134\n",
      "train loss: 6.111756597926663\n",
      "train loss: 4.965002145492488\n",
      "train loss: 6.507228517922724\n",
      "train loss: 5.216738565034235\n",
      "train loss: 6.140059297856964\n",
      "train loss: 5.4434965062475875\n",
      "train loss: 4.230936360155135\n",
      "train loss: 5.587494720419875\n",
      "=== epoch:6, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 5.67074275878137\n",
      "train loss: 6.34446090004611\n",
      "train loss: 4.780895242306479\n",
      "train loss: 4.81935717089451\n",
      "train loss: 6.329054003431569\n",
      "train loss: 5.29475973298049\n",
      "train loss: 5.888240661014635\n",
      "train loss: 5.466984809965488\n",
      "train loss: 5.2768724023247895\n",
      "=== epoch:7, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 5.836623696651351\n",
      "train loss: 5.012350787008499\n",
      "train loss: 5.591827367653618\n",
      "train loss: 5.079630956515694\n",
      "train loss: 4.922278667677733\n",
      "train loss: 4.808557582659351\n",
      "train loss: 4.8536445066891165\n",
      "train loss: 4.103610518179285\n",
      "train loss: 4.0069642355317905\n",
      "=== epoch:8, train acc:0.3333333333333333, test acc:0.3333333333333333 ===\n",
      "train loss: 3.643061590409456\n",
      "train loss: 5.4552247164598295\n",
      "train loss: 3.7430344527394057\n",
      "train loss: 4.959275236910596\n",
      "train loss: 5.307690693985943\n",
      "train loss: 4.5963628211181256\n",
      "train loss: 5.399070100668294\n",
      "train loss: 5.524742371238889\n",
      "train loss: 5.145929285765044\n",
      "=== epoch:9, train acc:0.3333333333333333, test acc:0.3333333333333333 ===\n",
      "train loss: 6.285074363959962\n",
      "train loss: 3.881290544903348\n",
      "train loss: 5.102461907309066\n",
      "train loss: 5.115467498806665\n",
      "train loss: 4.928689393340865\n",
      "train loss: 4.937204431150983\n",
      "train loss: 5.4545622001371274\n",
      "train loss: 5.542222067979693\n",
      "train loss: 4.116886882514349\n",
      "=== epoch:10, train acc:0.3333333333333333, test acc:0.3333333333333333 ===\n",
      "train loss: 4.346379129212133\n",
      "train loss: 4.569055979333235\n",
      "train loss: 5.560748684842027\n",
      "train loss: 2.9748043694429835\n",
      "train loss: 5.601403790606351\n",
      "train loss: 3.7833773737662404\n",
      "train loss: 4.490114450072221\n",
      "train loss: 5.021112033281264\n",
      "train loss: 4.7295619825370805\n",
      "=== epoch:11, train acc:0.3333333333333333, test acc:0.3333333333333333 ===\n",
      "train loss: 4.402095317992828\n",
      "train loss: 3.915823428721586\n",
      "train loss: 4.400393415802152\n",
      "train loss: 4.072132351720185\n",
      "train loss: 4.999136832944007\n",
      "train loss: 4.321687804925411\n",
      "train loss: 4.409124915966018\n",
      "train loss: 3.941754602530925\n",
      "train loss: 4.505709904762464\n",
      "=== epoch:12, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 4.6721577994939665\n",
      "train loss: 4.660872773411234\n",
      "train loss: 3.81339088252231\n",
      "train loss: 4.043491097458578\n",
      "train loss: 4.590088000660616\n",
      "train loss: 4.223195163538011\n",
      "train loss: 5.01647538674177\n",
      "train loss: 3.5659149171383837\n",
      "train loss: 4.6634481730567705\n",
      "=== epoch:13, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 3.592911511390912\n",
      "train loss: 3.1165280993334807\n",
      "train loss: 3.8048942696189902\n",
      "train loss: 3.1620729981749913\n",
      "train loss: 5.101404134073289\n",
      "train loss: 4.411720119462011\n",
      "train loss: 4.50951041050702\n",
      "train loss: 3.31482039602096\n",
      "train loss: 3.0477748968987095\n",
      "=== epoch:14, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 3.7874635340817977\n",
      "train loss: 4.365561145205582\n",
      "train loss: 4.053432224202071\n",
      "train loss: 4.025664056059343\n",
      "train loss: 3.624872917449105\n",
      "train loss: 3.5538140169370167\n",
      "train loss: 4.491046246538727\n",
      "train loss: 3.825320257891047\n",
      "train loss: 4.070165180090459\n",
      "=== epoch:15, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 3.7114470661869996\n",
      "train loss: 4.196675858214722\n",
      "train loss: 3.460015703146493\n",
      "train loss: 4.277857203126963\n",
      "train loss: 3.903307310346864\n",
      "train loss: 3.778181309756408\n",
      "train loss: 3.6923086317820624\n",
      "train loss: 4.029996028552937\n",
      "train loss: 3.853755939316195\n",
      "=== epoch:16, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 3.1035600946620128\n",
      "train loss: 3.583112473102393\n",
      "train loss: 3.764546585590531\n",
      "train loss: 3.910329354211566\n",
      "train loss: 3.4376672577069445\n",
      "train loss: 3.5364358484552163\n",
      "train loss: 3.6466856185518597\n",
      "train loss: 3.585982031198936\n",
      "train loss: 3.419655693819378\n",
      "=== epoch:17, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 3.6245510224177373\n",
      "train loss: 3.5421292071721076\n",
      "train loss: 3.56835822052681\n",
      "train loss: 3.447995342927006\n",
      "train loss: 3.1253663845389292\n",
      "train loss: 3.4411828271546914\n",
      "train loss: 3.0097380811274306\n",
      "train loss: 3.1203993221990847\n",
      "train loss: 3.3214863711485103\n",
      "=== epoch:18, train acc:0.34444444444444444, test acc:0.3333333333333333 ===\n",
      "train loss: 3.4300427943958014\n",
      "train loss: 3.197077798391169\n",
      "train loss: 3.3605141322637193\n",
      "train loss: 3.3268951111139704\n",
      "train loss: 3.0378403222767623\n",
      "train loss: 3.3049907115607775\n",
      "train loss: 3.2607569131861642\n",
      "train loss: 3.045680080015964\n",
      "train loss: 3.0497487560652203\n",
      "=== epoch:19, train acc:0.35555555555555557, test acc:0.3333333333333333 ===\n",
      "train loss: 2.9684807495867584\n",
      "train loss: 3.218058115790183\n",
      "train loss: 3.162869555995002\n",
      "train loss: 3.1499770064620396\n",
      "train loss: 3.0606551281022187\n",
      "train loss: 2.879088489512191\n",
      "train loss: 3.0451386464086365\n",
      "train loss: 2.961514802328132\n",
      "train loss: 3.2712390049930846\n",
      "=== epoch:20, train acc:0.4222222222222222, test acc:0.36666666666666664 ===\n",
      "train loss: 2.7759986073876814\n",
      "train loss: 2.7199111899883945\n",
      "train loss: 2.9892759188568068\n",
      "train loss: 3.032795240726422\n",
      "train loss: 2.931199355366327\n",
      "train loss: 3.02897135848164\n",
      "train loss: 2.8802457794980123\n",
      "train loss: 3.0331669173302522\n",
      "train loss: 2.8920905895117452\n",
      "=== epoch:21, train acc:0.5555555555555556, test acc:0.6 ===\n",
      "train loss: 2.5281770886270207\n",
      "train loss: 2.9207385363832046\n",
      "train loss: 2.502872325037922\n",
      "train loss: 3.0780846540969335\n",
      "train loss: 2.9637590201063526\n",
      "train loss: 2.6011715376867546\n",
      "train loss: 2.937179185070071\n",
      "train loss: 2.7240206378394562\n",
      "train loss: 2.486085032349394\n",
      "=== epoch:22, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 2.7011347660372174\n",
      "train loss: 2.7616244658556934\n",
      "train loss: 3.051525859890151\n",
      "train loss: 2.493480673505127\n",
      "train loss: 2.493465379952151\n",
      "train loss: 2.9201535405809858\n",
      "train loss: 2.405819705807188\n",
      "train loss: 2.8192995047666267\n",
      "train loss: 2.9663156375539006\n",
      "=== epoch:23, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 2.606214285709148\n",
      "train loss: 2.3066092646432685\n",
      "train loss: 2.683325054257629\n",
      "train loss: 2.5763165020331016\n",
      "train loss: 2.6415823459955377\n",
      "train loss: 2.2390962443357227\n",
      "train loss: 2.7966894790359826\n",
      "train loss: 2.250851908592489\n",
      "train loss: 2.5007864537921396\n",
      "=== epoch:24, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 2.6042313552229674\n",
      "train loss: 2.8193529862541746\n",
      "train loss: 2.5192378621000846\n",
      "train loss: 2.46362107307842\n",
      "train loss: 2.2588920263747614\n",
      "train loss: 2.7995102647881094\n",
      "train loss: 2.588592782964397\n",
      "train loss: 2.3608997741458757\n",
      "train loss: 2.495977610742074\n",
      "=== epoch:25, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 2.209880836613561\n",
      "train loss: 2.2933958950038806\n",
      "train loss: 2.4784938807024597\n",
      "train loss: 2.665164325387133\n",
      "train loss: 2.5086781940332643\n",
      "train loss: 2.373265112597852\n",
      "train loss: 2.3819923082607044\n",
      "train loss: 1.700149738695042\n",
      "train loss: 2.249927396669039\n",
      "=== epoch:26, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 2.4626060868675683\n",
      "train loss: 2.3058441953554456\n",
      "train loss: 2.3795425314442453\n",
      "train loss: 2.5342385588619605\n",
      "train loss: 2.5758726758550075\n",
      "train loss: 2.1740459085434978\n",
      "train loss: 2.086848504559797\n",
      "train loss: 2.186019739063138\n",
      "train loss: 2.1847502480996597\n",
      "=== epoch:27, train acc:0.6666666666666666, test acc:0.6666666666666666 ===\n",
      "train loss: 2.435498328886285\n",
      "train loss: 2.0758907048860142\n",
      "train loss: 1.776445860611249\n",
      "train loss: 1.7254426176034527\n",
      "train loss: 2.3732634224180074\n",
      "train loss: 2.2492656093350525\n",
      "train loss: 1.8043358027515382\n",
      "train loss: 2.2813167990749705\n",
      "train loss: 2.3818891916950573\n",
      "=== epoch:28, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 2.052392230047316\n",
      "train loss: 2.243960190252641\n",
      "train loss: 2.2898637774265156\n",
      "train loss: 1.7700728792406597\n",
      "train loss: 1.6790792813773268\n",
      "train loss: 2.01348452783113\n",
      "train loss: 2.3456444585576413\n",
      "train loss: 2.049763526824151\n",
      "train loss: 2.0368436399213308\n",
      "=== epoch:29, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 2.465930864145935\n",
      "train loss: 2.7454922540894438\n",
      "train loss: 2.0877048802232094\n",
      "train loss: 2.238042929903627\n",
      "train loss: 2.045410180996683\n",
      "train loss: 1.9256702353077277\n",
      "train loss: 2.159165995634445\n",
      "train loss: 2.3307397901480478\n",
      "train loss: 2.1942009243803344\n",
      "=== epoch:30, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 2.168291119498562\n",
      "train loss: 1.4535801382759312\n",
      "train loss: 1.902031422171518\n",
      "train loss: 2.1995887249293835\n",
      "train loss: 2.20715173978352\n",
      "train loss: 2.3173862483018577\n",
      "train loss: 2.274742205342364\n",
      "train loss: 1.7955782300201661\n",
      "train loss: 1.6791048265200286\n",
      "=== epoch:31, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.3319429745353641\n",
      "train loss: 1.7183573633240112\n",
      "train loss: 2.2076795454503824\n",
      "train loss: 1.5492763550113267\n",
      "train loss: 1.6031893901780934\n",
      "train loss: 1.5049690401563809\n",
      "train loss: 1.8621028144904315\n",
      "train loss: 2.159923779747771\n",
      "train loss: 1.9041088864405333\n",
      "=== epoch:32, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 2.040075792456254\n",
      "train loss: 2.4277459085640256\n",
      "train loss: 2.1612589056476263\n",
      "train loss: 2.0313805274738925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.9315248476733438\n",
      "train loss: 2.3642842835210516\n",
      "train loss: 1.5837210037180176\n",
      "train loss: 1.824892904691734\n",
      "train loss: 2.0146953633664535\n",
      "=== epoch:33, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.5197187121621814\n",
      "train loss: 1.3570072055421802\n",
      "train loss: 2.085117860138108\n",
      "train loss: 1.6793236853364693\n",
      "train loss: 1.841756392475629\n",
      "train loss: 1.8447315158403017\n",
      "train loss: 1.8982161440075984\n",
      "train loss: 1.9308070575260181\n",
      "train loss: 1.7542635972229301\n",
      "=== epoch:34, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.5454081943282603\n",
      "train loss: 1.8646932496577604\n",
      "train loss: 1.6413927028325177\n",
      "train loss: 2.0989241175742217\n",
      "train loss: 2.4270814353918597\n",
      "train loss: 1.8325749122183521\n",
      "train loss: 1.8906264161934374\n",
      "train loss: 2.255005563635315\n",
      "train loss: 2.415847021681916\n",
      "=== epoch:35, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.8883812162422304\n",
      "train loss: 1.7754445599806048\n",
      "train loss: 1.8430306395108238\n",
      "train loss: 2.167706044148081\n",
      "train loss: 1.885062608144949\n",
      "train loss: 1.7127777090408018\n",
      "train loss: 1.9550798994091516\n",
      "train loss: 2.0608763450451457\n",
      "train loss: 1.7484404552261164\n",
      "=== epoch:36, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.6303509802907852\n",
      "train loss: 1.7945224354641787\n",
      "train loss: 2.473654801219394\n",
      "train loss: 1.9554762851552578\n",
      "train loss: 2.3740801473027546\n",
      "train loss: 2.2052320196699076\n",
      "train loss: 2.1260307161868437\n",
      "train loss: 2.183712931667337\n",
      "train loss: 1.769134911874855\n",
      "=== epoch:37, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.3707919629923409\n",
      "train loss: 2.231636766440213\n",
      "train loss: 2.0038146752941457\n",
      "train loss: 1.5834471476488998\n",
      "train loss: 1.995321781470593\n",
      "train loss: 1.4997787180584057\n",
      "train loss: 1.7728754441382382\n",
      "train loss: 1.688480723738909\n",
      "train loss: 1.9883000517286291\n",
      "=== epoch:38, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.745156485231571\n",
      "train loss: 1.992135188743592\n",
      "train loss: 1.4777352434563331\n",
      "train loss: 1.830443698331922\n",
      "train loss: 2.0094555712809226\n",
      "train loss: 1.4827478011202013\n",
      "train loss: 1.708091602183105\n",
      "train loss: 1.8121480916281776\n",
      "train loss: 2.0113207645677855\n",
      "=== epoch:39, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.585291895213043\n",
      "train loss: 2.1684729208726203\n",
      "train loss: 2.4877447837194553\n",
      "train loss: 1.9497155453846176\n",
      "train loss: 1.8796646976114868\n",
      "train loss: 1.308649575938406\n",
      "train loss: 1.176888862195987\n",
      "train loss: 1.7720586402663077\n",
      "train loss: 1.8382467194646022\n",
      "=== epoch:40, train acc:0.6777777777777778, test acc:0.6666666666666666 ===\n",
      "train loss: 1.103230419495854\n",
      "train loss: 2.05572464606191\n",
      "train loss: 2.065583845917087\n",
      "train loss: 1.5214791015546814\n",
      "train loss: 1.59006209322087\n",
      "train loss: 1.9550897623427073\n",
      "train loss: 2.3183035868738116\n",
      "train loss: 1.4561952590409781\n",
      "train loss: 1.6488956114117652\n",
      "=== epoch:41, train acc:0.7, test acc:0.6666666666666666 ===\n",
      "train loss: 1.8251433521258131\n",
      "train loss: 1.8021857494386173\n",
      "train loss: 1.6629134608061074\n",
      "train loss: 1.5762050125632046\n",
      "train loss: 2.1466735152135894\n",
      "train loss: 1.9275482193228075\n",
      "train loss: 1.6369427909178593\n",
      "train loss: 1.8463178044490014\n",
      "train loss: 1.627797587289794\n",
      "=== epoch:42, train acc:0.7111111111111111, test acc:0.6666666666666666 ===\n",
      "train loss: 1.5416865057907716\n",
      "train loss: 1.3605944939087455\n",
      "train loss: 2.1877613444696986\n",
      "train loss: 1.9051285964206244\n",
      "train loss: 1.734166234803553\n",
      "train loss: 1.5982246819733608\n",
      "train loss: 1.7607625589884142\n",
      "train loss: 2.103826455780574\n",
      "train loss: 1.6906208710989155\n",
      "=== epoch:43, train acc:0.7222222222222222, test acc:0.6666666666666666 ===\n",
      "train loss: 1.7710069144923197\n",
      "train loss: 1.6836441425997006\n",
      "train loss: 1.6587821665561744\n",
      "train loss: 1.6432730582621804\n",
      "train loss: 2.2942027676974814\n",
      "train loss: 2.083446524018501\n",
      "train loss: 1.04935845955479\n",
      "train loss: 1.8081293582025213\n",
      "train loss: 1.7066909169970375\n",
      "=== epoch:44, train acc:0.7111111111111111, test acc:0.6666666666666666 ===\n",
      "train loss: 2.3570855716957655\n",
      "train loss: 1.5586665974455054\n",
      "train loss: 1.9583856547087979\n",
      "train loss: 1.8697516762230597\n",
      "train loss: 1.1526885740871797\n",
      "train loss: 1.870984726626172\n",
      "train loss: 1.520362805657766\n",
      "train loss: 1.9149651437906818\n",
      "train loss: 0.8884409183133886\n",
      "=== epoch:45, train acc:0.6888888888888889, test acc:0.6666666666666666 ===\n",
      "train loss: 1.8067531830168404\n",
      "train loss: 2.1970514547422044\n",
      "train loss: 1.390611112620665\n",
      "train loss: 1.13317762883924\n",
      "train loss: 1.8195685499720664\n",
      "train loss: 2.4676713307359632\n",
      "train loss: 1.9812286858202224\n",
      "train loss: 1.9300427526616095\n",
      "train loss: 2.109319263397152\n",
      "=== epoch:46, train acc:0.6777777777777778, test acc:0.6666666666666666 ===\n",
      "train loss: 1.633216676730915\n",
      "train loss: 1.209763812369181\n",
      "train loss: 1.492743245926395\n",
      "train loss: 1.4458082805646097\n",
      "train loss: 0.9104405913627268\n",
      "train loss: 2.340886565396755\n",
      "train loss: 1.8610251283437906\n",
      "train loss: 1.7543944876832294\n",
      "train loss: 2.0667795743440944\n",
      "=== epoch:47, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.2074662102964135\n",
      "train loss: 1.5563887930041544\n",
      "train loss: 1.8097870905454734\n",
      "train loss: 2.0925915592295548\n",
      "train loss: 2.102265186133953\n",
      "train loss: 1.2510825577135376\n",
      "train loss: 1.576826594685162\n",
      "train loss: 1.9423782636247766\n",
      "train loss: 1.317039340312978\n",
      "=== epoch:48, train acc:0.6555555555555556, test acc:0.6666666666666666 ===\n",
      "train loss: 1.1867151416857786\n",
      "train loss: 1.4750104473508319\n",
      "train loss: 2.232873403409862\n",
      "train loss: 1.3544675868969267\n",
      "train loss: 1.9994005990601391\n",
      "train loss: 1.6027186962911684\n",
      "train loss: 2.045048500395197\n",
      "train loss: 1.455847483785014\n",
      "train loss: 1.7919077863277149\n",
      "=== epoch:49, train acc:0.6666666666666666, test acc:0.6666666666666666 ===\n",
      "train loss: 2.023120986923189\n",
      "train loss: 1.6276319624845554\n",
      "train loss: 1.6425967327650581\n",
      "train loss: 1.0336472276359892\n",
      "train loss: 1.2221847118476128\n",
      "train loss: 1.2245176570804528\n",
      "train loss: 1.8308554779286157\n",
      "train loss: 1.5906250921772656\n",
      "train loss: 1.3449868489788714\n",
      "=== epoch:50, train acc:0.7, test acc:0.6666666666666666 ===\n",
      "train loss: 1.4997989320664091\n",
      "train loss: 1.5759693522542797\n",
      "train loss: 1.7709331373085715\n",
      "train loss: 1.4707214825399724\n",
      "train loss: 1.1869659866054671\n",
      "train loss: 1.7619411408896626\n",
      "train loss: 2.0810174041731107\n",
      "train loss: 1.396652351196908\n",
      "train loss: 1.569781932721297\n",
      "=== epoch:51, train acc:0.7111111111111111, test acc:0.6666666666666666 ===\n",
      "train loss: 0.9905618882640318\n",
      "train loss: 0.7020937923659287\n",
      "train loss: 1.6299051035476022\n",
      "train loss: 1.6007313206124034\n",
      "train loss: 2.3106617538289904\n",
      "train loss: 1.125813527281719\n",
      "train loss: 1.4159124592478232\n",
      "train loss: 2.245354885249779\n",
      "train loss: 1.4713414454093134\n",
      "=== epoch:52, train acc:0.8222222222222222, test acc:0.7 ===\n",
      "train loss: 1.4498623789119536\n",
      "train loss: 1.7842070393218863\n",
      "train loss: 2.0279780211707887\n",
      "train loss: 1.301915797533792\n",
      "train loss: 1.712569644516817\n",
      "train loss: 1.790461946471922\n",
      "train loss: 1.601591089256435\n",
      "train loss: 1.6034117768333787\n",
      "train loss: 1.662894572400638\n",
      "=== epoch:53, train acc:0.8555555555555555, test acc:0.7 ===\n",
      "train loss: 0.7378471138454326\n",
      "train loss: 2.041188914515003\n",
      "train loss: 1.4417893410567861\n",
      "train loss: 0.9856816975508417\n",
      "train loss: 2.0568163116047136\n",
      "train loss: 2.167541553382334\n",
      "train loss: 1.5014463598698027\n",
      "train loss: 1.788442942428331\n",
      "train loss: 1.8606557824092307\n",
      "=== epoch:54, train acc:0.8888888888888888, test acc:0.7333333333333333 ===\n",
      "train loss: 1.6808713034676321\n",
      "train loss: 1.566157448739084\n",
      "train loss: 1.7601220894422234\n",
      "train loss: 1.1212424950728241\n",
      "train loss: 1.0864599248037294\n",
      "train loss: 1.4608263148491156\n",
      "train loss: 1.6181075735576833\n",
      "train loss: 1.844972530308725\n",
      "train loss: 1.321259884099391\n",
      "=== epoch:55, train acc:0.8888888888888888, test acc:0.7333333333333333 ===\n",
      "train loss: 1.529735640766599\n",
      "train loss: 1.7089816096196893\n",
      "train loss: 1.119949219720338\n",
      "train loss: 1.6863341356095731\n",
      "train loss: 1.9062889676873804\n",
      "train loss: 1.5186785571064727\n",
      "train loss: 0.857209285862175\n",
      "train loss: 1.6484474770400848\n",
      "train loss: 1.2445046104291455\n",
      "=== epoch:56, train acc:0.8555555555555555, test acc:0.7333333333333333 ===\n",
      "train loss: 1.6120774942004343\n",
      "train loss: 1.7001087258306826\n",
      "train loss: 1.8356271954285903\n",
      "train loss: 1.2043302226910335\n",
      "train loss: 1.6183381077898176\n",
      "train loss: 1.7154434194017194\n",
      "train loss: 1.6098971007784413\n",
      "train loss: 1.1757908706125448\n",
      "train loss: 1.0531315733881543\n",
      "=== epoch:57, train acc:0.8444444444444444, test acc:0.7 ===\n",
      "train loss: 1.4360755665447855\n",
      "train loss: 1.3797435671355247\n",
      "train loss: 1.8175290372111557\n",
      "train loss: 1.351759799960111\n",
      "train loss: 0.9839105903731219\n",
      "train loss: 1.99837758741525\n",
      "train loss: 1.679749266552609\n",
      "train loss: 1.6977931759846707\n",
      "train loss: 1.4258688522205527\n",
      "=== epoch:58, train acc:0.8111111111111111, test acc:0.7 ===\n",
      "train loss: 2.0710612871266028\n",
      "train loss: 1.3662335421610874\n",
      "train loss: 2.471313151246858\n",
      "train loss: 1.3215504446334074\n",
      "train loss: 1.7419556466011654\n",
      "train loss: 1.7227114260806637\n",
      "train loss: 2.046542183796645\n",
      "train loss: 1.6134454735193067\n",
      "train loss: 1.6251461944191086\n",
      "=== epoch:59, train acc:0.8555555555555555, test acc:0.7 ===\n",
      "train loss: 1.7914542181648339\n",
      "train loss: 2.095025722342384\n",
      "train loss: 1.226960130264484\n",
      "train loss: 1.5227135594826349\n",
      "train loss: 1.239269736716745\n",
      "train loss: 1.534630525277356\n",
      "train loss: 1.7167273613827152\n",
      "train loss: 1.983337085565954\n",
      "train loss: 2.294025883255325\n",
      "=== epoch:60, train acc:0.8888888888888888, test acc:0.7333333333333333 ===\n",
      "train loss: 1.8565127831338386\n",
      "train loss: 1.725733617717367\n",
      "train loss: 1.719710180329533\n",
      "train loss: 1.5238660036169696\n",
      "train loss: 1.2490225485454605\n",
      "train loss: 1.3863641159134124\n",
      "train loss: 1.177479011956909\n",
      "train loss: 1.1015710455773116\n",
      "train loss: 1.2645504581902016\n",
      "=== epoch:61, train acc:0.9, test acc:0.7333333333333333 ===\n",
      "train loss: 1.677290314776446\n",
      "train loss: 1.0697515687330796\n",
      "train loss: 1.417571226052362\n",
      "train loss: 1.917991565588041\n",
      "train loss: 0.6144939942827408\n",
      "train loss: 1.3586850775897803\n",
      "train loss: 1.6971895721246946\n",
      "train loss: 1.5294049752701706\n",
      "train loss: 1.26724906825495\n",
      "=== epoch:62, train acc:0.9, test acc:0.7333333333333333 ===\n",
      "train loss: 1.2210744434675798\n",
      "train loss: 1.0410937385351824\n",
      "train loss: 1.1161131902425734\n",
      "train loss: 1.284994988283984\n",
      "train loss: 1.1026532165739944\n",
      "train loss: 1.53037317996579\n",
      "train loss: 1.5206692336237833\n",
      "train loss: 1.2405799024234763\n",
      "train loss: 1.8271053842551555\n",
      "=== epoch:63, train acc:0.8222222222222222, test acc:0.7 ===\n",
      "train loss: 1.1734590316767748\n",
      "train loss: 1.8891318667370156\n",
      "train loss: 1.6713821124522508\n",
      "train loss: 1.8526401820894427\n",
      "train loss: 1.8243513412389643\n",
      "train loss: 1.3515903509111364\n",
      "train loss: 1.2882889956254635\n",
      "train loss: 2.0095595595284834\n",
      "train loss: 1.3394653183226788\n",
      "=== epoch:64, train acc:0.8111111111111111, test acc:0.7 ===\n",
      "train loss: 1.4570608614461122\n",
      "train loss: 1.44102795642746\n",
      "train loss: 1.2926340719029876\n",
      "train loss: 1.8763436382355752\n",
      "train loss: 1.7452324606975094\n",
      "train loss: 1.8673921381466796\n",
      "train loss: 1.6903321863658525\n",
      "train loss: 1.2667472374569877\n",
      "train loss: 1.4802329382011474\n",
      "=== epoch:65, train acc:0.8555555555555555, test acc:0.7333333333333333 ===\n",
      "train loss: 1.6489521847309823\n",
      "train loss: 1.8339667234744426\n",
      "train loss: 1.882362319256036\n",
      "train loss: 1.3374232991947042\n",
      "train loss: 2.0343218441141615\n",
      "train loss: 1.7958157463422078\n",
      "train loss: 1.2851141925692295\n",
      "train loss: 1.263408732257644\n",
      "train loss: 1.3098304873138822\n",
      "=== epoch:66, train acc:0.9, test acc:0.7333333333333333 ===\n",
      "train loss: 1.4564051435548357\n",
      "train loss: 1.4958424125909706\n",
      "train loss: 1.389701200621822\n",
      "train loss: 1.8103641345326302\n",
      "train loss: 1.8072704569523736\n",
      "train loss: 1.6796312070668806\n",
      "train loss: 1.6002312835195225\n",
      "train loss: 0.40151194310202065\n",
      "train loss: 1.4102556566515811\n",
      "=== epoch:67, train acc:0.9, test acc:0.7333333333333333 ===\n",
      "train loss: 1.6379704959662527\n",
      "train loss: 2.0487941605907336\n",
      "train loss: 1.6058648176429622\n",
      "train loss: 0.9923256088239107\n",
      "train loss: 1.5535530356351153\n",
      "train loss: 1.8717008623173386\n",
      "train loss: 1.3734305148660892\n",
      "train loss: 1.734755132529322\n",
      "train loss: 2.1245226829594355\n",
      "=== epoch:68, train acc:0.9, test acc:0.7333333333333333 ===\n",
      "train loss: 1.986891280800447\n",
      "train loss: 1.5944241114035203\n",
      "train loss: 1.5849141471983597\n",
      "train loss: 1.9678408085739212\n",
      "train loss: 1.3517913396178096\n",
      "train loss: 1.516085091963782\n",
      "train loss: 1.5476781790457173\n",
      "train loss: 1.3189930505088245\n",
      "train loss: 1.894701811032183\n",
      "=== epoch:69, train acc:0.9111111111111111, test acc:0.7333333333333333 ===\n",
      "train loss: 1.894316491110135\n",
      "train loss: 1.4662097978435584\n",
      "train loss: 1.4190038060662549\n",
      "train loss: 1.042751442740388\n",
      "train loss: 1.4949719573725877\n",
      "train loss: 1.6439773005829839\n",
      "train loss: 1.549265321758311\n",
      "train loss: 1.6482455867960968\n",
      "train loss: 1.3855508545500335\n",
      "=== epoch:70, train acc:0.9222222222222223, test acc:0.7333333333333333 ===\n",
      "train loss: 1.5374429363579156\n",
      "train loss: 1.8125542656267912\n",
      "train loss: 1.5938243017188405\n",
      "train loss: 2.0615105719698934\n",
      "train loss: 1.2846345151179832\n",
      "train loss: 1.3753905177915506\n",
      "train loss: 1.6672340728678816\n",
      "train loss: 1.0178007019489406\n",
      "train loss: 1.3224171248442849\n",
      "=== epoch:71, train acc:0.9333333333333333, test acc:0.7333333333333333 ===\n",
      "train loss: 1.3627817450643642\n",
      "train loss: 1.9889197095018054\n",
      "train loss: 1.573941126031442\n",
      "train loss: 2.1663043483448456\n",
      "train loss: 1.3151197469383784\n",
      "train loss: 1.2104809978185727\n",
      "train loss: 1.3493664497530136\n",
      "train loss: 1.8402901162063174\n",
      "train loss: 1.7587589792266336\n",
      "=== epoch:72, train acc:0.9555555555555556, test acc:0.8333333333333334 ===\n",
      "train loss: 1.47362329337419\n",
      "train loss: 1.2819817594601624\n",
      "train loss: 1.6470163458836673\n",
      "train loss: 1.632208697670211\n",
      "train loss: 1.9485482637355218\n",
      "train loss: 1.7519913771691138\n",
      "train loss: 0.739060512102735\n",
      "train loss: 1.8469196484526742\n",
      "train loss: 1.6451322628146132\n",
      "=== epoch:73, train acc:0.9555555555555556, test acc:0.8333333333333334 ===\n",
      "train loss: 1.7590302133332512\n",
      "train loss: 1.2967493009765898\n",
      "train loss: 0.9966831194273652\n",
      "train loss: 1.7193878497968968\n",
      "train loss: 1.3071551712626814\n",
      "train loss: 1.2522166254424985\n",
      "train loss: 1.6592826212719254\n",
      "train loss: 1.3828797197610516\n",
      "train loss: 1.9685139446371591\n",
      "=== epoch:74, train acc:0.9666666666666667, test acc:0.9 ===\n",
      "Reached accuracy_limit\n",
      "=====Final Test Accuracy====\n",
      "test acc: 0.9\n"
     ]
    }
   ],
   "source": [
    "optimizer = [\"SGD\",\"Momentum\",\"NAG\",\"AdaGrad\",\"RMSprop\",\"Adam\"]\n",
    "new = dict()\n",
    "for opt in optimizer:\n",
    "    file_path = file_path\n",
    "    query = \"TirionFordring$DMLP$InputLayer_CSV(y_label_index_starting=4, y_label_index_last=4)§Perceptron(hidden_layer_num=10)§Activation_Relu()§Perceptron(final_layer=True)§Activation_Sigmoid()§MeanSquaredError()§PerceptronParameter(learning_rate=0.001, batch_size=10, epochs=100, optimizer='\"+opt+\"')\"\n",
    "    query = str(query)\n",
    "    a = LoopController(file_path,query)\n",
    "    a.learn()\n",
    "    new[a.param['optimizer']]=a.test_acc_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"seaborn\")\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "# x_ = list(range(max([len(i) for i in new.values()])))\n",
    "\n",
    "for key, val in new.items():\n",
    "    key = str(key)\n",
    "    plt.plot(val, label=key)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, weight_layer, grads):\n",
    "        for key in weight_layer.keys():\n",
    "            weight_layer[key] -= self.learning_rate * grads[key] \n",
    "            \n",
    "            \n",
    "class Momentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, weight_layer, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in weight_layer.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in weight_layer.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.learning_rate*grads[key] \n",
    "            weight_layer[key] += self.v[key]\n",
    "\n",
    "\n",
    "class NAG:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, weight_layer, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in weight_layer.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in weight_layer.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.learning_rate * grads[key]\n",
    "            weight_layer[key] += self.momentum * self.momentum * self.v[key]\n",
    "            weight_layer[key] -= (1 + self.momentum) * self.learning_rate * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, weight_layer, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in weight_layer.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in weight_layer.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            weight_layer[key] -= self.learning_rate * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, learning_rate=0.01, decay_rate = 0.99):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, weight_layer, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in weight_layer.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in weight_layer.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            weight_layer[key] -= self.learning_rate * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, weight_layer, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in weight_layer.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        learning_rate_t  = self.learning_rate * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in weight_layer.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            weight_layer[key] -= learning_rate_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #weight_layer[key] += self.learning_rate * unbias_m / (np.sqrt(unbisa_b) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SummationLayer\n",
    "class SummationLayer:\n",
    "    weight_dict = {}\n",
    "    pass\n",
    "    \n",
    "class Perceptron(SummationLayer):\n",
    "    def __init__(self, hidden_layer_num=10, final_layer=False):\n",
    "        self.hidden_layer_num = hidden_layer_num\n",
    "        self.final_layer = final_layer\n",
    "        self.original_x_shape = None\n",
    "        self.x = None\n",
    "        \n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        \n",
    "        self.weight_gradient = None\n",
    "        self.bias_gradient = None\n",
    "        \n",
    "    def update(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        \n",
    "        \n",
    "    def pass_hidden_layer_num(self):\n",
    "        if self.final_layer == True:\n",
    "            hidden_layer_num = \"final_layer\"\n",
    "        else:\n",
    "            hidden_layer_num = self.hidden_layer_num\n",
    "        \n",
    "        return hidden_layer_num\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "        \n",
    "        out = np.dot(self.x, self.weight) + self.bias\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        dot_matrix= np.dot(gradient, self.weight.T)\n",
    "        self.weight_gradient = np.dot(self.x.T, gradient)\n",
    "        self.bias_gradient = np.sum(gradient, axis=0)\n",
    "        \n",
    "        dot_matrix = dot_matrix.reshape(*self.original_x_shape)\n",
    "        \n",
    "        return dot_matrix\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation\n",
    "class ActivationLayer:\n",
    "    pass\n",
    "class Activation_Sigmoid(ActivationLayer):\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        dx = gradient * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "    \n",
    "    \n",
    "class Activation_Relu(ActivationLayer):\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CostLayer\n",
    "class CostLayer:\n",
    "    loss = None   #손실함수\n",
    "    y_hat = None  #출력 값\n",
    "    y = None      #정답 레이블(one-hot encoding형태)\n",
    "    \n",
    "class MeanSquaredError(CostLayer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, y_hat, y):\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        self.loss = 0.5 * np.sum((y_hat-y)**2)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, gradient=1):\n",
    "        batch_size = self.y.shape[0]\n",
    "        if self.y.size == self.y_hat.size:\n",
    "            dx = (self.y_hat - self.y)/batch_size\n",
    "        else:\n",
    "            dx = self.y_hat.copy()\n",
    "            dx[np.arage(batch_size), self.y]-=1\n",
    "            dx = dx/batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Layer\n",
    "class Parameter:\n",
    "    param = dict()\n",
    "class PerceptronParameter(Parameter):\n",
    "    def __init__(self,learning_rate=0.1, batch_size=10, epochs=10, optimizer=\"sgd\", accuracy_limit=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy_limit = accuracy_limit\n",
    "        \n",
    "    def get_parameter(self):\n",
    "        self.param[\"learning_rate\"] = self.learning_rate\n",
    "        self.param[\"batch_size\"] = self.batch_size\n",
    "        self.param[\"epochs\"] = self.epochs\n",
    "        self.param[\"optimizer\"] = self.optimizer\n",
    "        self.param[\"accuracy_limit\"] = self.accuracy_limit\n",
    "        return self.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input Layer\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections.abc import Iterable\n",
    "import random\n",
    "\n",
    "class InputLayer:\n",
    "    y_label_starting = None\n",
    "    y_label_last = None\n",
    "\n",
    "    y_label_shape = None\n",
    "\n",
    "    training_input_matrix = None\n",
    "    training_y_label = None\n",
    "\n",
    "    validation_input_matrix = None\n",
    "    validation_y_label = None\n",
    "\n",
    "    test_input_matrix = None\n",
    "    test_y_label = None\n",
    "\n",
    "    one_hot_encoding_mappings = dict()\n",
    "    one_hot_encoding_mappings_for_prediction = dict()\n",
    "    y_label_mappings = dict()\n",
    "    y_label_mappings_for_prediction = dict()\n",
    "\n",
    "    returning_shape = None\n",
    "\n",
    "\n",
    "class InputLayer_CSV(InputLayer):\n",
    "    def __init__(self, y_label_index_starting, y_label_index_last):\n",
    "        self.y_label_starting = y_label_index_starting\n",
    "        self.y_label_last = y_label_index_last\n",
    "\n",
    "    def build_layer(self, csv_path, train_validation_test_ratio=(6,2,2)):\n",
    "        y_label_index_starting = self.y_label_starting\n",
    "        y_label_index_last = self.y_label_last\n",
    "\n",
    "        def csv_to_header_and_data(csv_path):\n",
    "            file_instance = open(csv_path)\n",
    "            rdr = csv.reader(file_instance)\n",
    "            raw_data = [i for i in rdr]\n",
    "            return raw_data[0], raw_data[1:], len(raw_data[0])\n",
    "\n",
    "\n",
    "        def typecast_data_accordingly(header, data):\n",
    "            output = []\n",
    "            for each_row in data:\n",
    "                for index in range(len(header)):\n",
    "                    if header[index] == 'string':\n",
    "                        each_row[index] = str(each_row[index])\n",
    "                        continue\n",
    "                    if header[index] == 'number':\n",
    "                        each_row[index] = float(each_row[index])\n",
    "                output.append(each_row)\n",
    "            return output\n",
    "\n",
    "\n",
    "        def encoding_rules(header, data):\n",
    "            all_mappings = dict()\n",
    "            all_mappings_reverse = dict()\n",
    "\n",
    "            for header_index in range(len(header)):\n",
    "                if header[header_index] == 'number':\n",
    "                    continue\n",
    "\n",
    "                string_to_one_hot_encoding = dict()\n",
    "                one_hot_encoding_to_string = dict()\n",
    "\n",
    "                temp = set()\n",
    "                for each_list in data:\n",
    "                    temp.add(each_list[header_index])\n",
    "\n",
    "                counter = 0\n",
    "                for i in temp:\n",
    "                    cal = list(np.zeros(len(temp)))\n",
    "                    cal[counter] = 1\n",
    "                    string_to_one_hot_encoding[i] = cal\n",
    "                    one_hot_encoding_to_string[tuple(cal)] = i\n",
    "                    counter += 1\n",
    "\n",
    "                #행번호 밑에 각 string 별 변환해야 할 one-hot-encoding이 딕셔너리로 적혀있다.\n",
    "                all_mappings[header_index] = string_to_one_hot_encoding\n",
    "                all_mappings_reverse[header_index] = one_hot_encoding_to_string\n",
    "            return all_mappings, all_mappings_reverse\n",
    "\n",
    "        def flatten(items):\n",
    "            \"\"\"My homage to pylang of Stack Overflow\"\"\"\n",
    "            for x in items:\n",
    "                if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "                    for sub_x in flatten(x):\n",
    "                        yield sub_x\n",
    "                else:\n",
    "                    yield x\n",
    "\n",
    "        def apply_flattening(data, encoding_rules):\n",
    "            for column_index in encoding_rules:\n",
    "                for row_index in range(len(data)):\n",
    "                    data[row_index][column_index] = encoding_rules[column_index][data[row_index][column_index]]\n",
    "\n",
    "            for row_index in range(len(data)):\n",
    "                data[row_index] = list(flatten(data[row_index]))\n",
    "            return data\n",
    "\n",
    "        def assign_train_evaluation_test(data, ratio):\n",
    "            random.shuffle(data)\n",
    "            train_last_index = round(len(data)/sum(ratio)*ratio[0])\n",
    "            evaluation_last_index = train_last_index + round(len(data) / sum(ratio) * ratio[1])\n",
    "            test_last_index = evaluation_last_index + round(len(data) / sum(ratio) * ratio[2])\n",
    "            return data[:train_last_index], data[train_last_index:evaluation_last_index], data[evaluation_last_index:test_last_index]\n",
    "\n",
    "        #JSP에서 코딩해서 보낼 때 index는 0번부터 시작하게 하기. 꼭 확인해야함.\n",
    "        def seperate_y_label_and_arrayfy(data, y_label_starting, y_label_last, original_array_length, encoding_rule):\n",
    "            value_to_add = 0\n",
    "            if y_label_starting == 0:\n",
    "                for index in encoding_rule:\n",
    "                    if index <= y_label_last:\n",
    "                        value_to_add += (len(encoding_rule[index])-1)\n",
    "                else:\n",
    "                    y_label_last += value_to_add\n",
    "                    return np.array([i[y_label_last+1:] for i in  data]), np.array([i[:y_label_last+1] for i in data])\n",
    "\n",
    "            elif y_label_last+1 == original_array_length:\n",
    "                for index in encoding_rule:\n",
    "                    if index < y_label_starting:\n",
    "                        value_to_add += (len(encoding_rule[index])-1)\n",
    "                else:\n",
    "                    y_label_starting += value_to_add\n",
    "                    return np.array([i[:y_label_starting] for i in data]), np.array([i[y_label_starting:] for i in data])\n",
    "\n",
    "\n",
    "        header, data, original_array_length = csv_to_header_and_data(csv_path)\n",
    "        data = typecast_data_accordingly(header, data)\n",
    "        self.one_hot_encoding_mappings, self.one_hot_encoding_mappings_for_prediction = encoding_rules(header, data)\n",
    "        data = apply_flattening(data, self.one_hot_encoding_mappings)\n",
    "        train, evaluation, test = assign_train_evaluation_test(data, train_validation_test_ratio)\n",
    "\n",
    "        self.training_input_matrix, self.training_y_label = \\\n",
    "            seperate_y_label_and_arrayfy(train, y_label_index_starting, y_label_index_last, original_array_length, self.one_hot_encoding_mappings)\n",
    "        self.validation_input_matrix, self.validation_y_label = \\\n",
    "            seperate_y_label_and_arrayfy(evaluation, y_label_index_starting, y_label_index_last, original_array_length, self.one_hot_encoding_mappings)\n",
    "        self.test_input_matrix, self.test_y_label = \\\n",
    "            seperate_y_label_and_arrayfy(test, y_label_index_starting, y_label_index_last, original_array_length, self.one_hot_encoding_mappings)\n",
    "\n",
    "        #print(self.training_input_matrix)\n",
    "        #print(self.training_y_label)\n",
    "\n",
    "        #print(self.training_y_label)\n",
    "\n",
    "        #hard-coded for 1D-data only\n",
    "        self.returning_shape = self.training_input_matrix[0].reshape(1,-1).shape\n",
    "        self.y_label_shape = self.training_y_label[0].reshape(1,-1).shape\n",
    "        if self.one_hot_encoding_mappings != {}:\n",
    "            self.y_label_mappings = self.one_hot_encoding_mappings[self.y_label_starting]\n",
    "            self.y_label_mappings_for_prediction = self.one_hot_encoding_mappings_for_prediction[self.y_label_starting]\n",
    "            \n",
    "        return self.training_input_matrix, self.training_y_label, self.test_input_matrix, self.test_y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
